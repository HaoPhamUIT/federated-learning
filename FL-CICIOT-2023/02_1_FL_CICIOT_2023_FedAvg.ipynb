{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "VKGr6k91wdI-",
      "metadata": {
        "id": "VKGr6k91wdI-"
      },
      "source": [
        "# 01-Data_Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Kr3V2eUhxNrD",
      "metadata": {
        "id": "Kr3V2eUhxNrD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# %pip install flwr[simulation] torch torchvision matplotlib sklearn openml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bThv7qdJHQF6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "collapsed": true,
        "id": "bThv7qdJHQF6",
        "outputId": "4656239b-44f8-48ad-de8c-9ae8f6982133"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Collecting openml\n",
            "  Downloading openml-0.15.1-py3-none-any.whl.metadata (10 kB)\n",
            "Collecting flwr[simulation]\n",
            "  Downloading flwr-1.20.0-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting click<8.2.0 (from flwr[simulation])\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting cryptography<45.0.0,>=44.0.1 (from flwr[simulation])\n",
            "  Downloading cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl.metadata (5.7 kB)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (1.74.0)\n",
            "Collecting grpcio-health-checking<2.0.0,>=1.62.3 (from flwr[simulation])\n",
            "  Downloading grpcio_health_checking-1.74.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "Collecting iterators<0.0.3,>=0.0.2 (from flwr[simulation])\n",
            "  Downloading iterators-0.0.2-py3-none-any.whl.metadata (2.5 kB)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.0.2)\n",
            "Collecting pathspec<0.13.0,>=0.12.1 (from flwr[simulation])\n",
            "  Downloading pathspec-0.12.1-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting protobuf<5.0.0,>=4.21.6 (from flwr[simulation])\n",
            "  Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Collecting pycryptodome<4.0.0,>=3.18.0 (from flwr[simulation])\n",
            "  Downloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.4 kB)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (6.0.2)\n",
            "Collecting ray==2.31.0 (from flwr[simulation])\n",
            "  Downloading ray-2.31.0-cp312-cp312-manylinux2014_x86_64.whl.metadata (13 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.32.3)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (13.9.4)\n",
            "Collecting tomli<3.0.0,>=2.0.1 (from flwr[simulation])\n",
            "  Downloading tomli-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
            "Collecting tomli-w<2.0.0,>=1.0.0 (from flwr[simulation])\n",
            "  Downloading tomli_w-1.2.0-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting typer<0.13.0,>=0.12.5 (from flwr[simulation])\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (3.19.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (4.25.0)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (25.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (1.4.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (1.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.14.1)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Collecting liac-arff>=2.4.0 (from openml)\n",
            "  Downloading liac-arff-2.5.0.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting xmltodict (from openml)\n",
            "  Downloading xmltodict-0.14.2-py2.py3-none-any.whl.metadata (8.0 kB)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from openml) (2.2.2)\n",
            "Collecting minio (from openml)\n",
            "  Downloading minio-7.2.16-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from openml) (18.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openml) (4.67.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr[simulation]) (1.17.1)\n",
            "INFO: pip is looking at multiple versions of grpcio-health-checking to determine which version is compatible with other requirements. This could take a while.\n",
            "Collecting grpcio-health-checking<2.0.0,>=1.62.3 (from flwr[simulation])\n",
            "  Downloading grpcio_health_checking-1.73.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.73.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.72.2-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.72.1-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.71.2-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.71.0-py3-none-any.whl.metadata (1.0 kB)\n",
            "  Downloading grpcio_health_checking-1.70.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: pip is still looking at multiple versions of grpcio-health-checking to determine which version is compatible with other requirements. This could take a while.\n",
            "  Downloading grpcio_health_checking-1.69.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.68.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.68.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.67.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.67.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "INFO: This is taking longer than usual. You might need to provide the dependency resolver with stricter constraints to reduce runtime. See https://pip.pypa.io/warnings/backtracking for guidance. If you want to abort this run, press Ctrl + C.\n",
            "  Downloading grpcio_health_checking-1.66.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.66.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.66.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.65.5-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.65.4-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.65.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.65.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.64.3-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.64.1-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.64.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.63.2-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.63.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "  Downloading grpcio_health_checking-1.62.3-py3-none-any.whl.metadata (1.3 kB)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->openml) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->openml) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from minio->openml) (25.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->minio->openml) (25.1.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.27.0)\n",
            "Downloading ray-2.31.0-cp312-cp312-manylinux2014_x86_64.whl (66.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m66.7/66.7 MB\u001b[0m \u001b[31m10.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openml-0.15.1-py3-none-any.whl (160 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m160.4/160.4 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m98.2/98.2 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading cryptography-44.0.3-cp39-abi3-manylinux_2_34_x86_64.whl (4.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.2/4.2 MB\u001b[0m \u001b[31m66.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading grpcio_health_checking-1.62.3-py3-none-any.whl (18 kB)\n",
            "Downloading iterators-0.0.2-py3-none-any.whl (3.9 kB)\n",
            "Downloading pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Downloading protobuf-4.25.8-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.9/294.9 kB\u001b[0m \u001b[31m22.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pycryptodome-3.23.0-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.3/2.3 MB\u001b[0m \u001b[31m76.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli-2.2.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (242 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m242.3/242.3 kB\u001b[0m \u001b[31m17.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomli_w-1.2.0-py3-none-any.whl (6.7 kB)\n",
            "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.3/47.3 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading flwr-1.20.0-py3-none-any.whl (617 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m617.6/617.6 kB\u001b[0m \u001b[31m34.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading minio-7.2.16-py3-none-any.whl (95 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m95.8/95.8 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading xmltodict-0.14.2-py2.py3-none-any.whl (10.0 kB)\n",
            "Building wheels for collected packages: liac-arff\n",
            "  Building wheel for liac-arff (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for liac-arff: filename=liac_arff-2.5.0-py3-none-any.whl size=11717 sha256=68511ff6b8273fbc5cd17b726f1b108b8b3b79f22c91259b20b4b812cf234f43\n",
            "  Stored in directory: /root/.cache/pip/wheels/a9/ac/cf/c2919807a5c623926d217c0a18eb5b457e5c19d242c3b5963a\n",
            "Successfully built liac-arff\n",
            "Installing collected packages: xmltodict, tomli-w, tomli, pycryptodome, protobuf, pathspec, liac-arff, iterators, click, grpcio-health-checking, cryptography, typer, ray, minio, flwr, openml\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "  Attempting uninstall: click\n",
            "    Found existing installation: click 8.2.1\n",
            "    Uninstalling click-8.2.1:\n",
            "      Successfully uninstalled click-8.2.1\n",
            "  Attempting uninstall: cryptography\n",
            "    Found existing installation: cryptography 43.0.3\n",
            "    Uninstalling cryptography-43.0.3:\n",
            "      Successfully uninstalled cryptography-43.0.3\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.16.0\n",
            "    Uninstalling typer-0.16.0:\n",
            "      Successfully uninstalled typer-0.16.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.13.0 requires protobuf<7.0.0,>=5.29.1, but you have protobuf 4.25.8 which is incompatible.\n",
            "pyopenssl 24.2.1 requires cryptography<44,>=41.0.5, but you have cryptography 44.0.3 which is incompatible.\n",
            "pydrive2 1.21.3 requires cryptography<44, but you have cryptography 44.0.3 which is incompatible.\n",
            "grpcio-status 1.71.2 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.8 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed click-8.1.8 cryptography-44.0.3 flwr-1.20.0 grpcio-health-checking-1.62.3 iterators-0.0.2 liac-arff-2.5.0 minio-7.2.16 openml-0.15.1 pathspec-0.12.1 protobuf-4.25.8 pycryptodome-3.23.0 ray-2.31.0 tomli-2.2.1 tomli-w-1.2.0 typer-0.12.5 xmltodict-0.14.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "google"
                ]
              },
              "id": "14ba2df7e5844c01bf1560d0381800cb"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install flwr[simulation] torch torchvision matplotlib scikit-learn openml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K1gp_nItxkUT",
      "metadata": {
        "id": "K1gp_nItxkUT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4578cf6-093d-47fa-c2b8-fcf24dab9bf5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import flwr as fl\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from flwr.common import Metrics\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "OKah0ChG06AA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKah0ChG06AA",
        "outputId": "2e900169-b2ce-4410-fda4-ae7790c2e5af"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flwr 1.20.0\n",
            "numpy 2.0.2\n",
            "torch 2.8.0+cu126\n",
            "Training on cpu\n"
          ]
        }
      ],
      "source": [
        "print(\"flwr\", fl.__version__)\n",
        "print(\"numpy\", np.__version__)\n",
        "print(\"torch\", torch.__version__)\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69d0407d6015e2ba",
      "metadata": {
        "id": "69d0407d6015e2ba"
      },
      "outputs": [],
      "source": [
        "### THIS SECTION NEEDS TO BE SET TO DETERMINE WHICH CONFIGURATION METHOD TO UTILISE\n",
        "\n",
        "SPLIT_AVAILABLE_METHODS = ['STRATIFIED','LEAVE_ONE_OUT', 'ONE_CLASS', 'HALF_BENIGN' ]\n",
        "METHOD = 'LEAVE_ONE_OUT'\n",
        "NUM_OF_STRATIFIED_CLIENTS = 10  # only applies to stratified method\n",
        "NUM_OF_ROUNDS = 5              # Number of FL rounds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1caa8703e9870d43",
      "metadata": {
        "id": "1caa8703e9870d43"
      },
      "outputs": [],
      "source": [
        "individual_classifier = True\n",
        "group_classifier = False\n",
        "binary_classifier = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6lWhv3SmrgMx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lWhv3SmrgMx",
        "outputId": "c96cca02-d38d-4520-c573-471e6087391f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "28e5b1bb8c5b4c77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28e5b1bb8c5b4c77",
        "outputId": "daab8429-5c4a-48b8-8c3f-59b468625cdf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 4 training files...\n",
            "Available columns in dataset: ['Header_Length', 'Protocol Type', 'Time_To_Live', 'Rate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IGMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Variance', 'Label']\n",
            "Dataset shape: (712311, 40)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rounding numbers in Merged01.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "\r 25%|██▌       | 1/4 [00:03<00:10,  3.57s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rounding numbers in Merged02.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            " 50%|█████     | 2/4 [00:11<00:12,  6.09s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rounding numbers in Merged03.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "\r 75%|███████▌  | 3/4 [00:17<00:06,  6.30s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rounding numbers in Merged04.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "100%|██████████| 4/4 [00:24<00:00,  6.16s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined dataset shape: (2834805, 40)\n",
            "All numbers have been rounded during loading process\n",
            "Using 39 feature columns\n",
            "Feature columns: ['Header_Length', 'Protocol Type', 'Time_To_Live', 'Rate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number']...\n",
            "Label distribution:\n",
            "Label\n",
            "0      66396\n",
            "1     244023\n",
            "2     247295\n",
            "3     245610\n",
            "4     326263\n",
            "5     270683\n",
            "6     432865\n",
            "7     217410\n",
            "8      17330\n",
            "9      17271\n",
            "10     27248\n",
            "11      1380\n",
            "12      1766\n",
            "13    199952\n",
            "14    121972\n",
            "15    160806\n",
            "16      4426\n",
            "17     59802\n",
            "18     45003\n",
            "19     53710\n",
            "20       141\n",
            "21      5795\n",
            "22      4962\n",
            "23     22623\n",
            "24      8165\n",
            "25     10938\n",
            "26     18654\n",
            "27       344\n",
            "28       180\n",
            "29       258\n",
            "30        84\n",
            "31       323\n",
            "32       313\n",
            "33       814\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Load and combine all training data\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define dataset directory (adjust path as needed)\n",
        "DATASET_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/data/CICIoT2023/'\n",
        "\n",
        "# Load all CSV files\n",
        "df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
        "df_sets.sort()\n",
        "\n",
        "# Use 80% for training\n",
        "training_sets = df_sets[:int(len(df_sets)*.8)]\n",
        "\n",
        "print(f\"Loading {len(training_sets)} training files...\")\n",
        "\n",
        "# First, let's check what columns are actually available in the dataset\n",
        "sample_df = pd.read_csv(DATASET_DIRECTORY + training_sets[0])\n",
        "print(f\"Available columns in dataset: {list(sample_df.columns)}\")\n",
        "print(f\"Dataset shape: {sample_df.shape}\")\n",
        "\n",
        "# Combine all training data with immediate rounding\n",
        "combined_df = pd.DataFrame()\n",
        "for file in tqdm(training_sets):\n",
        "    df_temp = pd.read_csv(DATASET_DIRECTORY + file)\n",
        "\n",
        "    # Round numbers immediately after loading each file\n",
        "    print(f\"Rounding numbers in {file}...\")\n",
        "    for col in df_temp.columns:\n",
        "        if col != 'Label' and df_temp[col].dtype in ['float64', 'float32']:\n",
        "            # Get max value to determine rounding precision\n",
        "            col_max = df_temp[col].abs().max()\n",
        "\n",
        "            if col_max > 1000:\n",
        "                # Large values: round to 2 decimal places\n",
        "                df_temp[col] = df_temp[col].round(2)\n",
        "            elif col_max > 1:\n",
        "                # Medium values: round to 4 decimal places\n",
        "                df_temp[col] = df_temp[col].round(4)\n",
        "            else:\n",
        "                # Small values (0-1): round to 6 decimal places\n",
        "                df_temp[col] = df_temp[col].round(6)\n",
        "\n",
        "    combined_df = pd.concat([combined_df, df_temp], ignore_index=True)\n",
        "\n",
        "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
        "print(\"All numbers have been rounded during loading process\")\n",
        "\n",
        "# Use actual column names from the dataset (excluding the Label column)\n",
        "X_columns = [col for col in combined_df.columns if col != 'Label']\n",
        "y_column = 'Label'\n",
        "\n",
        "print(f\"Using {len(X_columns)} feature columns\")\n",
        "print(f\"Feature columns: {X_columns[:10]}...\")  # Show first 10 columns\n",
        "\n",
        "# Apply label mapping based on classification type\n",
        "dict_34_classes = {\n",
        "    'BENIGN': 0, 'DDOS-RSTFINFLOOD': 1, 'DDOS-PSHACK_FLOOD': 2, 'DDOS-SYN_FLOOD': 3,\n",
        "    'DDOS-UDP_FLOOD': 4, 'DDOS-TCP_FLOOD': 5, 'DDOS-ICMP_FLOOD': 6, 'DDOS-SYNONYMOUSIP_FLOOD': 7,\n",
        "    'DDOS-ACK_FRAGMENTATION': 8, 'DDOS-UDP_FRAGMENTATION': 9, 'DDOS-ICMP_FRAGMENTATION': 10,\n",
        "    'DDOS-SLOWLORIS': 11, 'DDOS-HTTP_FLOOD': 12, 'DOS-UDP_FLOOD': 13, 'DOS-SYN_FLOOD': 14,\n",
        "    'DOS-TCP_FLOOD': 15, 'DOS-HTTP_FLOOD': 16, 'MIRAI-GREETH_FLOOD': 17, 'MIRAI-GREIP_FLOOD': 18,\n",
        "    'MIRAI-UDPPLAIN': 19, 'RECON-PINGSWEEP': 20, 'RECON-OSSCAN': 21, 'RECON-PORTSCAN': 22,\n",
        "    'VULNERABILITYSCAN': 23, 'RECON-HOSTDISCOVERY': 24, 'DNS_SPOOFING': 25, 'MITM-ARPSPOOFING': 26,\n",
        "    'BROWSERHIJACKING': 27, 'BACKDOOR_MALWARE': 28, 'XSS': 29, 'UPLOADING_ATTACK': 30,\n",
        "    'SQLINJECTION': 31, 'COMMANDINJECTION': 32, 'DICTIONARYBRUTEFORCE': 33\n",
        "}\n",
        "\n",
        "dict_8_classes = {\n",
        "    0: 0,  # Benign\n",
        "    1:1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1,  # DDoS\n",
        "    13: 7, 14: 7, 15: 7, 16: 7,  # DoS\n",
        "    17: 2, 18: 2, 19: 2,  # Mirai\n",
        "    20: 3, 21: 3, 22: 3, 23: 3, 24: 3,  # Reconnaissance\n",
        "    25: 4, 26: 4,  # Spoofing\n",
        "    27: 5, 28: 5, 29: 5, 30: 5, 31: 5, 32: 5,  # Web\n",
        "    33: 6  # Brute Force\n",
        "}\n",
        "\n",
        "dict_2_classes = {\n",
        "    0: 0,  # Benign\n",
        "    1:1, 2:1, 3:1, 4:1, 5:1, 6:1, 7:1, 8:1, 9:1, 10:1, 11:1, 12:1, 13:1, 14:1, 15:1, 16:1,\n",
        "    17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1,\n",
        "    27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1  # All attacks as malicious\n",
        "}\n",
        "\n",
        "# Apply label mapping\n",
        "combined_df['Label'] = combined_df['Label'].map(dict_34_classes)\n",
        "\n",
        "if group_classifier:\n",
        "    combined_df['Label'] = combined_df['Label'].map(dict_8_classes)\n",
        "elif binary_classifier:\n",
        "    combined_df['Label'] = combined_df['Label'].map(dict_2_classes)\n",
        "\n",
        "# Remove rows with missing labels\n",
        "combined_df = combined_df.dropna(subset=['Label'])\n",
        "combined_df['Label'] = combined_df['Label'].astype(int)\n",
        "\n",
        "print(f\"Label distribution:\\n{combined_df['Label'].value_counts().sort_index()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K-UyWRgN2Xkf",
      "metadata": {
        "id": "K-UyWRgN2Xkf"
      },
      "source": [
        "# Training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "jLsxKT1I1G_K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLsxKT1I1G_K",
        "outputId": "e753723f-bf5a-427a-b11a-f7f03267b7a3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training sets: 4\n",
            "Test sets: 1\n",
            "Reading training data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [00:04<00:14,  4.79s/it]/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            " 50%|█████     | 2/4 [00:08<00:07,  3.92s/it]/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            " 75%|███████▌  | 3/4 [00:12<00:04,  4.02s/it]/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "100%|██████████| 4/4 [00:15<00:00,  3.79s/it]/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "100%|██████████| 4/4 [00:15<00:00,  3.92s/it]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Complete training data size: (2834805, 40)\n",
            "Splitting the data into 99.0%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing training data to pickle file...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training data size: (2806456, 40)\n"
          ]
        }
      ],
      "source": [
        "# Check to see if the file 'training_data.pkl' exists in the directory. If it does, load it. If not, print an error.\n",
        "if os.path.isfile('training_data.pkl'):\n",
        "    print(\"File exists, loading data...\")\n",
        "    train_df = pd.read_pickle('training_data.pkl')\n",
        "    print(\"Training data loaded from pickle file.\")\n",
        "\n",
        "else:\n",
        "    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
        "    df_sets.sort()\n",
        "    training_sets = df_sets[:int(len(df_sets)*.8)]\n",
        "    test_sets = df_sets[int(len(df_sets)*.8):]\n",
        "\n",
        "    # Print the number of files in each set\n",
        "    print('Training sets: {}'.format(len(training_sets)))\n",
        "    print('Test sets: {}'.format(len(test_sets)))\n",
        "\n",
        "    # ######################\n",
        "    # # TEMP CODE - This would replicate the original authors code with the last CSV\n",
        "    # # for training data. Uncomment this section to use this code.\n",
        "    # ######################\n",
        "    # # Set training_sets to the last entry of training_sets\n",
        "    # training_sets = training_sets[-33:]\n",
        "    # print(f\"TO REPLICATE ORIGINAL AUTHORS CODE WITH ONE FILE TRAIN - {training_sets}\")\n",
        "    # #####################\n",
        "    # # END TEMP CODE\n",
        "    # ######################\n",
        "\n",
        "    # Concatenate all training sets into one dataframe\n",
        "    dfs = []\n",
        "    print(\"Reading training data...\")\n",
        "    for train_set in tqdm(training_sets):\n",
        "        df_new = pd.read_csv(DATASET_DIRECTORY + train_set)\n",
        "        dfs.append(df_new)\n",
        "    train_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Complete training data set size\n",
        "    print(\"Complete training data size: {}\".format(train_df.shape))\n",
        "\n",
        "    # Map y column to the dict_34_classes values - The pickle file already has this done.\n",
        "    train_df['Label'] = train_df['Label'].map(dict_34_classes)\n",
        "\n",
        "    # The training data is the 80% of the CSV files in the dataset. The test data is the remaining 20%.\n",
        "    # The Ray Federated learning mechanism cannot cope with all of the 80% training data, so we will split\n",
        "    # the training data using test_train_split. The test data will be ignored as we will use all the data\n",
        "    # from the train_sets files as our training data to keep parity with the original authors code.\n",
        "    #\n",
        "    # By using a subset of the training data split this way, we can have a randomised selection of data\n",
        "    # from all the training CSV files, stratified by the attack types.\n",
        "\n",
        "    # Percentage of original training data to use.\n",
        "    TRAIN_SIZE = 0.99\n",
        "\n",
        "    print(f\"Splitting the data into {TRAIN_SIZE*100}%\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(train_df[X_columns], train_df[y_column], test_size= (1 - TRAIN_SIZE), random_state=42, stratify=train_df[y_column])\n",
        "\n",
        "    # Recombine X_train, and y_train into a dataframe\n",
        "    train_df = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "    # Clean up unused variables\n",
        "\n",
        "    del X_train, y_train, X_test, y_test\n",
        "\n",
        "    # Save the output to a pickle file\n",
        "    print(\"Writing training data to pickle file...\")\n",
        "    train_df.to_pickle('training_data.pkl')\n",
        "\n",
        "print(\"Training data size: {}\".format(train_df.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0J9uy3er17f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J9uy3er17f1",
        "outputId": "6506b36f-9026-4f77-e7bc-3e7dd511e116"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts of attacks in train_df:\n",
            "Label\n",
            "6     428536\n",
            "4     323000\n",
            "5     267976\n",
            "2     244822\n",
            "3     243154\n",
            "1     241583\n",
            "7     215236\n",
            "13    197952\n",
            "15    159198\n",
            "14    120752\n",
            "0      65732\n",
            "17     59204\n",
            "19     53173\n",
            "18     44553\n",
            "10     26976\n",
            "23     22397\n",
            "26     18467\n",
            "8      17157\n",
            "9      17098\n",
            "25     10829\n",
            "24      8083\n",
            "21      5737\n",
            "22      4912\n",
            "16      4382\n",
            "12      1748\n",
            "11      1366\n",
            "33       806\n",
            "27       341\n",
            "31       320\n",
            "32       310\n",
            "29       255\n",
            "28       178\n",
            "20       140\n",
            "30        83\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# show the unique values counts in the label column for train_df\n",
        "print(\"Counts of attacks in train_df:\")\n",
        "print(train_df['Label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "PT6QtcaV2MTH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 495
        },
        "id": "PT6QtcaV2MTH",
        "outputId": "8d321df6-3d26-4dc3-ad4b-56fddf834ba3"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Header_Length  Protocol Type  Time_To_Live          Rate  \\\n",
              "106878           20.00              6         64.00  66313.106719   \n",
              "1146355          20.00              6         64.00  26442.466272   \n",
              "1166244           8.00             17         64.00   6952.614915   \n",
              "2670446          20.00              6         64.00  34433.166407   \n",
              "380158            7.20             17         83.10   8807.492335   \n",
              "...                ...            ...           ...           ...   \n",
              "1443499          19.68              6         62.76  15858.081591   \n",
              "1029352           0.00              1         64.00  32564.472050   \n",
              "323791           20.00              6         64.00  48663.464439   \n",
              "354404           20.00              6         64.00  17346.170389   \n",
              "1166569          20.00              6         64.00  32488.799380   \n",
              "\n",
              "         fin_flag_number  syn_flag_number  rst_flag_number  psh_flag_number  \\\n",
              "106878              0.00              1.0             0.00              0.0   \n",
              "1146355             0.00              1.0             0.00              0.0   \n",
              "1166244             0.00              0.0             0.00              0.0   \n",
              "2670446             0.00              1.0             0.00              0.0   \n",
              "380158              0.00              0.0             0.00              0.0   \n",
              "...                  ...              ...              ...              ...   \n",
              "1443499             0.98              0.0             0.98              0.0   \n",
              "1029352             0.00              0.0             0.00              0.0   \n",
              "323791              0.00              0.0             0.00              0.0   \n",
              "354404              0.00              0.0             0.00              1.0   \n",
              "1166569             1.00              0.0             1.00              0.0   \n",
              "\n",
              "         ack_flag_number  ece_flag_number  ...  Tot sum  Min  Max     AVG  \\\n",
              "106878               0.0              0.0  ...     6000   60   60   60.00   \n",
              "1146355              0.0              0.0  ...     6000   60   60   60.00   \n",
              "1166244              0.0              0.0  ...    55400  554  554  554.00   \n",
              "2670446              0.0              0.0  ...     6000   60   60   60.00   \n",
              "380158               0.0              0.0  ...     6100   60   70   61.00   \n",
              "...                  ...              ...  ...      ...  ...  ...     ...   \n",
              "1443499              0.0              0.0  ...     6303   60  363   63.03   \n",
              "1029352              0.0              0.0  ...     6000   60   60   60.00   \n",
              "323791               0.0              0.0  ...     6000   60   60   60.00   \n",
              "354404               1.0              0.0  ...     6000   60   60   60.00   \n",
              "1166569              0.0              0.0  ...     6000   60   60   60.00   \n",
              "\n",
              "               Std  Tot size       IAT  Number    Variance  Label  \n",
              "106878    0.000000     60.00  0.000065     100    0.000000      7  \n",
              "1146355   0.000000     60.00  0.000038     100    0.000000      3  \n",
              "1166244   0.000000    554.00  0.000160     100    0.000000     19  \n",
              "2670446   0.000000     60.00  0.000029     100    0.000000      3  \n",
              "380158    3.015113     61.00  0.000114     100    9.090909     13  \n",
              "...            ...       ...       ...     ...         ...    ...  \n",
              "1443499  30.300000     63.03  0.000064     100  918.090000      1  \n",
              "1029352   0.000000     60.00  0.000031     100    0.000000      6  \n",
              "323791    0.000000     60.00  0.000021     100    0.000000      5  \n",
              "354404    0.000000     60.00  0.000058     100    0.000000      2  \n",
              "1166569   0.000000     60.00  0.000031     100    0.000000      1  \n",
              "\n",
              "[2806456 rows x 40 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f1f635cb-c47b-4e11-8399-e19b101b0df1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Header_Length</th>\n",
              "      <th>Protocol Type</th>\n",
              "      <th>Time_To_Live</th>\n",
              "      <th>Rate</th>\n",
              "      <th>fin_flag_number</th>\n",
              "      <th>syn_flag_number</th>\n",
              "      <th>rst_flag_number</th>\n",
              "      <th>psh_flag_number</th>\n",
              "      <th>ack_flag_number</th>\n",
              "      <th>ece_flag_number</th>\n",
              "      <th>...</th>\n",
              "      <th>Tot sum</th>\n",
              "      <th>Min</th>\n",
              "      <th>Max</th>\n",
              "      <th>AVG</th>\n",
              "      <th>Std</th>\n",
              "      <th>Tot size</th>\n",
              "      <th>IAT</th>\n",
              "      <th>Number</th>\n",
              "      <th>Variance</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>106878</th>\n",
              "      <td>20.00</td>\n",
              "      <td>6</td>\n",
              "      <td>64.00</td>\n",
              "      <td>66313.106719</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1146355</th>\n",
              "      <td>20.00</td>\n",
              "      <td>6</td>\n",
              "      <td>64.00</td>\n",
              "      <td>26442.466272</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1166244</th>\n",
              "      <td>8.00</td>\n",
              "      <td>17</td>\n",
              "      <td>64.00</td>\n",
              "      <td>6952.614915</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>55400</td>\n",
              "      <td>554</td>\n",
              "      <td>554</td>\n",
              "      <td>554.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>554.00</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2670446</th>\n",
              "      <td>20.00</td>\n",
              "      <td>6</td>\n",
              "      <td>64.00</td>\n",
              "      <td>34433.166407</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380158</th>\n",
              "      <td>7.20</td>\n",
              "      <td>17</td>\n",
              "      <td>83.10</td>\n",
              "      <td>8807.492335</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6100</td>\n",
              "      <td>60</td>\n",
              "      <td>70</td>\n",
              "      <td>61.00</td>\n",
              "      <td>3.015113</td>\n",
              "      <td>61.00</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>100</td>\n",
              "      <td>9.090909</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1443499</th>\n",
              "      <td>19.68</td>\n",
              "      <td>6</td>\n",
              "      <td>62.76</td>\n",
              "      <td>15858.081591</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6303</td>\n",
              "      <td>60</td>\n",
              "      <td>363</td>\n",
              "      <td>63.03</td>\n",
              "      <td>30.300000</td>\n",
              "      <td>63.03</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>100</td>\n",
              "      <td>918.090000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029352</th>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>64.00</td>\n",
              "      <td>32564.472050</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323791</th>\n",
              "      <td>20.00</td>\n",
              "      <td>6</td>\n",
              "      <td>64.00</td>\n",
              "      <td>48663.464439</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354404</th>\n",
              "      <td>20.00</td>\n",
              "      <td>6</td>\n",
              "      <td>64.00</td>\n",
              "      <td>17346.170389</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1166569</th>\n",
              "      <td>20.00</td>\n",
              "      <td>6</td>\n",
              "      <td>64.00</td>\n",
              "      <td>32488.799380</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2806456 rows × 40 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f1f635cb-c47b-4e11-8399-e19b101b0df1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f1f635cb-c47b-4e11-8399-e19b101b0df1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f1f635cb-c47b-4e11-8399-e19b101b0df1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-8bd91775-7d43-45bb-9e37-a4064a36e2ba\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-8bd91775-7d43-45bb-9e37-a4064a36e2ba')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-8bd91775-7d43-45bb-9e37-a4064a36e2ba button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_8806aabc-ca49-4e72-8762-f47e086571bf\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_8806aabc-ca49-4e72-8762-f47e086571bf button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df"
            }
          },
          "metadata": {},
          "execution_count": 11
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NGyzOX3x2dNf",
      "metadata": {
        "id": "NGyzOX3x2dNf"
      },
      "source": [
        "# Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "wXD4NTdM2hCz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXD4NTdM2hCz",
        "outputId": "607e9842-7609-4977-8a67-f0c84beb501a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File testing_data.pkl does not exist, constructing data...\n",
            "Test sets: 1\n",
            "Reading test data...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1/1 [00:05<00:00,  5.05s/it]/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "100%|██████████| 1/1 [00:05<00:00,  5.05s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing test data to pickle file testing_data.pkl...\n",
            "Testing data size: (744804, 40)\n"
          ]
        }
      ],
      "source": [
        "# Check to see if the file 'test_data.pkl' exists in the directory. If it does, load it. If not, print an error.\n",
        "testing_data_pickle_file = 'testing_data.pkl'\n",
        "\n",
        "if os.path.isfile(testing_data_pickle_file):\n",
        "    print(f\"File {testing_data_pickle_file} exists, loading data...\")\n",
        "    test_df = pd.read_pickle(testing_data_pickle_file)\n",
        "    print(\"Test data loaded from pickle file.\")\n",
        "\n",
        "else:\n",
        "    print(f\"File {testing_data_pickle_file} does not exist, constructing data...\")\n",
        "\n",
        "    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
        "    df_sets.sort()\n",
        "    training_sets = df_sets[:int(len(df_sets)*.8)]\n",
        "    test_sets = df_sets[int(len(df_sets)*.8):]\n",
        "\n",
        "    # Print the number of files in each set\n",
        "    print('Test sets: {}'.format(len(test_sets)))\n",
        "\n",
        "    # Concatenate all testing sets into one dataframe\n",
        "    dfs = []\n",
        "    print(\"Reading test data...\")\n",
        "    for test_set in tqdm(test_sets):\n",
        "        df_new = pd.read_csv(DATASET_DIRECTORY + test_set)\n",
        "        dfs.append(df_new)\n",
        "    test_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Map y column to the dict_34_classes values - The pickle file already has this done.\n",
        "    test_df['Label'] = test_df['Label'].map(dict_34_classes)\n",
        "\n",
        "    # Save the output to a pickle file\n",
        "    print(f\"Writing test data to pickle file {testing_data_pickle_file}...\")\n",
        "    test_df.to_pickle(testing_data_pickle_file)\n",
        "\n",
        "print(\"Testing data size: {}\".format(test_df.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-nkwO63h2yQm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nkwO63h2yQm",
        "outputId": "370930aa-c5e0-4cf3-a677-11b08e856139"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in train_df: 2806456\n",
            "Number of rows in test_df: 744804\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of rows in train_df: {}\".format(len(train_df)))\n",
        "print(\"Number of rows in test_df: {}\".format(len(test_df)))\n",
        "\n",
        "train_size = len(train_df)\n",
        "test_size = len(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NM5ZMAep2-Cx",
      "metadata": {
        "id": "NM5ZMAep2-Cx"
      },
      "source": [
        "\n",
        "# Scale the test and train data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mIuxtu6z2_Rs",
      "metadata": {
        "id": "mIuxtu6z2_Rs"
      },
      "source": [
        "\n",
        "Scale the training data input features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cOOsYXiO3A1-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOOsYXiO3A1-",
        "outputId": "6ccb7438-8fd7-4d27-96c9-845e60ed5c46"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for and handling infinite values...\n",
            "Infinite values handled and rows with NaN removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# Check for and handle infinite values\n",
        "print(\"Checking for and handling infinite values...\")\n",
        "train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "train_df.dropna(inplace=True)\n",
        "print(\"Infinite values handled and rows with NaN removed.\")\n",
        "\n",
        "train_df[X_columns] = scaler.fit_transform(train_df[X_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "J6ijRAD23ooV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6ijRAD23ooV",
        "outputId": "e675a4e6-a387-4074-af50-3a10cd428d67"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for and handling infinite values in test data...\n",
            "Infinite values handled and rows with NaN removed from test data.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Check for and handle infinite values\n",
        "print(\"Checking for and handling infinite values in test data...\")\n",
        "test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "test_df.dropna(inplace=True)\n",
        "print(\"Infinite values handled and rows with NaN removed from test data.\")\n",
        "\n",
        "# Fit the scaler on the training data and then transform the test data\n",
        "#scaler.fit(train_df[X_columns])\n",
        "test_df[X_columns] = scaler.transform(test_df[X_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "Ks2iTZxO4DyJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks2iTZxO4DyJ",
        "outputId": "cce29a35-9a9a-4237-f875-6cbc060cfe96"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual 34 Class classifier... - No adjustments to labels in test and train dataframes\n"
          ]
        }
      ],
      "source": [
        "class_size_map = {2: \"Binary\", 8: \"Group\", 34: \"Individual\"}\n",
        "\n",
        "if group_classifier:\n",
        "    print(\"Group 8 Class Classifier... - Adjusting labels in test and train dataframes\")\n",
        "    # Map y column to the dict_7_classes values\n",
        "    test_df['label'] = test_df['label'].map(dict_8_classes)\n",
        "    train_df['label'] = train_df['label'].map(dict_8_classes)\n",
        "    class_size = \"8\"\n",
        "\n",
        "elif binary_classifier:\n",
        "    print(\"Binary 2 Class Classifier... - Adjusting labels in test and train dataframes\")\n",
        "    # Map y column to the dict_2_classes values\n",
        "    test_df['label'] = test_df['label'].map(dict_2_classes)\n",
        "    train_df['label'] = train_df['label'].map(dict_2_classes)\n",
        "    class_size = \"2\"\n",
        "\n",
        "else:\n",
        "    print (\"Individual 34 Class classifier... - No adjustments to labels in test and train dataframes\")\n",
        "    class_size = \"34\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R5FT7UxH6Q7Z",
      "metadata": {
        "id": "R5FT7UxH6Q7Z"
      },
      "source": [
        "# Split the Training Data into partitions for the Federated Learning clients depending on the test required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "-4By9-B5CNR3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4By9-B5CNR3",
        "outputId": "29b85e2d-4ef2-4256-ddf5-502effb730ea"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mLEAVE_ONE_OUT METHOD\u001b[0m with 34 class classifier\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Define the colours used for text printing\n",
        "from enum import Enum\n",
        "class Colours(Enum):\n",
        "    RED = \"\\033[31m\"\n",
        "    YELLOW = \"\\033[33m\"\n",
        "    NORMAL = \"\\033[0m\"\n",
        "\n",
        "# Define fl_X_train and fl_y_train\n",
        "fl_X_train = []\n",
        "fl_y_train = []\n",
        "\n",
        "client_df = pd.DataFrame()\n",
        "\n",
        "# Define the target label column\n",
        "y_column = 'Label'\n",
        "\n",
        "# STRATIFIED method: evenly distribute class labels across clients\n",
        "if METHOD == 'STRATIFIED':\n",
        "    print(f\"{Colours.YELLOW.value}STRATIFIED METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
        "    skf = StratifiedKFold(n_splits=NUM_OF_STRATIFIED_CLIENTS, shuffle=True, random_state=42)\n",
        "    for _, test_index in skf.split(train_df[X_columns], train_df[y_column]):\n",
        "        fl_X_train.append(train_df.iloc[test_index][X_columns])\n",
        "        fl_y_train.append(train_df.iloc[test_index][y_column])\n",
        "\n",
        "# LEAVE_ONE_OUT: remove one class (or benign) from each client’s dataset\n",
        "elif METHOD == 'LEAVE_ONE_OUT':\n",
        "    print(f\"{Colours.YELLOW.value}LEAVE_ONE_OUT METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
        "\n",
        "    num_splits = int(class_size) - 1 if (individual_classifier or group_classifier) else 10\n",
        "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for i, (_, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
        "        current_fold_df = train_df.iloc[test_index]\n",
        "        if binary_classifier:\n",
        "            # Even-indexed client: exclude attack class 1\n",
        "            if i % 2 == 0:\n",
        "                client_df = current_fold_df[current_fold_df[y_column] != 1].copy()\n",
        "            else:\n",
        "                client_df = current_fold_df.copy()\n",
        "        else:\n",
        "            # Exclude one specific attack class\n",
        "            client_df = current_fold_df[current_fold_df[y_column] != (i + 1)].copy()\n",
        "\n",
        "        fl_X_train.append(client_df[X_columns])\n",
        "        fl_y_train.append(client_df[y_column])\n",
        "\n",
        "# ONE_CLASS: each client has Benign + 1 attack class only\n",
        "elif METHOD == 'ONE_CLASS':\n",
        "    print(f\"{Colours.YELLOW.value}ONE_CLASS METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
        "\n",
        "    num_splits = int(class_size) - 1 if (individual_classifier or group_classifier) else 10\n",
        "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for i, (_, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
        "        current_fold_df = train_df.iloc[test_index]\n",
        "        if binary_classifier:\n",
        "            # Even-indexed client: only Benign data\n",
        "            if i % 2 == 0:\n",
        "                client_df = current_fold_df[current_fold_df[y_column] != 1].copy()\n",
        "            else:\n",
        "                client_df = current_fold_df.copy()\n",
        "        else:\n",
        "            # Include only Benign and the (i+1)-th attack class\n",
        "            mask = (current_fold_df[y_column] == 0) | (current_fold_df[y_column] == (i + 1))\n",
        "            client_df = current_fold_df[mask].copy()\n",
        "\n",
        "        fl_X_train.append(client_df[X_columns])\n",
        "        fl_y_train.append(client_df[y_column])\n",
        "\n",
        "# HALF_BENIGN: alternate clients between only-benign and full-class datasets\n",
        "elif METHOD == 'HALF_BENIGN':\n",
        "    print(f\"{Colours.YELLOW.value}HALF_BENIGN METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    for i, (_, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
        "        current_fold_df = train_df.iloc[test_index]\n",
        "        if i % 2 == 0:\n",
        "            # Even-indexed clients: only Benign data\n",
        "            client_df = current_fold_df[current_fold_df[y_column] == 0].copy()\n",
        "        else:\n",
        "            # Odd-indexed clients: all data\n",
        "            client_df = current_fold_df.copy()\n",
        "\n",
        "        fl_X_train.append(client_df[X_columns])\n",
        "        fl_y_train.append(client_df[y_column])\n",
        "\n",
        "# Handle unknown METHOD value\n",
        "else:\n",
        "    print(f\"{Colours.RED.value}ERROR: Method {METHOD} not recognised{Colours.NORMAL.value}\")\n",
        "\n",
        "# Update the number of clients created\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2vEhlpsLMCMF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2vEhlpsLMCMF",
        "outputId": "d28ab3f2-82df-42c6-c976-f3c63495f54a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Client ID: 0 ---\n",
            "fl_X_train[0].shape: (77723, 39)\n",
            "fl_y_train[0].value_counts():\n",
            "Label\n",
            "6     12985\n",
            "4      9788\n",
            "5      8121\n",
            "2      7419\n",
            "3      7368\n",
            "7      6523\n",
            "13     5998\n",
            "15     4825\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      678\n",
            "26      560\n",
            "8       519\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      148\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "27       11\n",
            "31        9\n",
            "32        9\n",
            "29        8\n",
            "20        5\n",
            "28        5\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[0].unique(): [ 4  6 13 18 25 19 24  2  5  7 14 15  3 17 10 23 11  8  0 33 26 22 12 21\n",
            "  9 16 27 30 29 28 20 31 32]\n",
            "\n",
            "--- Client ID: 1 ---\n",
            "fl_X_train[1].shape: (77624, 39)\n",
            "fl_y_train[1].value_counts():\n",
            "Label\n",
            "6     12985\n",
            "4      9788\n",
            "5      8121\n",
            "3      7368\n",
            "1      7320\n",
            "7      6523\n",
            "13     5998\n",
            "15     4825\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      678\n",
            "26      560\n",
            "8       519\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      148\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "27       11\n",
            "31        9\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        5\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[1].unique(): [ 1  7  6 18  3 19  4 14  5 10 13  0 15 17 16 26 23 25  9 12 22 24 21  8\n",
            " 33 31 11 27 30 29 32 28 20]\n",
            "\n",
            "--- Client ID: 2 ---\n",
            "fl_X_train[2].shape: (77675, 39)\n",
            "fl_y_train[2].value_counts():\n",
            "Label\n",
            "6     12985\n",
            "4      9788\n",
            "5      8121\n",
            "2      7419\n",
            "1      7320\n",
            "7      6523\n",
            "13     5998\n",
            "15     4825\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1349\n",
            "10      818\n",
            "23      678\n",
            "26      560\n",
            "8       519\n",
            "9       519\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      148\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "27       11\n",
            "32        9\n",
            "31        9\n",
            "29        8\n",
            "28        5\n",
            "20        5\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[2].unique(): [ 7  2  5 14  6 13  0 18  4  1 25 15 26 17  8 19 23 10 21  9 16 11 22 12\n",
            " 24 32 28 27 33 29 31 20 30]\n",
            "\n",
            "--- Client ID: 3 ---\n",
            "fl_X_train[3].shape: (75254, 39)\n",
            "fl_y_train[3].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "5      8121\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6523\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1793\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      678\n",
            "26      560\n",
            "9       519\n",
            "8       519\n",
            "25      328\n",
            "24      244\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "27       11\n",
            "31        9\n",
            "32        9\n",
            "29        8\n",
            "20        5\n",
            "28        5\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[3].unique(): [ 7  6  3  9 15  1  5 14  2 23 26 19 13 10  8 18  0 25 24 17 12 21 16 22\n",
            " 11 27 33 31 29 20 32 28 30]\n",
            "\n",
            "--- Client ID: 4 ---\n",
            "fl_X_train[4].shape: (76921, 39)\n",
            "fl_y_train[4].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6523\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1793\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      678\n",
            "26      560\n",
            "8       519\n",
            "9       518\n",
            "25      329\n",
            "24      244\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "32       10\n",
            "27       10\n",
            "31        9\n",
            "29        8\n",
            "28        5\n",
            "20        5\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[4].unique(): [ 3 15 13  6  1  7 21  8  4  2 14 19 18 17 25 23 33  0 24 16 10 22  9 26\n",
            " 11 28 12 32 29 20 31 27 30]\n",
            "\n",
            "--- Client ID: 5 ---\n",
            "fl_X_train[5].shape: (72056, 39)\n",
            "fl_y_train[5].value_counts():\n",
            "Label\n",
            "4     9788\n",
            "5     8120\n",
            "2     7419\n",
            "3     7368\n",
            "1     7320\n",
            "7     6523\n",
            "13    5998\n",
            "15    4824\n",
            "14    3660\n",
            "0     1992\n",
            "17    1793\n",
            "19    1611\n",
            "18    1350\n",
            "10     817\n",
            "23     678\n",
            "26     559\n",
            "8      520\n",
            "9      518\n",
            "25     329\n",
            "24     245\n",
            "21     174\n",
            "22     149\n",
            "16     133\n",
            "12      53\n",
            "11      41\n",
            "33      25\n",
            "27      10\n",
            "32      10\n",
            "31       9\n",
            "29       8\n",
            "28       5\n",
            "20       5\n",
            "30       2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[5].unique(): [ 5  4 19  1 15 14  2  8 13 17  7  3 23 10  9  0 18 25 26 21 11 33 16 12\n",
            " 28 22 24 27 29 32 20 30 31]\n",
            "\n",
            "--- Client ID: 6 ---\n",
            "fl_X_train[6].shape: (78520, 39)\n",
            "fl_y_train[6].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7369\n",
            "1      7321\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      678\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      329\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "32       10\n",
            "27       10\n",
            "31        9\n",
            "29        8\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[6].unique(): [ 1 15  0  2 13  3  4 19 18 14  6  5 17  9  8 10 26 23 21 22 16 24 32 25\n",
            " 12 11 31 33 27 29 30 28 20]\n",
            "\n",
            "--- Client ID: 7 ---\n",
            "fl_X_train[7].shape: (84522, 39)\n",
            "fl_y_train[7].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7369\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      678\n",
            "26      559\n",
            "9       518\n",
            "25      329\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "27       10\n",
            "32       10\n",
            "31        9\n",
            "29        8\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[7].unique(): [17  6  5  1  4 15  2 13  7 10  3  0 18 23  9 14 25 26 22 19 16 24 33 21\n",
            " 27 12 11 32 28 29 31 30 20]\n",
            "\n",
            "--- Client ID: 8 ---\n",
            "fl_X_train[8].shape: (84524, 39)\n",
            "fl_y_train[8].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "10      817\n",
            "23      678\n",
            "26      559\n",
            "8       520\n",
            "25      329\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "31       10\n",
            "27       10\n",
            "32       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[8].unique(): [ 3 14  1  6 22 15  4  7  8 13  5 17  2 19 25  0 16 18 26 23 10 21 24 12\n",
            " 20 32 27 11 33 28 31 29 30]\n",
            "\n",
            "--- Client ID: 9 ---\n",
            "fl_X_train[9].shape: (84225, 39)\n",
            "fl_y_train[9].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "32       10\n",
            "27       10\n",
            "31       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[9].unique(): [ 1 14 15  2  0  4  5  6 18  7 17 19  3 13 23 25  8 26  9 21 16 24 22 32\n",
            " 29 11 28 12 33 31 20 27 30]\n",
            "\n",
            "--- Client ID: 10 ---\n",
            "fl_X_train[10].shape: (85001, 39)\n",
            "fl_y_train[10].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "33       25\n",
            "31       10\n",
            "27       10\n",
            "32       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[10].unique(): [ 3  5  4  1  7  6  0 10 13 14  2 17 15 18 23 22 19  8 25 21 24 26  9 12\n",
            " 29 16 33 27 28 32 31 30 20]\n",
            "\n",
            "--- Client ID: 11 ---\n",
            "fl_X_train[11].shape: (84989, 39)\n",
            "fl_y_train[11].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "11       41\n",
            "33       25\n",
            "32       10\n",
            "31       10\n",
            "27       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[11].unique(): [13  9  7 15  5  2 10  3 14  6 18  1 23 25  4  0 17 16 11 19 26  8 21 33\n",
            " 22 24 32 20 29 27 31 28 30]\n",
            "\n",
            "--- Client ID: 12 ---\n",
            "fl_X_train[12].shape: (79044, 39)\n",
            "fl_y_train[12].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "32       10\n",
            "27       10\n",
            "31       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[12].unique(): [10  7  6  5  3 19 17 25  4  2  1 14 15  0 23 18  8 26  9 24 21 11 12 22\n",
            " 16 28 33 32 29 31 27 20 30]\n",
            "\n",
            "--- Client ID: 13 ---\n",
            "fl_X_train[13].shape: (81383, 39)\n",
            "fl_y_train[13].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "0      1991\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "31       10\n",
            "32       10\n",
            "27       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[13].unique(): [19  1  6  7  0 13 17  3 15  5  4  2 18 10  8 26 25 23 22 12 16 21 24  9\n",
            " 33 29 28 11 20 27 32 30 31]\n",
            "\n",
            "--- Client ID: 14 ---\n",
            "fl_X_train[14].shape: (80218, 39)\n",
            "fl_y_train[14].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "14     3659\n",
            "0      1991\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "32       10\n",
            "31       10\n",
            "27       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[14].unique(): [13 18 14  4  1  2  3  6 17  7  5 26 10  0  8 19  9 25 23 21 22 24 12 32\n",
            " 16 31 11 33 20 27 28 29 30]\n",
            "\n",
            "--- Client ID: 15 ---\n",
            "fl_X_train[15].shape: (84909, 39)\n",
            "fl_y_train[15].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1991\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "32       10\n",
            "31       10\n",
            "27       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[15].unique(): [ 2  1  4  7  5 10  3  6 25 19 13  9 15 14  0 11 21 17  8 18 24 12 22 26\n",
            " 23 28 32 33 20 27 31 29 30]\n",
            "\n",
            "--- Client ID: 16 ---\n",
            "fl_X_train[16].shape: (83248, 39)\n",
            "fl_y_train[16].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1991\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "32       10\n",
            "27       10\n",
            "31       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[16].unique(): [13  6 14  1 15  4  9  5  3 18  8 12 23  0  2 19 21 26 10  7 16 24 25 11\n",
            " 22 32 28 31 20 33 27 30 29]\n",
            "\n",
            "--- Client ID: 17 ---\n",
            "fl_X_train[17].shape: (83692, 39)\n",
            "fl_y_train[17].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1991\n",
            "17     1794\n",
            "19     1611\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "27       10\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[17].unique(): [ 1  4 14  6  3 23  0  2  7 13  9 21 26 15 10  5 17 11 19 24  8 32 25 27\n",
            " 16 22 31 29 12 33 30 28 20]\n",
            "\n",
            "--- Client ID: 18 ---\n",
            "fl_X_train[18].shape: (83431, 39)\n",
            "fl_y_train[18].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1991\n",
            "17     1794\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "31       10\n",
            "27       10\n",
            "32        9\n",
            "29        8\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[18].unique(): [ 7 23  5  6  3  4  1 15 13 26 14  0 17  2 18  8 10 25 16  9 24 22 11 21\n",
            " 29 33 32 20 27 12 30 28 31]\n",
            "\n",
            "--- Client ID: 19 ---\n",
            "fl_X_train[19].shape: (85038, 39)\n",
            "fl_y_train[19].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1991\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      173\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "31       10\n",
            "27       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[19].unique(): [ 2  4  6 13  7  5 10 15  1  0 14  8 18 26  3 25  9 24 17 23 19 16 21 22\n",
            " 11 29 12 27 28 32 33 31 30]\n",
            "\n",
            "--- Client ID: 20 ---\n",
            "fl_X_train[20].shape: (84869, 39)\n",
            "fl_y_train[20].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1991\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "31       10\n",
            "27       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[20].unique(): [ 7  6 15  5  4  2  3 17  0 14 19 13  8  1 26 23 18 10 22 11  9 25 24 16\n",
            " 33 12 31 29 27 30 32 28 20]\n",
            "\n",
            "--- Client ID: 21 ---\n",
            "fl_X_train[21].shape: (84893, 39)\n",
            "fl_y_train[21].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      173\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "31       10\n",
            "27       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[21].unique(): [23  6  2 13  9  5 17  7 15  4 14  1 33  3 10 24  0 19 18 12  8 21 16 26\n",
            " 25 11 29 31 27 32 28 30 20]\n",
            "\n",
            "--- Client ID: 22 ---\n",
            "fl_X_train[22].shape: (84363, 39)\n",
            "fl_y_train[22].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      173\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "27       10\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[22].unique(): [15  4  1  2  7 14  3  5 18  6  8 10 13  0 21 19 17 12 26  9 24 25 16 22\n",
            " 11 29 20 33 28 32 27 31 30]\n",
            "\n",
            "--- Client ID: 23 ---\n",
            "fl_X_train[23].shape: (84797, 39)\n",
            "fl_y_train[23].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "21      173\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "27       10\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[23].unique(): [ 2 25  1  4  7  3  5  6 13  0 14 18  8 23 15 19  9 17 26 22 11 10 16 21\n",
            " 33 12 27 32 20 29 30 31 28]\n",
            "\n",
            "--- Client ID: 24 ---\n",
            "fl_X_train[24].shape: (84714, 39)\n",
            "fl_y_train[24].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "27       10\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[24].unique(): [ 3  5  7 10 18  6  2  1  4 15 26 14 17 13  0  9 19 22 27 23  8 21 16 24\n",
            " 11 33 12 31 28 20 32 29 30]\n",
            "\n",
            "--- Client ID: 25 ---\n",
            "fl_X_train[25].shape: (84482, 39)\n",
            "fl_y_train[25].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "27       10\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[25].unique(): [ 6 13  7  5 14  1  4  3 19 17  0  2  8 21 15 23 18 24 22  9 10 16 29 25\n",
            " 12 20 11 27 33 31 32 28 30]\n",
            "\n",
            "--- Client ID: 26 ---\n",
            "fl_X_train[26].shape: (85031, 39)\n",
            "fl_y_train[26].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       52\n",
            "11       42\n",
            "33       24\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[26].unique(): [ 3  6  7 17 15  9  1 13  4 14  5  2  0 21 18 19 22 10  8 24 16 26 23 25\n",
            " 31 32 20 30 11 12 33 29 28]\n",
            "\n",
            "--- Client ID: 27 ---\n",
            "fl_X_train[27].shape: (85037, 39)\n",
            "fl_y_train[27].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7320\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       53\n",
            "11       41\n",
            "33       24\n",
            "27       11\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[27].unique(): [ 0  7 13 24  2 15  6  5  3  4 18 14 26  1 23 11 17  8 19  9 12 25 21 22\n",
            " 10 16 27 31 33 30 20 29 32]\n",
            "\n",
            "--- Client ID: 28 ---\n",
            "fl_X_train[28].shape: (85034, 39)\n",
            "fl_y_train[28].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       53\n",
            "11       41\n",
            "33       24\n",
            "27       11\n",
            "31       10\n",
            "32        9\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[28].unique(): [ 3  4  9 13  1 14  2  5 19  7 25  0 23 15  6 10 17  8 26 18 16 22 12 11\n",
            " 21 24 27 33 32 20 31 28 30]\n",
            "\n",
            "--- Client ID: 29 ---\n",
            "fl_X_train[29].shape: (85039, 39)\n",
            "fl_y_train[29].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       53\n",
            "11       41\n",
            "33       24\n",
            "27       11\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[29].unique(): [ 0  4 19 13  5  6 14 18 15  3  1  2 10 17 24 26  7 23  9 11 16  8 22 21\n",
            " 25 29 33 32 31 27 12 20 28]\n",
            "\n",
            "--- Client ID: 30 ---\n",
            "fl_X_train[30].shape: (85032, 39)\n",
            "fl_y_train[30].value_counts():\n",
            "Label\n",
            "6     12985\n",
            "4      9788\n",
            "5      8121\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       53\n",
            "11       41\n",
            "33       24\n",
            "27       11\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[30].unique(): [ 7 17  6  4  3 13 15  2 24  1  0 10 26  5 14 18  8 19 25 23 16 22 21 33\n",
            "  9 28 11 12 27 32 29 30 20]\n",
            "\n",
            "--- Client ID: 31 ---\n",
            "fl_X_train[31].shape: (85033, 39)\n",
            "fl_y_train[31].value_counts():\n",
            "Label\n",
            "6     12985\n",
            "4      9788\n",
            "5      8121\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6522\n",
            "13     5998\n",
            "15     4825\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      148\n",
            "16      132\n",
            "12       53\n",
            "11       41\n",
            "33       24\n",
            "27       11\n",
            "31        9\n",
            "29        8\n",
            "20        5\n",
            "28        5\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[31].unique(): [13  4  3  6 14  1 15  5  7 19  2 24 26  0 23 25  9 17 22 10  8 18 16 21\n",
            " 11 12 29 27 31 20 33 28 30]\n",
            "\n",
            "--- Client ID: 32 ---\n",
            "fl_X_train[32].shape: (85018, 39)\n",
            "fl_y_train[32].value_counts():\n",
            "Label\n",
            "6     12985\n",
            "4      9788\n",
            "5      8121\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6522\n",
            "13     5998\n",
            "15     4825\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      678\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      148\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "27       11\n",
            "31        9\n",
            "32        9\n",
            "29        8\n",
            "20        5\n",
            "28        5\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[32].unique(): [ 7 13  6 15  5  1  4  3 14  2 19 18 26 17  0 16  8 25 10 23 11  9 24 31\n",
            " 22 21 12 20 28 27 29 32 30]\n",
            "\n",
            "fl_X_train[0].equals(fl_X_train[1]): False\n"
          ]
        }
      ],
      "source": [
        "# Update the number of clients created\n",
        "NUM_OF_CLIENTS = len(fl_X_train)\n",
        "# --- Inspect the training data for each client ---\n",
        "for i in range(NUM_OF_CLIENTS):\n",
        "    print(f\"\\n--- Client ID: {i} ---\")\n",
        "    print(f\"fl_X_train[{i}].shape: {fl_X_train[i].shape}\")\n",
        "    print(f\"fl_y_train[{i}].value_counts():\\n{fl_y_train[i].value_counts()}\")\n",
        "    print(f\"fl_y_train[{i}].unique(): {fl_y_train[i].unique()}\")\n",
        "\n",
        "# Check if two clients have identical feature data\n",
        "print(f\"\\nfl_X_train[0].equals(fl_X_train[1]): {fl_X_train[0].equals(fl_X_train[1])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada9yHsr_lqP",
      "metadata": {
        "id": "ada9yHsr_lqP"
      },
      "source": [
        "Visualize Data Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UNrBBSrXQ_8G",
      "metadata": {
        "id": "UNrBBSrXQ_8G"
      },
      "source": [
        "STRATIFIED Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kJMALUIIRItz",
      "metadata": {
        "id": "kJMALUIIRItz"
      },
      "outputs": [],
      "source": [
        "#STRATIFIED Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "SL58X3MxRbrH",
      "metadata": {
        "id": "SL58X3MxRbrH"
      },
      "outputs": [],
      "source": [
        "#LEAVE_ONE_OUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hi6fOex_ShvS",
      "metadata": {
        "id": "hi6fOex_ShvS"
      },
      "outputs": [],
      "source": [
        "#Half begign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZzkWjT0WUUg9",
      "metadata": {
        "id": "ZzkWjT0WUUg9"
      },
      "outputs": [],
      "source": [
        "#One class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "_1yUTXTWUlbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 734
        },
        "id": "_1yUTXTWUlbe",
        "outputId": "d4f3d554-51dc-42e8-a40b-58aa6314f284"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3333892375.py:16: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "  colors1 = plt.cm.get_cmap('tab20', 20)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "client_distributions = {}\n",
        "for i in range(NUM_OF_CLIENTS):\n",
        "    # .value_counts() returns a Series, convert it to a dictionary\n",
        "    client_distributions[f'Client {i}'] = fl_y_train[i].value_counts().to_dict()\n",
        "\n",
        "# 2. Convert the dictionary to a Pandas DataFrame for easy manipulation\n",
        "df_dist = pd.DataFrame(client_distributions).fillna(0).astype(int)\n",
        "df_dist = df_dist.sort_index() # Sort the labels numerically for consistent colors\n",
        "\n",
        "# 3. Plot a Stacked Bar Chart\n",
        "# Convert absolute counts to percentages for the chart y-axis\n",
        "df_percent = df_dist.divide(df_dist.sum(axis=0), axis=1) * 100\n",
        "\n",
        "# Create a diverse color palette to ensure distinct colors for all 34 labels\n",
        "# We combine multiple colormaps to get enough unique colors\n",
        "colors1 = plt.cm.get_cmap('tab20', 20)\n",
        "colors2 = plt.cm.get_cmap('tab20b', 20)\n",
        "colors = np.vstack((colors1.colors, colors2.colors))\n",
        "\n",
        "# Plot the chart with a much wider figure size\n",
        "ax = df_percent.T.plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    figsize=(22, 8), # Increased width for readability\n",
        "    color=colors,\n",
        "    width=0.8,\n",
        "    edgecolor=\"white\"\n",
        ")\n",
        "\n",
        "# 4. Customize the plot for better presentation\n",
        "plt.title(f'Data Distribution Across Clients (Method: {METHOD})', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Client ID', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('Percentage of Samples (%)', fontsize=16, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right', fontsize=12) # Rotate labels for better fit\n",
        "plt.yticks(fontsize=12)\n",
        "plt.ylim(0, 105) # Add a little space at the top for annotations\n",
        "\n",
        "# Move the legend outside the plot and arrange it in 2 columns\n",
        "plt.legend(\n",
        "    title='Labels',\n",
        "    bbox_to_anchor=(1.02, 1),\n",
        "    loc='upper left',\n",
        "    borderaxespad=0.,\n",
        "    fontsize='medium',\n",
        "    ncol=2 # Arrange legend in two columns\n",
        ")\n",
        "\n",
        "# Adjust layout to prevent the legend from being cut off\n",
        "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
        "\n",
        "# Add total sample count (n=...) above each bar for context\n",
        "for i, total in enumerate(df_dist.sum(axis=0)):\n",
        "    ax.text(i, 101, f'n={total}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "#  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YatCDOP-MYhk",
      "metadata": {
        "id": "YatCDOP-MYhk"
      },
      "source": [
        "Prepare an output directory where we can store the results of the federated learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "NImC3PNjMZbj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NImC3PNjMZbj",
        "outputId": "bfbfd54c-5723-4365-b77b-9efda07308da"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output directory and summary file created at: Output/train_size-2806456/LEAVE_ONE_OUT_Classifier-34_Clients-33\n"
          ]
        }
      ],
      "source": [
        "# Create an \"Output\" directory if it doesnt exist already\n",
        "if not os.path.exists(\"Output\"):\n",
        "    os.makedirs(\"Output\")\n",
        "\n",
        "sub_dir_name = f\"train_size-{train_size}\"\n",
        "\n",
        "# if sub_dir_name does not exist, create it\n",
        "if not os.path.exists(f\"Output/{sub_dir_name}\"):\n",
        "    os.makedirs(f\"Output/{sub_dir_name}\")\n",
        "\n",
        "test_directory_name = f\"{METHOD}_Classifier-{class_size}_Clients-{NUM_OF_CLIENTS}\"\n",
        "output_path = f\"Output/{sub_dir_name}/{test_directory_name}\" # Lưu lại đường dẫn để dùng sau\n",
        "\n",
        "# Create an \"Output/{METHOD}-{NUM_OF_CLIENTS}-{NUM_OF_ROUNDS}\" directory if it doesnt exist already\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "\n",
        "# Ensure the directory is empty\n",
        "for file in os.listdir(output_path):\n",
        "    file_path = os.path.join(output_path, file)\n",
        "    if os.path.isfile(file_path):\n",
        "        os.unlink(file_path)\n",
        "\n",
        "# Original training size is the sum of all the fl_X_train sizes\n",
        "original_training_size = 0\n",
        "for i in range(len(fl_X_train)):\n",
        "    original_training_size += fl_X_train[i].shape[0]\n",
        "\n",
        "# Write this same info to the output directory/Class Split Info.txt\n",
        "with open(f\"{output_path}/Class Split Info.txt\", \"w\") as f:\n",
        "    for i in range(len(fl_X_train)):\n",
        "        f.write(f\"Client ID: {i}\\n\")\n",
        "        f.write(f\"fl_X_train.shape: {fl_X_train[i].shape}\\n\")\n",
        "        f.write(f\"Training data used {original_training_size}\\n\")\n",
        "        f.write(f\"fl_y_train.value_counts():\\n{fl_y_train[i].value_counts().to_string()}\\n\")\n",
        "        f.write(f\"fl_y_train.unique(): {fl_y_train[i].unique()}\\n\\n\")\n",
        "\n",
        "print(f\"Output directory and summary file created at: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hLLaFTx9PeNs",
      "metadata": {
        "id": "hLLaFTx9PeNs"
      },
      "source": [
        "Convert the training dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "K3SnxWmJPfAx",
      "metadata": {
        "id": "K3SnxWmJPfAx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a122dd1-8be0-4620-9bc3-8745b22cc790"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Convert the testing dataframe to numpy arrays for TensorFlow/Keras\n",
        "X_test = test_df[X_columns].to_numpy()\n",
        "y_test = test_df[y_column].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6sCWMHTXPmDb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sCWMHTXPmDb",
        "outputId": "433e568d-1328-401b-d1ac-964d2302deb5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final data conversion to numpy complete. Dataframes deleted to free up memory.\n"
          ]
        }
      ],
      "source": [
        "# Determine the number of unique classes in the target column\n",
        "num_unique_classes = len(train_df[y_column].unique())\n",
        "\n",
        "# Store the shapes of the original dataframes for logging purposes before deleting them\n",
        "train_df_shape = train_df.shape\n",
        "test_df_shape = test_df.shape\n",
        "\n",
        "# Now that we have fl_X_train, fl_y_train, X_test, and y_test extracted,\n",
        "# we can safely delete the large dataframes to free up memory\n",
        "if 'train_df' in locals():\n",
        "    del train_df\n",
        "if 'test_df' in locals():\n",
        "    del test_df\n",
        "if 'client_df' in locals():\n",
        "    del client_df\n",
        "print(\"Final data conversion to numpy complete. Dataframes deleted to free up memory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OvyRoEagQYez",
      "metadata": {
        "id": "OvyRoEagQYez"
      },
      "source": [
        "Data check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZP98p2NCQXtE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP98p2NCQXtE",
        "outputId": "8797071f-6095-470c-e32b-8ec2d8b6e5c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NUM_CLIENTS: 33\n",
            "NUM_ROUNDS: 5\n",
            "\n",
            "Original training size: 2723339\n",
            "Checking training data split groups\n",
            "0 : X Shape (77723, 39) Y Shape (77723,)\n",
            "1 : X Shape (77624, 39) Y Shape (77624,)\n",
            "2 : X Shape (77675, 39) Y Shape (77675,)\n",
            "3 : X Shape (75254, 39) Y Shape (75254,)\n",
            "4 : X Shape (76921, 39) Y Shape (76921,)\n",
            "5 : X Shape (72056, 39) Y Shape (72056,)\n",
            "6 : X Shape (78520, 39) Y Shape (78520,)\n",
            "7 : X Shape (84522, 39) Y Shape (84522,)\n",
            "8 : X Shape (84524, 39) Y Shape (84524,)\n",
            "9 : X Shape (84225, 39) Y Shape (84225,)\n",
            "10 : X Shape (85001, 39) Y Shape (85001,)\n",
            "11 : X Shape (84989, 39) Y Shape (84989,)\n",
            "12 : X Shape (79044, 39) Y Shape (79044,)\n",
            "13 : X Shape (81383, 39) Y Shape (81383,)\n",
            "14 : X Shape (80218, 39) Y Shape (80218,)\n",
            "15 : X Shape (84909, 39) Y Shape (84909,)\n",
            "16 : X Shape (83248, 39) Y Shape (83248,)\n",
            "17 : X Shape (83692, 39) Y Shape (83692,)\n",
            "18 : X Shape (83431, 39) Y Shape (83431,)\n",
            "19 : X Shape (85038, 39) Y Shape (85038,)\n",
            "20 : X Shape (84869, 39) Y Shape (84869,)\n",
            "21 : X Shape (84893, 39) Y Shape (84893,)\n",
            "22 : X Shape (84363, 39) Y Shape (84363,)\n",
            "23 : X Shape (84797, 39) Y Shape (84797,)\n",
            "24 : X Shape (84714, 39) Y Shape (84714,)\n",
            "25 : X Shape (84482, 39) Y Shape (84482,)\n",
            "26 : X Shape (85031, 39) Y Shape (85031,)\n",
            "27 : X Shape (85037, 39) Y Shape (85037,)\n",
            "28 : X Shape (85034, 39) Y Shape (85034,)\n",
            "29 : X Shape (85039, 39) Y Shape (85039,)\n",
            "30 : X Shape (85032, 39) Y Shape (85032,)\n",
            "31 : X Shape (85033, 39) Y Shape (85033,)\n",
            "32 : X Shape (85018, 39) Y Shape (85018,)\n",
            "\n",
            "Checking testing data\n",
            "X_test size: (744790, 39)\n",
            "y_test size: (744790,)\n",
            "\n",
            "Deploy Simulation\n"
          ]
        }
      ],
      "source": [
        "print(\"NUM_CLIENTS:\", NUM_OF_CLIENTS)\n",
        "\n",
        "print(\"NUM_ROUNDS:\", NUM_OF_ROUNDS)\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"Original training size: {}\".format(original_training_size))\n",
        "\n",
        "\n",
        "print(\"Checking training data split groups\")\n",
        "for i in range(len(fl_X_train)):\n",
        "    print(i, \":\", \"X Shape\", fl_X_train[i].shape, \"Y Shape\", fl_y_train[i].shape)\n",
        "\n",
        "\n",
        "# Print the sizes of X_test and y_test\n",
        "print(\"\\nChecking testing data\")\n",
        "print(\"X_test size: {}\".format(X_test.shape))\n",
        "print(\"y_test size: {}\".format(y_test.shape))\n",
        "\n",
        "print(\"\\nDeploy Simulation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "hdm4eb53tKQU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdm4eb53tKQU",
        "outputId": "b2870f5d-9326-4e84-a633-55651a28a3f6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test min/max/mean: -52.92539879493338 476.80242930958366 -0.000634890663062242\n",
            "Client 0 X min/max/mean: -52.92539879493338 136.22155747982657 -0.02330975502083858\n"
          ]
        }
      ],
      "source": [
        "print(\"X_test min/max/mean:\", np.min(X_test), np.max(X_test), np.mean(X_test))\n",
        "print(\"Client 0 X min/max/mean:\", np.min(fl_X_train[0]), np.max(fl_X_train[0]), np.mean(fl_X_train[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yQNirMRQtYL_",
      "metadata": {
        "id": "yQNirMRQtYL_"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "E-jVdOjKRFdj",
      "metadata": {
        "id": "E-jVdOjKRFdj"
      },
      "source": [
        "#Federated Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CA7bCig1RrDu",
      "metadata": {
        "id": "CA7bCig1RrDu"
      },
      "source": [
        "Import the libraries and print the versions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "WcQyhdi1RFLY",
      "metadata": {
        "id": "WcQyhdi1RFLY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import flwr as fl\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Make TensorFlow log less verbose\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ShtOj_q-R0R7",
      "metadata": {
        "id": "ShtOj_q-R0R7"
      },
      "source": [
        "\n",
        "Define the Client and Server code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "XbTpGaYTS5Ok",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbTpGaYTS5Ok",
        "outputId": "bf37e610-519c-4b75-a7b8-f8b9b9d54df4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn 1.6.1.\n",
            "flwr 1.20.0\n",
            "numpy 2.0.2\n",
            "tf 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import flwr as fl\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print('scikit-learn {}.'.format(sklearn.__version__))\n",
        "print(\"flwr\", fl.__version__)\n",
        "print(\"numpy\", np.__version__)\n",
        "print(\"tf\", tf.__version__)\n",
        "# Make TensorFlow log less verbose\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "import datetime\n",
        "\n",
        "client_evaluations = []\n",
        "\n",
        "class NumpyFlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid, model, train_data, train_labels):\n",
        "        self.model = model\n",
        "        self.cid = cid\n",
        "        self.train_data = train_data\n",
        "        self.train_labels = train_labels\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return self.model.get_weights()\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        print (\"Client \", self.cid, \"Training...\")\n",
        "        self.model.fit(self.train_data, self.train_labels, epochs=5, batch_size=32)\n",
        "        print (\"Client \", self.cid, \"Training complete...\")\n",
        "        return self.model.get_weights(), len(self.train_data), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        print (\"Client \", self.cid, \"Evaluating...\")\n",
        "        loss, accuracy = self.model.evaluate(self.train_data, self.train_labels, batch_size=32)\n",
        "        print(f\"{Colours.YELLOW.value}Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}{Colours.NORMAL.value}\")\n",
        "\n",
        "        # Write the same message to the \"Output/{cid}_Evaluation.txt\" file\n",
        "        with open(f\"Output/{sub_dir_name}/{test_directory_name}/{self.cid}_Evaluation.txt\", \"a\") as f:\n",
        "            f.write(f\"{datetime.datetime.now()} - Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}\\n\")\n",
        "\n",
        "            # Close the file\n",
        "            f.close()\n",
        "\n",
        "        return loss, len(self.train_data), {\"accuracy\": accuracy}\n",
        "\n",
        "    def predict(self, incoming):\n",
        "        prediction = np.argmax( self.model.predict(incoming) ,axis=1)\n",
        "        return prediction\n",
        "\n",
        "def client_fn(cid: str) -> NumpyFlowerClient:\n",
        "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
        "\n",
        "    # Load model\n",
        "    #model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)\n",
        "    #model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    print (\"Client ID:\", cid)\n",
        "\n",
        "    model = Sequential([\n",
        "      #Flatten(input_shape=(79,1)),\n",
        "      Dense(50, activation='relu', input_shape=(fl_X_train[0].shape[1],)),\n",
        "      Dense(25, activation='relu'),\n",
        "      Dense(num_unique_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    partition_id = int(cid)\n",
        "    X_train_c = fl_X_train[partition_id]\n",
        "    y_train_c = fl_y_train[partition_id]\n",
        "\n",
        "    # Create a  single Flower client representing a single organization\n",
        "    return NumpyFlowerClient(cid, model, X_train_c, y_train_c)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "eval_count = 0\n",
        "\n",
        "def get_evaluate_fn(server_model):\n",
        "    global eval_count\n",
        "\n",
        "    def evaluate(server_round, parameters, config):\n",
        "        global eval_count\n",
        "\n",
        "        # Update model weights from the latest client parameters\n",
        "        server_model.set_weights(parameters)\n",
        "        print(f\"Server Evaluating... Evaluation Count: {eval_count}\")\n",
        "\n",
        "        # Evaluate the model on the test set\n",
        "        loss, accuracy = server_model.evaluate(X_test, y_test)\n",
        "\n",
        "        # Record accuracy and loss for visualization or tracking\n",
        "        server_accuracy_history.append(accuracy)\n",
        "        server_loss_history.append(loss)\n",
        "\n",
        "        # Make predictions and save them to a file\n",
        "        y_pred = server_model.predict(X_test)\n",
        "        print(\"Prediction: \", y_pred, y_pred.shape)\n",
        "        np.save(\"y_pred-\" + str(eval_count) + \".npy\", y_pred)\n",
        "\n",
        "        # Print evaluation result to console\n",
        "        print(f\"{Colours.YELLOW.value}Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}{Colours.NORMAL.value}\")\n",
        "\n",
        "        # Write evaluation log to file\n",
        "        with open(f\"Output/{sub_dir_name}/{test_directory_name}/Server_Evaluation.txt\", \"a\") as f:\n",
        "            f.write(f\"{datetime.datetime.now()} - {server_round} : Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\\n\")\n",
        "\n",
        "        # Increment evaluation counter\n",
        "        eval_count += 1\n",
        "\n",
        "        return loss, {\"accuracy\": accuracy}\n",
        "\n",
        "    return evaluate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h-afiVhOUOOD",
      "metadata": {
        "id": "h-afiVhOUOOD"
      },
      "source": [
        "Initialize Server Model and Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "DWCJDKtAUQYC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWCJDKtAUQYC",
        "outputId": "3e240250-364b-4e4a-dad1-0d7698cba502"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "from flwr.common import ndarrays_to_parameters\n",
        "server_model = Sequential([\n",
        "    #Flatten(input_shape=(79,1)),\n",
        "    Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(25, activation='relu'),\n",
        "    Dense(num_unique_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "server_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "initial_weights = server_model.get_weights()\n",
        "initial_parameters = ndarrays_to_parameters(initial_weights)\n",
        "\n",
        "# Create FedAvg strategy\n",
        "strategy = fl.server.strategy.FedAvg(\n",
        "        fraction_fit=1.0,\n",
        "        fraction_evaluate=0.5,\n",
        "        min_fit_clients=10, #10,\n",
        "        min_evaluate_clients=5, #5,\n",
        "        min_available_clients=10, #10,\n",
        "        evaluate_fn=get_evaluate_fn(server_model),\n",
        "        #evaluate_metrics_aggregation_fn=weighted_average,\n",
        ")\n",
        "# Test FedAdam\n",
        "# strategy = fl.server.strategy.FedAdam(\n",
        "#         fraction_fit=1.0,\n",
        "#         fraction_evaluate=0.5,\n",
        "#         min_fit_clients=10,\n",
        "#         min_evaluate_clients=5,\n",
        "#         min_available_clients=10,\n",
        "#         evaluate_fn=get_evaluate_fn(server_model),\n",
        "#         eta=1.0,           # Server-side learning rate\n",
        "#         beta_1=0.9,\n",
        "#         beta_2=0.999,\n",
        "#         tau=1e-9,\n",
        "#         initial_parameters=initial_parameters\n",
        "# )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SRP0GFTBUeFI",
      "metadata": {
        "id": "SRP0GFTBUeFI"
      },
      "source": [
        "## Deploy Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cmUsZg1bUl0r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cmUsZg1bUl0r",
        "outputId": "cc5c969f-dcb1-4ef2-b4a3-070116937de6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\n",
            "Deploy simulation... Method = LEAVE_ONE_OUT - Individual (34) Classifier\n",
            "Number of Clients = 33\n",
            "\n",
            "Writing output to: train_size-2806456/LEAVE_ONE_OUT_Classifier-34_Clients-33\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
            "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
            "\n",
            "\t\t$ flwr new  # Create a new Flower app from a template\n",
            "\n",
            "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
            "\n",
            "\tUsing `start_simulation()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "WARNING:flwr:DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
            "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
            "\n",
            "\t\t$ flwr new  # Create a new Flower app from a template\n",
            "\n",
            "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
            "\n",
            "\tUsing `start_simulation()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=5, no round_timeout\n",
            "2025-08-20 07:09:35,754\tINFO worker.py:1771 -- Started a local Ray instance.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'object_store_memory': 3996350054.0, 'node:172.28.0.12': 1.0, 'node:__internal_head__': 1.0, 'memory': 7992700110.0}\n",
            "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "\u001b[36m(pid=1715)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=1715)\u001b[0m E0000 00:00:1755673782.575003    1715 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=1713)\u001b[0m E0000 00:00:1755673782.646503    1713 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=1713)\u001b[0m W0000 00:00:1755673782.683726    1713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=1713)\u001b[0m W0000 00:00:1755673782.683787    1713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=1713)\u001b[0m W0000 00:00:1755673782.683792    1713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=1713)\u001b[0m W0000 00:00:1755673782.683798    1713 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(pid=1713)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=1713)\u001b[0m E0000 00:00:1755673782.630630    1713 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=1715)\u001b[0m E0000 00:00:1755673782.617427    1715 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=1715)\u001b[0m W0000 00:00:1755673782.679177    1715 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 5\n",
            "Server Evaluating... Evaluation Count: 0\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m49s\u001b[0m 2ms/step - accuracy: 0.0199 - loss: 3.5720\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 1ms/step\n",
            "Prediction:  [[0.03361375 0.03480801 0.02630133 ... 0.02588343 0.02550754 0.02976171]\n",
            " [0.02422309 0.03234754 0.05421331 ... 0.02352194 0.01819041 0.01463096]\n",
            " [0.02369872 0.03795942 0.02384055 ... 0.024004   0.02578877 0.02094381]\n",
            " ...\n",
            " [0.01489241 0.06645945 0.02957171 ... 0.00542535 0.02070759 0.03304766]\n",
            " [0.02643991 0.03157369 0.03413931 ... 0.03026343 0.02391561 0.02359286]\n",
            " [0.02582383 0.03114824 0.03485632 ... 0.02909939 0.02367673 0.02291951]] (744790, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 3.5717809200286865, {'accuracy': 0.0199546180665493}\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 33 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mServer evaluation complete - Accuracy: 0.0200, Loss: 3.5718\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 28\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  28 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 13\n",
            "\u001b[1m  17/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.0939 - loss: 3.4326    \n",
            "\u001b[1m  53/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.1764 - loss: 3.2414\n",
            "\u001b[1m  83/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.2450 - loss: 3.0637\n",
            "\u001b[1m 113/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.3013 - loss: 2.8839\n",
            "\u001b[1m 245/2544\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.4523 - loss: 2.2440\n",
            "\u001b[1m 294/2544\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.4832 - loss: 2.0907\n",
            "\u001b[1m 305/2544\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.4892 - loss: 2.0601\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  13 Training...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m  16/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.0703 - loss: 3.4553        \n",
            "\u001b[1m1535/2544\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6708 - loss: 1.0405\u001b[32m [repeated 88x across cluster]\u001b[0m\n",
            "\u001b[1m1384/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6348 - loss: 1.1623\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m  18/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7742 - loss: 0.4476   \n",
            "\u001b[1m 488/2544\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7696 - loss: 0.4400\u001b[32m [repeated 95x across cluster]\u001b[0m\n",
            "\u001b[1m 410/2544\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4415\n",
            "\u001b[1m 952/2544\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7699 - loss: 0.4358\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m  18/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7845 - loss: 0.4615   \n",
            "\u001b[1m1227/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7424 - loss: 0.5102\n",
            "\u001b[1m1272/2544\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7702 - loss: 0.4345\u001b[32m [repeated 89x across cluster]\u001b[0m\n",
            "\u001b[1m1957/2544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7706 - loss: 0.4327\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:39\u001b[0m 39ms/step - accuracy: 0.8750 - loss: 0.3559\n",
            "\u001b[1m  32/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.8063 - loss: 0.4046\n",
            "\u001b[1m 229/2544\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7833 - loss: 0.4137\u001b[32m [repeated 91x across cluster]\u001b[0m\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7400 - loss: 0.5103\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 39ms/step - accuracy: 0.8750 - loss: 0.3395\n",
            "\u001b[1m  36/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7783 - loss: 0.4491\n",
            "\u001b[1m1888/2544\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7753 - loss: 0.4170\u001b[32m [repeated 98x across cluster]\u001b[0m\n",
            "\u001b[1m1876/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7433 - loss: 0.5008\n",
            "\u001b[1m2195/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7433 - loss: 0.5006\n",
            "\u001b[1m2226/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7433 - loss: 0.5006\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m  10/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7703 - loss: 0.4031   \n",
            "\u001b[1m  34/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7558 - loss: 0.4123\n",
            "\u001b[1m  49/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7590 - loss: 0.4160\n",
            "\u001b[1m  62/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7625 - loss: 0.4163\n",
            "\u001b[1m  74/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7649 - loss: 0.4161\n",
            "\u001b[1m  80/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7657 - loss: 0.4161\n",
            "\u001b[1m  95/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7669 - loss: 0.4168\n",
            "\u001b[1m 108/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7678 - loss: 0.4169\n",
            "\u001b[1m 120/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7684 - loss: 0.4168\n",
            "\u001b[1m 134/2544\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7693 - loss: 0.4164\n",
            "\u001b[1m 145/2544\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7698 - loss: 0.4163\n",
            "\u001b[1m2380/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7433 - loss: 0.5004\u001b[32m [repeated 79x across cluster]\u001b[0m\n",
            "\u001b[1m 166/2544\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7706 - loss: 0.4161\n",
            "\u001b[1m 178/2544\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7710 - loss: 0.4157\n",
            "\u001b[1m 184/2544\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7712 - loss: 0.4155\n",
            "\u001b[1m 197/2544\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7716 - loss: 0.4152\n",
            "\u001b[1m 216/2544\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7721 - loss: 0.4149\n",
            "\u001b[1m 225/2544\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7722 - loss: 0.4148\n",
            "\u001b[1m 240/2544\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7724 - loss: 0.4148\n",
            "\u001b[1m 264/2544\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7725 - loss: 0.4152\n",
            "\u001b[1m 287/2544\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7726 - loss: 0.4154\n",
            "\u001b[1m 316/2544\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7729 - loss: 0.4153\n",
            "\u001b[1m 351/2544\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7733 - loss: 0.4149\n",
            "\u001b[1m 386/2544\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7736 - loss: 0.4144\n",
            "\u001b[1m 420/2544\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7737 - loss: 0.4141\n",
            "\u001b[1m 451/2544\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7737 - loss: 0.4140\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 41ms/step - accuracy: 0.7500 - loss: 0.5391\n",
            "\u001b[1m 484/2544\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7738 - loss: 0.4138\n",
            "\u001b[1m  35/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7656 - loss: 0.4982\n",
            "\u001b[1m 520/2544\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7738 - loss: 0.4137\n",
            "\u001b[1m 552/2544\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7738 - loss: 0.4136\n",
            "\u001b[1m 587/2544\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7737 - loss: 0.4135\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7434 - loss: 0.5001\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m1639/2544\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7745 - loss: 0.4141\u001b[32m [repeated 79x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 43ms/step - accuracy: 0.8438 - loss: 0.3112\n",
            "\u001b[1m  27/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7760 - loss: 0.3791\n",
            "\u001b[1m  52/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7736 - loss: 0.3907\n",
            "\u001b[1m  76/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7732 - loss: 0.3948\n",
            "\u001b[1m2198/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7457 - loss: 0.4939\n",
            "\u001b[1m 103/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7741 - loss: 0.3979\n",
            "\u001b[1m 129/2544\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7739 - loss: 0.3997 \n",
            "\u001b[1m2544/2544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7750 - loss: 0.4136\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m  32/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7164 - loss: 0.5037\n",
            "\u001b[1m 570/2544\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7744 - loss: 0.4064\u001b[32m [repeated 87x across cluster]\u001b[0m\n",
            "\u001b[1m 514/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7419 - loss: 0.4918\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 42ms/step - accuracy: 0.6250 - loss: 0.5705\n",
            "\u001b[1m 626/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7425 - loss: 0.4912\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
            "\u001b[1m 975/2544\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7754 - loss: 0.4067\n",
            "\u001b[1m 983/2544\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7754 - loss: 0.4067\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m 836/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7433 - loss: 0.4907\n",
            "\u001b[1m1415/2544\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7760 - loss: 0.4069\u001b[32m [repeated 59x across cluster]\u001b[0m\n",
            "\u001b[1m 961/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7437 - loss: 0.4904\n",
            "\u001b[1m 939/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7437 - loss: 0.4905\u001b[32m [repeated 19x across cluster]\u001b[0m\n",
            "\u001b[1m1290/2544\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7758 - loss: 0.4068\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  13 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 25\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2132/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7452 - loss: 0.4892\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  25 Training...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m2213/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7453 - loss: 0.4891\n",
            "\u001b[1m2284/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7454 - loss: 0.4890\n",
            "\u001b[1m2269/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7454 - loss: 0.4890\u001b[32m [repeated 84x across cluster]\u001b[0m\n",
            "\u001b[1m2346/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7455 - loss: 0.4889\n",
            "\u001b[1m2651/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7457 - loss: 0.4886\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7457 - loss: 0.4886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \r\u001b[1m   1/2641\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:27:50\u001b[0m 2s/step - accuracy: 0.0625 - loss: 3.4526\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  17/2641\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.0662 - loss: 3.4423    \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  28 Training complete...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 6\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  6 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n",
            "\u001b[1m 982/2641\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6115 - loss: 1.2976\n",
            "\u001b[1m1019/2641\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6146 - loss: 1.2790\u001b[32m [repeated 56x across cluster]\u001b[0m\n",
            "\u001b[1m1038/2641\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6161 - loss: 1.2698\n",
            "\u001b[1m1262/2641\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6315 - loss: 1.1775\n",
            "\u001b[1m  15/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.0676 - loss: 3.4676    \n",
            "\u001b[1m1522/2641\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6448 - loss: 1.0977\n",
            "\u001b[1m1546/2641\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6458 - loss: 1.0914\n",
            "\u001b[1m1553/2641\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6461 - loss: 1.0896\n",
            "\u001b[1m1576/2641\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6470 - loss: 1.0837\n",
            "\u001b[1m1584/2641\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6474 - loss: 1.0817\n",
            "\u001b[1m1898/2641\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6585 - loss: 1.0139\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m1330/2454\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.6860 - loss: 1.1097\u001b[32m [repeated 89x across cluster]\u001b[0m\n",
            "\u001b[1m2231/2641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6675 - loss: 0.9592\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2641\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 41ms/step - accuracy: 0.7500 - loss: 0.5536\n",
            "\u001b[1m  44/2641\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7485 - loss: 0.4861\n",
            "\u001b[1m  18/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8392 - loss: 0.3338   \n",
            "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.7238 - loss: 0.8706\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
            "\u001b[1m 574/2641\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7433 - loss: 0.4992\u001b[32m [repeated 84x across cluster]\u001b[0m\n",
            "\u001b[1m 451/2454\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7980 - loss: 0.4188\n",
            "\u001b[1m1162/2641\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7413 - loss: 0.5012\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m1759/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7928 - loss: 0.4214\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m2117/2641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7408 - loss: 0.5015\u001b[32m [repeated 91x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m  12/2641\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7283 - loss: 0.5131  \n",
            "\u001b[1m   1/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:08\u001b[0m 77ms/step - accuracy: 0.7812 - loss: 0.3907\n",
            "\u001b[1m  24/2641\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7396 - loss: 0.5024\n",
            "\u001b[1m  14/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7973 - loss: 0.3811\n",
            "\u001b[1m  49/2641\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - accuracy: 0.7397 - loss: 0.5050\n",
            "\u001b[1m  32/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.8096 - loss: 0.3797\n",
            "\u001b[1m  52/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.8124 - loss: 0.3838\n",
            "\u001b[1m2641/2641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7410 - loss: 0.5011\n",
            "\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7925 - loss: 0.4206\u001b[32m [repeated 38x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m 826/2641\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7421 - loss: 0.4920\u001b[32m [repeated 85x across cluster]\u001b[0m\n",
            "\u001b[1m 367/2641\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7386 - loss: 0.4978\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[1m 885/2454\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.8011 - loss: 0.4016\n",
            "\u001b[1m1708/2641\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7428 - loss: 0.4903\u001b[32m [repeated 46x across cluster]\u001b[0m\n",
            "\u001b[1m2267/2641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7432 - loss: 0.4899\n",
            "\u001b[1m2316/2641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7433 - loss: 0.4898\n",
            "\u001b[1m2309/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7999 - loss: 0.4020\n",
            "\u001b[1m 945/2641\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7423 - loss: 0.4915\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m 979/2641\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7424 - loss: 0.4914\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m  16/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7834 - loss: 0.4279   \n",
            "\u001b[1m 580/2641\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7467 - loss: 0.4777\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[1m 986/2454\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7990 - loss: 0.3984\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m1042/2454\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7989 - loss: 0.3984\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m  18/2641\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7423 - loss: 0.4874   \n",
            "\u001b[1m1620/2641\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7456 - loss: 0.4789\u001b[32m [repeated 89x across cluster]\u001b[0m\n",
            "\u001b[1m2138/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7986 - loss: 0.3978\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[1m2223/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7986 - loss: 0.3978\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m  16/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7928 - loss: 0.3985   \n",
            "\u001b[1m 199/2641\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7463 - loss: 0.4764\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[1m2400/2641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7457 - loss: 0.4793\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m  19/2641\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7736 - loss: 0.4362   \n",
            "\u001b[1m1805/2641\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7510 - loss: 0.4771\u001b[32m [repeated 98x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  6 Training complete...\n",
            "\u001b[1m2083/2641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7508 - loss: 0.4772\n",
            "\u001b[1m2100/2641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7508 - loss: 0.4772\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 17\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  17 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \r\u001b[1m   1/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44:02\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 3.5724\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  11/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - accuracy: 0.0428 - loss: 3.5898       \n",
            "\u001b[1m  29/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.1003 - loss: 3.4509\n",
            "\u001b[1m  49/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - accuracy: 0.1585 - loss: 3.3184\n",
            "\u001b[1m  67/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.2075 - loss: 3.2067\n",
            "\u001b[1m  85/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.2483 - loss: 3.0983\n",
            "\u001b[1m 104/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.2858 - loss: 2.9847\n",
            "\u001b[1m 114/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.3034 - loss: 2.9258\n",
            "\u001b[1m 135/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.3362 - loss: 2.8059\n",
            "\u001b[1m 158/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.3663 - loss: 2.6834\n",
            "\u001b[1m 177/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.3872 - loss: 2.5913\n",
            "\u001b[1m 197/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.4063 - loss: 2.5028\n",
            "\u001b[1m 216/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.4223 - loss: 2.4252\n",
            "\u001b[1m 234/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.4360 - loss: 2.3572\n",
            "\u001b[1m 254/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.4497 - loss: 2.2874\n",
            "\u001b[1m 275/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.4625 - loss: 2.2200\n",
            "\u001b[1m2641/2641\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7505 - loss: 0.4773\u001b[32m [repeated 37x across cluster]\u001b[0m\n",
            "\u001b[1m 288/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.4698 - loss: 2.1811\n",
            "\u001b[1m 308/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.4802 - loss: 2.1251\n",
            "\u001b[1m 326/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.4888 - loss: 2.0782\n",
            "\u001b[1m 342/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.4960 - loss: 2.0391\n",
            "\u001b[1m 361/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5040 - loss: 1.9955\n",
            "\u001b[1m 378/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5106 - loss: 1.9589\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  25 Training complete...\n",
            "\u001b[1m 401/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5190 - loss: 1.9128\n",
            "\u001b[1m 424/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.5266 - loss: 1.8700\n",
            "\u001b[1m 443/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.5325 - loss: 1.8369\n",
            "\u001b[1m 462/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.5381 - loss: 1.8056\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 23\n",
            "\u001b[1m 479/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5429 - loss: 1.7791\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  23 Training...\n",
            "\u001b[1m 495/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5471 - loss: 1.7552\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m 512/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5514 - loss: 1.7309\n",
            "\u001b[1m 533/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5564 - loss: 1.7025\n",
            "\u001b[1m 553/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5609 - loss: 1.6768\n",
            "\u001b[1m 571/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5647 - loss: 1.6548\n",
            "\u001b[1m 585/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5676 - loss: 1.6382\n",
            "\u001b[1m 605/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5715 - loss: 1.6156\n",
            "\u001b[1m 615/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5734 - loss: 1.6047\n",
            "\u001b[1m 633/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5767 - loss: 1.5856\n",
            "\u001b[1m 653/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5801 - loss: 1.5654\n",
            "\u001b[1m 671/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5831 - loss: 1.5479\n",
            "\u001b[1m 686/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5855 - loss: 1.5338\n",
            "\u001b[1m 703/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5881 - loss: 1.5185\n",
            "\u001b[1m 731/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5921 - loss: 1.4943\n",
            "\u001b[1m   1/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:51:25\u001b[0m 4s/step - accuracy: 0.0938 - loss: 3.4134\n",
            "\u001b[1m 749/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5946 - loss: 1.4794\n",
            "\u001b[1m  17/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.0897 - loss: 3.4628\n",
            "\u001b[1m 760/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5961 - loss: 1.4706\n",
            "\u001b[1m 246/2650\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.4314 - loss: 2.2934\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
            "\u001b[1m 634/2650\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.5640 - loss: 1.5716\u001b[32m [repeated 27x across cluster]\u001b[0m\n",
            "\u001b[1m1661/2650\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6455 - loss: 1.0771\n",
            "\u001b[1m 279/2650\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.4524 - loss: 2.1879 \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 43ms/step - accuracy: 0.8125 - loss: 0.3218\n",
            "\u001b[1m  31/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7458 - loss: 0.4707\n",
            "\u001b[1m2253/2650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6634 - loss: 0.9675\u001b[32m [repeated 95x across cluster]\u001b[0m\n",
            "\u001b[1m2650/2650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.6717 - loss: 0.9170\n",
            "\u001b[1m 415/2650\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7415 - loss: 0.5043\n",
            "\u001b[1m1188/2616\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7452 - loss: 0.5037\n",
            "\u001b[1m 558/2650\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7404 - loss: 0.5064\n",
            "\u001b[1m 566/2650\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7404 - loss: 0.5065\n",
            "\u001b[1m 573/2650\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7403 - loss: 0.5066\n",
            "\u001b[1m 581/2650\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7402 - loss: 0.5067\n",
            "\u001b[1m 589/2650\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7402 - loss: 0.5067\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 45ms/step - accuracy: 0.8438 - loss: 0.4226\n",
            "\u001b[1m  32/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7428 - loss: 0.4838\n",
            "\u001b[1m 812/2650\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7390 - loss: 0.5081\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[1m2001/2616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7464 - loss: 0.5025\n",
            "\u001b[1m1309/2650\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7386 - loss: 0.5080\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[1m2171/2616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7468 - loss: 0.5020\n",
            "\u001b[1m1831/2650\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7389 - loss: 0.5072\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 46ms/step - accuracy: 0.7812 - loss: 0.4287\n",
            "\u001b[1m  37/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7584 - loss: 0.4824\n",
            "\u001b[1m  15/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7380 - loss: 0.4614   \n",
            "\u001b[1m2242/2650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7392 - loss: 0.5065\n",
            "\u001b[1m2616/2616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7476 - loss: 0.5008\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m1169/2616\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7542 - loss: 0.4815\n",
            "\u001b[1m 772/2650\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7398 - loss: 0.4968\u001b[32m [repeated 89x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m2398/2616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7543 - loss: 0.4826\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 42ms/step - accuracy: 0.6875 - loss: 0.4903\n",
            "\u001b[1m2616/2616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7543 - loss: 0.4826\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[1m  38/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7299 - loss: 0.4951\n",
            "\u001b[1m 179/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7427 - loss: 0.4868\n",
            "\u001b[1m 186/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7431 - loss: 0.4865\n",
            "\u001b[1m 193/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7434 - loss: 0.4863\n",
            "\u001b[1m 207/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7442 - loss: 0.4859\n",
            "\u001b[1m 223/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7450 - loss: 0.4854\n",
            "\u001b[1m 238/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7458 - loss: 0.4850\n",
            "\u001b[1m 246/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7462 - loss: 0.4847\n",
            "\u001b[1m 259/2616\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7468 - loss: 0.4842\n",
            "\u001b[1m 274/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7475 - loss: 0.4837\n",
            "\u001b[1m 288/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7481 - loss: 0.4832\n",
            "\u001b[1m 304/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7486 - loss: 0.4827\n",
            "\u001b[1m 316/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7489 - loss: 0.4824\n",
            "\u001b[1m2163/2650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7420 - loss: 0.4959\u001b[32m [repeated 79x across cluster]\u001b[0m\n",
            "\u001b[1m 330/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7494 - loss: 0.4820\n",
            "\u001b[1m 342/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7497 - loss: 0.4817\n",
            "\u001b[1m 356/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7501 - loss: 0.4814\n",
            "\u001b[1m 372/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7505 - loss: 0.4811\n",
            "\u001b[1m 386/2616\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7509 - loss: 0.4809\n",
            "\u001b[1m 402/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7512 - loss: 0.4806\n",
            "\u001b[1m 410/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7514 - loss: 0.4805\n",
            "\u001b[1m 424/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7516 - loss: 0.4802\n",
            "\u001b[1m 431/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7518 - loss: 0.4801\n",
            "\u001b[1m 438/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7519 - loss: 0.4800\n",
            "\u001b[1m 450/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7521 - loss: 0.4798\n",
            "\u001b[1m 465/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7524 - loss: 0.4796\n",
            "\u001b[1m 480/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7526 - loss: 0.4794\n",
            "\u001b[1m 495/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7528 - loss: 0.4792\n",
            "\u001b[1m 508/2616\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7529 - loss: 0.4791\n",
            "\u001b[1m 526/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7531 - loss: 0.4789\n",
            "\u001b[1m 548/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7533 - loss: 0.4787\n",
            "\u001b[1m 574/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7536 - loss: 0.4785\n",
            "\u001b[1m 589/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7537 - loss: 0.4784\n",
            "\u001b[1m 620/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7539 - loss: 0.4781\n",
            "\u001b[1m 633/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7540 - loss: 0.4780\n",
            "\u001b[1m 652/2616\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7541 - loss: 0.4780\n",
            "\u001b[1m 667/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7542 - loss: 0.4779\n",
            "\u001b[1m 681/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7542 - loss: 0.4779\n",
            "\u001b[1m 696/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7543 - loss: 0.4778\n",
            "\u001b[1m 709/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7544 - loss: 0.4778\n",
            "\u001b[1m 722/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7544 - loss: 0.4778\n",
            "\u001b[1m 738/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7545 - loss: 0.4778\n",
            "\u001b[1m 754/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7545 - loss: 0.4777\n",
            "\u001b[1m 760/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7546 - loss: 0.4777\n",
            "\u001b[1m 775/2616\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7546 - loss: 0.4777\n",
            "\u001b[1m 790/2616\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7547 - loss: 0.4777\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m 808/2616\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7547 - loss: 0.4777\n",
            "\u001b[1m   1/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:08\u001b[0m 116ms/step - accuracy: 0.7188 - loss: 0.4487\n",
            "\u001b[1m2650/2650\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7422 - loss: 0.4955\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m 836/2616\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7548 - loss: 0.4776\n",
            "\u001b[1m  18/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7383 - loss: 0.4525\n",
            "\u001b[1m 850/2616\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7549 - loss: 0.4776\n",
            "\u001b[1m  56/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7455 - loss: 0.4495\n",
            "\u001b[1m  64/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7462 - loss: 0.4504\n",
            "\u001b[1m 155/2650\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7484 - loss: 0.4625\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "\u001b[1m1187/2616\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7560 - loss: 0.4763\u001b[32m [repeated 37x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m 446/2650\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7461 - loss: 0.4751 \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[1m  16/2616\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7568 - loss: 0.4859   \n",
            "\u001b[1m1940/2650\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7453 - loss: 0.4838\u001b[32m [repeated 94x across cluster]\u001b[0m\n",
            "\u001b[1m  66/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7515 - loss: 0.4851\n",
            "\u001b[1m  81/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7503 - loss: 0.4849\n",
            "\u001b[1m  95/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7498 - loss: 0.4844\n",
            "\u001b[1m 107/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7496 - loss: 0.4836\n",
            "\u001b[1m 122/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7493 - loss: 0.4829\n",
            "\u001b[1m 166/2650\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7493 - loss: 0.4809\n",
            "\u001b[1m 181/2650\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7493 - loss: 0.4802\n",
            "\u001b[1m 196/2650\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7493 - loss: 0.4797\n",
            "\u001b[1m 211/2650\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7493 - loss: 0.4793\n",
            "\u001b[1m 226/2650\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7492 - loss: 0.4790\n",
            "\u001b[1m 240/2650\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7491 - loss: 0.4788\n",
            "\u001b[1m 255/2650\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7491 - loss: 0.4787\n",
            "\u001b[1m 270/2650\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7492 - loss: 0.4785\n",
            "\u001b[1m 314/2650\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7493 - loss: 0.4785\n",
            "\u001b[1m 324/2650\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7493 - loss: 0.4786\n",
            "\u001b[1m1601/2616\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7588 - loss: 0.4702\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m  17/2650\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7645 - loss: 0.4813   \n",
            "\u001b[1m 653/2650\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7469 - loss: 0.4821\u001b[32m [repeated 84x across cluster]\u001b[0m\n",
            "\u001b[1m2073/2616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7584 - loss: 0.4702\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[1m2102/2616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7584 - loss: 0.4702\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m1497/2650\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7463 - loss: 0.4822\u001b[32m [repeated 89x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  17 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 22\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  22 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n",
            "\u001b[1m   1/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15:59\u001b[0m 2s/step - accuracy: 0.0312 - loss: 3.4791\n",
            "\u001b[1m  33/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.1383 - loss: 3.3455\n",
            "\u001b[1m2616/2616\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7582 - loss: 0.4700\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 779/2637\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5916 - loss: 1.4460\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 795/2637\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5936 - loss: 1.4340\u001b[32m [repeated 62x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  23 Training complete...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 12\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  12 Training...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m   1/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:08:39\u001b[0m 2s/step - accuracy: 0.0312 - loss: 3.3969\n",
            "\u001b[1m  30/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.1371 - loss: 3.3535\n",
            "\u001b[1m 263/2471\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.4864 - loss: 2.1697\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[1m1185/2471\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6622 - loss: 1.1580\u001b[32m [repeated 83x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:43\u001b[0m 108ms/step - accuracy: 0.6562 - loss: 0.6517\n",
            "\u001b[1m2637/2637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.6781 - loss: 0.9159\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[1m  16/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7033 - loss: 0.5505\n",
            "\u001b[1m  34/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - accuracy: 0.7296 - loss: 0.5290\n",
            "\u001b[1m  50/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - accuracy: 0.7354 - loss: 0.5199\n",
            "\u001b[1m  64/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7367 - loss: 0.5160\n",
            "\u001b[1m  70/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7366 - loss: 0.5152\n",
            "\u001b[1m  89/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7367 - loss: 0.5125\n",
            "\u001b[1m  96/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7369 - loss: 0.5111\n",
            "\u001b[1m 117/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7380 - loss: 0.5081\n",
            "\u001b[1m 130/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7386 - loss: 0.5065\n",
            "\u001b[1m 154/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7394 - loss: 0.5046\n",
            "\u001b[1m 164/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7397 - loss: 0.5040\n",
            "\u001b[1m 191/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7404 - loss: 0.5031\n",
            "\u001b[1m 198/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7405 - loss: 0.5029\n",
            "\u001b[1m1723/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6880 - loss: 0.9978\n",
            "\u001b[1m 209/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7408 - loss: 0.5026\n",
            "\u001b[1m 228/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7413 - loss: 0.5022\n",
            "\u001b[1m 244/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7417 - loss: 0.5020\n",
            "\u001b[1m 259/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7420 - loss: 0.5018\n",
            "\u001b[1m 276/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7422 - loss: 0.5016\n",
            "\u001b[1m 290/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7423 - loss: 0.5015\n",
            "\u001b[1m 308/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7425 - loss: 0.5012\n",
            "\u001b[1m 324/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7425 - loss: 0.5011\n",
            "\u001b[1m 339/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7425 - loss: 0.5010\n",
            "\u001b[1m 352/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7425 - loss: 0.5009\n",
            "\u001b[1m 364/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7425 - loss: 0.5009\n",
            "\u001b[1m 381/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7425 - loss: 0.5009\n",
            "\u001b[1m1907/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6941 - loss: 0.9600\u001b[32m [repeated 61x across cluster]\u001b[0m\n",
            "\u001b[1m 393/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7425 - loss: 0.5008\n",
            "\u001b[1m 409/2637\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7426 - loss: 0.5007\n",
            "\u001b[1m 430/2637\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7427 - loss: 0.5006\n",
            "\u001b[1m 457/2637\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7428 - loss: 0.5004\n",
            "\u001b[1m 489/2637\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7430 - loss: 0.5001\n",
            "\u001b[1m 527/2637\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7432 - loss: 0.4998\n",
            "\u001b[1m 558/2637\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7434 - loss: 0.4995\n",
            "\u001b[1m 591/2637\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7436 - loss: 0.4992\n",
            "\u001b[1m 618/2637\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7438 - loss: 0.4990\n",
            "\u001b[1m 649/2637\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7440 - loss: 0.4987\n",
            "\u001b[1m 680/2637\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7442 - loss: 0.4984\n",
            "\u001b[1m 713/2637\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7443 - loss: 0.4982\n",
            "\u001b[1m 743/2637\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7444 - loss: 0.4981\n",
            "\u001b[1m 777/2637\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7445 - loss: 0.4980\n",
            "\u001b[1m  33/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7736 - loss: 0.4325\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 46ms/step - accuracy: 0.8125 - loss: 0.3591\n",
            "\u001b[1m2471/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7079 - loss: 0.8736\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[1m1355/2637\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7450 - loss: 0.4975\n",
            "\u001b[1m 859/2471\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7790 - loss: 0.4465\u001b[32m [repeated 77x across cluster]\u001b[0m\n",
            "\u001b[1m2266/2637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7447 - loss: 0.4968\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m  17/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7173 - loss: 0.5329   \n",
            "\u001b[1m1993/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7781 - loss: 0.4462\n",
            "\u001b[1m2407/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7779 - loss: 0.4456\u001b[32m [repeated 93x across cluster]\u001b[0m\n",
            "\u001b[1m  21/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7606 - loss: 0.4789\n",
            "\u001b[1m  36/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7630 - loss: 0.4718\n",
            "\u001b[1m  49/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7651 - loss: 0.4667\n",
            "\u001b[1m  64/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7677 - loss: 0.4610\n",
            "\u001b[1m  79/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7704 - loss: 0.4555\n",
            "\u001b[1m  94/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7724 - loss: 0.4511\n",
            "\u001b[1m 108/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7735 - loss: 0.4483\n",
            "\u001b[1m 122/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7741 - loss: 0.4467\n",
            "\u001b[1m 129/2471\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7743 - loss: 0.4459\n",
            "\u001b[1m 138/2471\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7745 - loss: 0.4450\n",
            "\u001b[1m 151/2471\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7748 - loss: 0.4437\n",
            "\u001b[1m 162/2471\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7750 - loss: 0.4429\n",
            "\u001b[1m 176/2471\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7753 - loss: 0.4421\n",
            "\u001b[1m 189/2471\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7756 - loss: 0.4416\n",
            "\u001b[1m 202/2471\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7757 - loss: 0.4410\n",
            "\u001b[1m 212/2471\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7757 - loss: 0.4406\n",
            "\u001b[1m 230/2471\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7757 - loss: 0.4400\n",
            "\u001b[1m 249/2471\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7756 - loss: 0.4395\n",
            "\u001b[1m 264/2471\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7756 - loss: 0.4390\n",
            "\u001b[1m 991/2637\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7418 - loss: 0.4960\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m 275/2471\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7757 - loss: 0.4386\n",
            "\u001b[1m   7/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.7536 - loss: 0.5081  \n",
            "\u001b[1m 289/2471\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7757 - loss: 0.4382\n",
            "\u001b[1m 307/2471\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7758 - loss: 0.4377\n",
            "\u001b[1m 329/2471\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7758 - loss: 0.4372\n",
            "\u001b[1m 341/2471\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7759 - loss: 0.4370\n",
            "\u001b[1m 354/2471\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7759 - loss: 0.4368\n",
            "\u001b[1m 366/2471\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7759 - loss: 0.4367\n",
            "\u001b[1m 379/2471\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7759 - loss: 0.4365\n",
            "\u001b[1m 394/2471\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7759 - loss: 0.4364\n",
            "\u001b[1m 408/2471\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7760 - loss: 0.4362\n",
            "\u001b[1m 420/2471\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7760 - loss: 0.4361\n",
            "\u001b[1m 434/2471\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7760 - loss: 0.4359\n",
            "\u001b[1m 449/2471\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7761 - loss: 0.4358\n",
            "\u001b[1m 462/2471\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7762 - loss: 0.4356\n",
            "\u001b[1m 475/2471\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7762 - loss: 0.4355\n",
            "\u001b[1m 488/2471\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7763 - loss: 0.4354\n",
            "\u001b[1m 500/2471\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7764 - loss: 0.4353\n",
            "\u001b[1m 515/2471\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7764 - loss: 0.4352\n",
            "\u001b[1m 528/2471\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7765 - loss: 0.4351\n",
            "\u001b[1m 544/2471\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.7766 - loss: 0.4350\n",
            "\u001b[1m 557/2471\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.7766 - loss: 0.4348\n",
            "\u001b[1m 570/2471\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.7767 - loss: 0.4348\n",
            "\u001b[1m 592/2471\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.7768 - loss: 0.4347\n",
            "\u001b[1m 629/2471\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7770 - loss: 0.4345\n",
            "\u001b[1m 644/2471\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7770 - loss: 0.4344\n",
            "\u001b[1m1497/2637\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7429 - loss: 0.4935\u001b[32m [repeated 50x across cluster]\u001b[0m\n",
            "\u001b[1m 664/2471\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7771 - loss: 0.4343\n",
            "\u001b[1m 685/2471\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7771 - loss: 0.4341\n",
            "\u001b[1m 700/2471\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7772 - loss: 0.4341\n",
            "\u001b[1m 719/2471\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7772 - loss: 0.4340\n",
            "\u001b[1m 733/2471\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7772 - loss: 0.4339\n",
            "\u001b[1m 747/2471\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7773 - loss: 0.4339\n",
            "\u001b[1m 763/2471\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7773 - loss: 0.4338\n",
            "\u001b[1m 777/2471\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7773 - loss: 0.4338\n",
            "\u001b[1m 799/2471\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7774 - loss: 0.4337\n",
            "\u001b[1m 833/2471\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7774 - loss: 0.4336\n",
            "\u001b[1m 870/2471\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7775 - loss: 0.4334\n",
            "\u001b[1m 897/2471\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7776 - loss: 0.4333\n",
            "\u001b[1m 928/2471\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7776 - loss: 0.4333\n",
            "\u001b[1m 956/2471\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7776 - loss: 0.4332\n",
            "\u001b[1m2161/2637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7442 - loss: 0.4911\n",
            "\u001b[1m1490/2471\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7779 - loss: 0.4327\n",
            "\u001b[1m2626/2637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7448 - loss: 0.4898\n",
            "\u001b[1m2637/2637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7449 - loss: 0.4898\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 42ms/step - accuracy: 0.8125 - loss: 0.4532\n",
            "\u001b[1m  32/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7490 - loss: 0.4862 \n",
            "\u001b[1m2056/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7783 - loss: 0.4325\u001b[32m [repeated 78x across cluster]\u001b[0m\n",
            "\u001b[1m 170/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7369 - loss: 0.4905\n",
            "\u001b[1m  33/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7756 - loss: 0.4330\n",
            "\u001b[1m2471/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7783 - loss: 0.4326\n",
            "\u001b[1m1055/2637\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7433 - loss: 0.4843\n",
            "\u001b[1m1074/2637\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7434 - loss: 0.4842\n",
            "\u001b[1m 913/2637\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7425 - loss: 0.4848\n",
            "\u001b[1m1179/2637\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7439 - loss: 0.4838\n",
            "\u001b[1m1194/2637\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7439 - loss: 0.4837\n",
            "\u001b[1m1211/2637\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7440 - loss: 0.4837\n",
            "\u001b[1m1227/2637\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7441 - loss: 0.4836\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 44ms/step - accuracy: 0.7500 - loss: 0.4019\n",
            "\u001b[1m1086/2471\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7794 - loss: 0.4268\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[1m1626/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7798 - loss: 0.4275\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[1m1893/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7799 - loss: 0.4276\u001b[32m [repeated 89x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:21\u001b[0m 99ms/step - accuracy: 0.8438 - loss: 0.3828\n",
            "\u001b[1m  20/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7613 - loss: 0.4600\n",
            "\u001b[1m  36/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - accuracy: 0.7542 - loss: 0.4670\n",
            "\u001b[1m  45/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - accuracy: 0.7538 - loss: 0.4666\n",
            "\u001b[1m  54/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7519 - loss: 0.4679\n",
            "\u001b[1m  65/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7498 - loss: 0.4704\n",
            "\u001b[1m  81/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7477 - loss: 0.4723\n",
            "\u001b[1m 116/2637\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7461 - loss: 0.4742\n",
            "\u001b[1m 151/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7463 - loss: 0.4756\n",
            "\u001b[1m 181/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7471 - loss: 0.4759\n",
            "\u001b[1m 208/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7478 - loss: 0.4758\n",
            "\u001b[1m 232/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7482 - loss: 0.4757\n",
            "\u001b[1m 258/2637\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7487 - loss: 0.4756\n",
            "\u001b[1m 280/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7492 - loss: 0.4754\n",
            "\u001b[1m 305/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7498 - loss: 0.4752\n",
            "\u001b[1m 331/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7502 - loss: 0.4751\n",
            "\u001b[1m 354/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7506 - loss: 0.4751\n",
            "\u001b[1m 383/2637\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7511 - loss: 0.4750 \n",
            "\u001b[1m2547/2637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7473 - loss: 0.4805\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[1m  14/2471\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7969 - loss: 0.3731   \n",
            "\u001b[1m 511/2471\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7841 - loss: 0.4158\u001b[32m [repeated 76x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m 947/2471\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7832 - loss: 0.4171\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[1m1777/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7824 - loss: 0.4190\u001b[32m [repeated 95x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  22 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 16\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  16 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n",
            "\u001b[1m2637/2637\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7526 - loss: 0.4753\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \r\u001b[1m   1/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:42:20\u001b[0m 4s/step - accuracy: 0.0625 - loss: 3.5149\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2456/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7823 - loss: 0.4197\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2466/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7823 - loss: 0.4197\u001b[32m [repeated 40x across cluster]\u001b[0m\n",
            "\u001b[1m  19/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.0815 - loss: 3.4624\n",
            "\u001b[1m  39/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.1369 - loss: 3.3617\n",
            "\u001b[1m  56/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.1828 - loss: 3.2656\n",
            "\u001b[1m  72/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.2238 - loss: 3.1722\n",
            "\u001b[1m  87/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.2581 - loss: 3.0837\n",
            "\u001b[1m 103/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.2899 - loss: 2.9879\n",
            "\u001b[1m 118/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.3157 - loss: 2.8996\n",
            "\u001b[1m 140/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.3480 - loss: 2.7768\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  12 Training complete...\n",
            "\u001b[1m 157/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.3691 - loss: 2.6890\n",
            "\u001b[1m 170/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.3836 - loss: 2.6257\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 10\n",
            "\u001b[1m 187/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.4008 - loss: 2.5480\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  10 Training...\n",
            "\u001b[1m 195/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.4083 - loss: 2.5133\n",
            "\u001b[1m 227/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4346 - loss: 2.3854\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m 236/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4411 - loss: 2.3523\n",
            "\u001b[1m 251/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4514 - loss: 2.2999\n",
            "\u001b[1m 275/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4662 - loss: 2.2226\n",
            "\u001b[1m 291/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4752 - loss: 2.1751\n",
            "\u001b[1m 307/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4836 - loss: 2.1303\n",
            "\u001b[1m 325/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.4923 - loss: 2.0830\n",
            "\u001b[1m  14/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.0699 - loss: 3.4554       \n",
            "\u001b[1m 353/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.5047 - loss: 2.0152\n",
            "\u001b[1m2471/2471\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7823 - loss: 0.4197\n",
            "\u001b[1m 172/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.3683 - loss: 2.5830\n",
            "\u001b[1m 430/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.5176 - loss: 1.8428\n",
            "\u001b[1m 446/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.5225 - loss: 1.8157\n",
            "\u001b[1m 461/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.5268 - loss: 1.7914\n",
            "\u001b[1m 912/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6016 - loss: 1.3585\u001b[32m [repeated 41x across cluster]\u001b[0m\n",
            "\u001b[1m 554/2602\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5633 - loss: 1.6743\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[1m 587/2602\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.5699 - loss: 1.6345\n",
            "\u001b[1m1490/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6396 - loss: 1.1261\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:27\u001b[0m 57ms/step - accuracy: 0.8125 - loss: 0.3858\n",
            "\u001b[1m  33/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7424 - loss: 0.5258\n",
            "\u001b[1m2411/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6672 - loss: 0.9532\u001b[32m [repeated 91x across cluster]\u001b[0m\n",
            "\u001b[1m2657/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - accuracy: 0.6719 - loss: 0.9236\n",
            "\u001b[1m  16/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4905   \n",
            "\u001b[1m 341/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7421 - loss: 0.5059\n",
            "\u001b[1m 355/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7420 - loss: 0.5060\n",
            "\u001b[1m 369/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7419 - loss: 0.5060\n",
            "\u001b[1m 381/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7419 - loss: 0.5061\n",
            "\u001b[1m 395/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7418 - loss: 0.5062\n",
            "\u001b[1m 407/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7417 - loss: 0.5063\n",
            "\u001b[1m 421/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7415 - loss: 0.5065\n",
            "\u001b[1m 433/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7415 - loss: 0.5066\n",
            "\u001b[1m 446/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7413 - loss: 0.5066\n",
            "\u001b[1m 457/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7413 - loss: 0.5067\n",
            "\u001b[1m 463/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7412 - loss: 0.5068\n",
            "\u001b[1m 477/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7411 - loss: 0.5069\n",
            "\u001b[1m 804/2602\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7520 - loss: 0.5047\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m 494/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7410 - loss: 0.5070\n",
            "\u001b[1m 510/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7409 - loss: 0.5071\n",
            "\u001b[1m 523/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7408 - loss: 0.5072\n",
            "\u001b[1m 536/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7408 - loss: 0.5073\n",
            "\u001b[1m 552/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7407 - loss: 0.5074\n",
            "\u001b[1m 569/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7407 - loss: 0.5076\n",
            "\u001b[1m 584/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7406 - loss: 0.5077\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m 598/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7406 - loss: 0.5078\n",
            "\u001b[1m 613/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7405 - loss: 0.5079\n",
            "\u001b[1m 624/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7405 - loss: 0.5080\n",
            "\u001b[1m 637/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7404 - loss: 0.5082\n",
            "\u001b[1m 653/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7404 - loss: 0.5083\n",
            "\u001b[1m 665/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7403 - loss: 0.5084\n",
            "\u001b[1m 678/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7403 - loss: 0.5085\n",
            "\u001b[1m1181/2602\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7518 - loss: 0.5041\u001b[32m [repeated 59x across cluster]\u001b[0m\n",
            "\u001b[1m 690/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7403 - loss: 0.5086\n",
            "\u001b[1m 701/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7402 - loss: 0.5087\n",
            "\u001b[1m 708/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7402 - loss: 0.5087\n",
            "\u001b[1m 726/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7401 - loss: 0.5089\n",
            "\u001b[1m 734/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7400 - loss: 0.5090\n",
            "\u001b[1m 756/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7399 - loss: 0.5093\n",
            "\u001b[1m 763/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7399 - loss: 0.5094\n",
            "\u001b[1m 777/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7398 - loss: 0.5095\n",
            "\u001b[1m1156/2602\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7518 - loss: 0.5041\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m 790/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7398 - loss: 0.5096\n",
            "\u001b[1m 801/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7397 - loss: 0.5097\n",
            "\u001b[1m 819/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7397 - loss: 0.5098\n",
            "\u001b[1m 831/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7396 - loss: 0.5099\n",
            "\u001b[1m 848/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7396 - loss: 0.5100\n",
            "\u001b[1m 860/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7396 - loss: 0.5101\n",
            "\u001b[1m 873/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7396 - loss: 0.5102\n",
            "\u001b[1m 887/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5103\n",
            "\u001b[1m 902/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5104\n",
            "\u001b[1m 922/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5105\n",
            "\u001b[1m 930/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5105\n",
            "\u001b[1m 940/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5106\n",
            "\u001b[1m 952/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5106\n",
            "\u001b[1m 962/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5107\n",
            "\u001b[1m 973/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5108\n",
            "\u001b[1m 985/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5108\n",
            "\u001b[1m1009/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5109\n",
            "\u001b[1m1033/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5110\n",
            "\u001b[1m1058/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5111\n",
            "\u001b[1m1077/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5112\n",
            "\u001b[1m1089/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5112\n",
            "\u001b[1m1100/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5113\n",
            "\u001b[1m1116/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5113\n",
            "\u001b[1m1144/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5114\n",
            "\u001b[1m1188/2602\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7518 - loss: 0.5040\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m1629/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 6ms/step - accuracy: 0.7396 - loss: 0.5121\u001b[32m [repeated 65x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 46ms/step - accuracy: 0.7500 - loss: 0.6600\n",
            "\u001b[1m  29/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7594 - loss: 0.5212\n",
            "\u001b[1m 199/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7601 - loss: 0.4912\n",
            "\u001b[1m 225/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7589 - loss: 0.4912 \n",
            "\u001b[1m2425/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7397 - loss: 0.5115\n",
            "\u001b[1m2564/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7397 - loss: 0.5114\n",
            "\u001b[1m2538/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7397 - loss: 0.5115\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[1m  27/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7843 - loss: 0.4636\n",
            "\u001b[1m  36/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7774 - loss: 0.4715\n",
            "\u001b[1m 779/2602\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7550 - loss: 0.4873\u001b[32m [repeated 76x across cluster]\u001b[0m\n",
            "\u001b[1m 463/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7527 - loss: 0.4877\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 47ms/step - accuracy: 0.8125 - loss: 0.3786\n",
            "\u001b[1m 436/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7532 - loss: 0.4872\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
            "\u001b[1m2657/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7397 - loss: 0.5114\n",
            "\u001b[1m1001/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7493 - loss: 0.4915\n",
            "\u001b[1m1109/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7489 - loss: 0.4921\u001b[32m [repeated 84x across cluster]\u001b[0m\n",
            "\u001b[1m1716/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7544 - loss: 0.4854\n",
            "\u001b[1m1939/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7545 - loss: 0.4850\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[1m1961/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 6ms/step - accuracy: 0.7471 - loss: 0.4939\u001b[32m [repeated 87x across cluster]\u001b[0m\n",
            "\u001b[1m1247/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7484 - loss: 0.4926\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m  16/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7709 - loss: 0.4608   \n",
            "\u001b[1m   1/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 40ms/step - accuracy: 0.8438 - loss: 0.3280\n",
            "\u001b[1m  35/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7542 - loss: 0.4986\n",
            "\u001b[1m 785/2602\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7505 - loss: 0.4817\n",
            "\u001b[1m 849/2602\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7505 - loss: 0.4815\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m 822/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7481 - loss: 0.4913\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m2349/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7534 - loss: 0.4793\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m2063/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7459 - loss: 0.4919\u001b[32m [repeated 94x across cluster]\u001b[0m\n",
            "\u001b[1m2114/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7458 - loss: 0.4920\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:57\u001b[0m 114ms/step - accuracy: 0.7812 - loss: 0.4208\n",
            "\u001b[1m  15/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7566 - loss: 0.4586\n",
            "\u001b[1m  35/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7668 - loss: 0.4508\n",
            "\u001b[1m  48/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7650 - loss: 0.4530\n",
            "\u001b[1m  60/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7632 - loss: 0.4555\n",
            "\u001b[1m  82/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - accuracy: 0.7624 - loss: 0.4582\n",
            "\u001b[1m  96/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7621 - loss: 0.4601\n",
            "\u001b[1m 110/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7620 - loss: 0.4612\n",
            "\u001b[1m 125/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7624 - loss: 0.4614\n",
            "\u001b[1m 138/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7627 - loss: 0.4617\n",
            "\u001b[1m 159/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7633 - loss: 0.4616\n",
            "\u001b[1m 173/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7634 - loss: 0.4616\n",
            "\u001b[1m 190/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7635 - loss: 0.4614\n",
            "\u001b[1m 205/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7635 - loss: 0.4612\n",
            "\u001b[1m 217/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7635 - loss: 0.4611\n",
            "\u001b[1m 232/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7636 - loss: 0.4610\n",
            "\u001b[1m 245/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7636 - loss: 0.4609\n",
            "\u001b[1m 259/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7637 - loss: 0.4609\n",
            "\u001b[1m 273/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7636 - loss: 0.4609\n",
            "\u001b[1m 287/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7635 - loss: 0.4610\n",
            "\u001b[1m 302/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7635 - loss: 0.4610\n",
            "\u001b[1m 317/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7636 - loss: 0.4611\n",
            "\u001b[1m 334/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7636 - loss: 0.4611\n",
            "\u001b[1m 348/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7636 - loss: 0.4610\n",
            "\u001b[1m 355/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7636 - loss: 0.4610\n",
            "\u001b[1m 391/2602\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7638 - loss: 0.4607\n",
            "\u001b[1m 420/2602\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7638 - loss: 0.4606\n",
            "\u001b[1m 443/2602\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7639 - loss: 0.4605\n",
            "\u001b[1m 456/2602\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7639 - loss: 0.4605\n",
            "\u001b[1m 469/2602\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7640 - loss: 0.4605\n",
            "\u001b[1m 483/2602\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7640 - loss: 0.4605\n",
            "\u001b[1m 499/2602\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7640 - loss: 0.4606\n",
            "\u001b[1m 514/2602\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7639 - loss: 0.4607\n",
            "\u001b[1m 530/2602\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7639 - loss: 0.4608\n",
            "\u001b[1m2146/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7458 - loss: 0.4920\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[1m 540/2602\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7639 - loss: 0.4608\n",
            "\u001b[1m 557/2602\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7639 - loss: 0.4609\n",
            "\u001b[1m 573/2602\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7638 - loss: 0.4610\n",
            "\u001b[1m  34/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7554 - loss: 0.4720 \n",
            "\u001b[1m 598/2602\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7638 - loss: 0.4612\n",
            "\u001b[1m 625/2602\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7638 - loss: 0.4614\n",
            "\u001b[1m 655/2602\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7637 - loss: 0.4617\n",
            "\u001b[1m 684/2602\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7636 - loss: 0.4620\n",
            "\u001b[1m 718/2602\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7636 - loss: 0.4623\n",
            "\u001b[1m 747/2602\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7635 - loss: 0.4626\n",
            "\u001b[1m 230/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7496 - loss: 0.4825\n",
            "\u001b[1m 187/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7503 - loss: 0.4807\u001b[32m [repeated 43x across cluster]\u001b[0m\n",
            "\u001b[1m 785/2602\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7634 - loss: 0.4629\n",
            "\u001b[1m 818/2602\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7633 - loss: 0.4631\n",
            "\u001b[1m 841/2602\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7632 - loss: 0.4633 \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:52\u001b[0m 88ms/step - accuracy: 0.8438 - loss: 0.3993\n",
            "\u001b[1m 944/2602\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7630 - loss: 0.4639\n",
            "\u001b[1m 548/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7464 - loss: 0.4875\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m1657/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7472 - loss: 0.4889\u001b[32m [repeated 88x across cluster]\u001b[0m\n",
            "\u001b[1m 534/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7464 - loss: 0.4875\n",
            "\u001b[1m1923/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7472 - loss: 0.4887\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  16 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 7\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  7 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n",
            "\u001b[1m   1/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19:31\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 3.6655\n",
            "\u001b[1m  29/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.0993 - loss: 3.4227\n",
            "\u001b[1m  45/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - accuracy: 0.1468 - loss: 3.3263\n",
            "\u001b[1m  58/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.1870 - loss: 3.2466\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  74/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.2291 - loss: 3.1444\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  89/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.2617 - loss: 3.0503\n",
            "\u001b[1m 120/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.3165 - loss: 2.8646\n",
            "\u001b[1m 152/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.3593 - loss: 2.6917\n",
            "\u001b[1m2602/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7608 - loss: 0.4689\n",
            "\u001b[1m 180/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.3892 - loss: 2.5564\n",
            "\u001b[1m 216/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.4207 - loss: 2.4041\n",
            "\u001b[1m 229/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.4303 - loss: 2.3547\n",
            "\u001b[1m 244/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.4406 - loss: 2.3009\n",
            "\u001b[1m 263/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.4526 - loss: 2.2377\n",
            "\u001b[1m 283/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.4640 - loss: 2.1764\n",
            "\u001b[1m 299/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.4724 - loss: 2.1309\n",
            "\u001b[1m 319/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.4822 - loss: 2.0779\n",
            "\u001b[1m 338/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.4907 - loss: 2.0311\n",
            "\u001b[1m  40/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - accuracy: 0.1313 - loss: 3.3562\u001b[32m [repeated 43x across cluster]\u001b[0m\n",
            "\u001b[1m 354/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.4973 - loss: 1.9943\n",
            "\u001b[1m 372/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.5043 - loss: 1.9551\n",
            "\u001b[1m 391/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.5112 - loss: 1.9162\n",
            "\u001b[1m 412/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.5183 - loss: 1.8760\n",
            "\u001b[1m 430/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.5240 - loss: 1.8436\n",
            "\u001b[1m 451/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.5302 - loss: 1.8080\n",
            "\u001b[1m 468/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.5350 - loss: 1.7809\n",
            "\u001b[1m 487/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.5399 - loss: 1.7521\n",
            "\u001b[1m 512/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5460 - loss: 1.7166\n",
            "\u001b[1m 531/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5502 - loss: 1.6913\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  10 Training complete...\n",
            "\u001b[1m 550/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5543 - loss: 1.6672\n",
            "\u001b[1m 572/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5588 - loss: 1.6406\n",
            "\u001b[1m 590/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5622 - loss: 1.6198\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 2\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  2 Training...\n",
            "\u001b[1m 609/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5657 - loss: 1.5989\n",
            "\u001b[1m 628/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5691 - loss: 1.5788\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m 648/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5725 - loss: 1.5586\n",
            "\u001b[1m 667/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5755 - loss: 1.5402\n",
            "\u001b[1m 683/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5780 - loss: 1.5253\n",
            "\u001b[1m 699/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5804 - loss: 1.5109\n",
            "\u001b[1m 715/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5827 - loss: 1.4970\n",
            "\u001b[1m 736/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5856 - loss: 1.4794\n",
            "\u001b[1m 752/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5878 - loss: 1.4665\n",
            "\u001b[1m 768/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.5898 - loss: 1.4540 \n",
            "\u001b[1m   9/2428\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.0496 - loss: 3.5199       \n",
            "\u001b[1m 832/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.5976 - loss: 1.4075\n",
            "\u001b[1m 145/2428\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.3905 - loss: 2.6934\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[1m1135/2642\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.6247 - loss: 1.2435\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
            "\u001b[1m 418/2428\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5705 - loss: 1.8045\n",
            "\u001b[1m 461/2428\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5838 - loss: 1.7292\n",
            "\u001b[1m 563/2428\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.6087 - loss: 1.5840\n",
            "\u001b[1m 661/2428\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.6267 - loss: 1.4765\n",
            "\u001b[1m 611/2428\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.6181 - loss: 1.5280\n",
            "\u001b[1m 530/2428\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.6015 - loss: 1.6266\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m  16/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7053 - loss: 0.5510   \n",
            "\u001b[1m1775/2428\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7067 - loss: 0.9755\n",
            "\u001b[1m1844/2428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7090 - loss: 0.9609\u001b[32m [repeated 83x across cluster]\u001b[0m\n",
            "\u001b[1m1816/2428\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7081 - loss: 0.9667\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m2428/2428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.7243 - loss: 0.8645\n",
            "\u001b[1m   1/2428\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 46ms/step - accuracy: 0.7812 - loss: 0.4609\n",
            "\u001b[1m  31/2428\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.8001 - loss: 0.4323\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m 808/2428\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.8002 - loss: 0.4120\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[1m 848/2428\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.8002 - loss: 0.4119\u001b[32m [repeated 91x across cluster]\u001b[0m\n",
            "\u001b[1m 990/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7356 - loss: 0.5112\n",
            "\u001b[1m1926/2642\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7369 - loss: 0.5111\n",
            "\u001b[1m1974/2642\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7370 - loss: 0.5111\n",
            "\u001b[1m1475/2428\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.8000 - loss: 0.4096\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[1m1587/2428\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7999 - loss: 0.4093\u001b[32m [repeated 86x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:18\u001b[0m 53ms/step - accuracy: 0.5625 - loss: 0.7265\n",
            "\u001b[1m  31/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7362 - loss: 0.5179\n",
            "\u001b[1m 686/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7468 - loss: 0.4962\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m 569/2428\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7978 - loss: 0.3906\u001b[32m [repeated 92x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2428\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 44ms/step - accuracy: 0.7812 - loss: 0.3590\n",
            "\u001b[1m  34/2428\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7972 - loss: 0.3707\n",
            "\u001b[1m1554/2642\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7456 - loss: 0.4984\n",
            "\u001b[1m1603/2642\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7455 - loss: 0.4985\n",
            "\u001b[1m1915/2428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.8012 - loss: 0.3900\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m2176/2428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8014 - loss: 0.3901\u001b[32m [repeated 93x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m  19/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7227 - loss: 0.5021   \n",
            "\u001b[1m 235/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7477 - loss: 0.4824\n",
            "\u001b[1m 249/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7479 - loss: 0.4827\n",
            "\u001b[1m 264/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7480 - loss: 0.4830\n",
            "\u001b[1m 279/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7482 - loss: 0.4832\n",
            "\u001b[1m 298/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7483 - loss: 0.4836\n",
            "\u001b[1m 314/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7483 - loss: 0.4839\n",
            "\u001b[1m 333/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7484 - loss: 0.4843\n",
            "\u001b[1m 115/2428\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7958 - loss: 0.3864\n",
            "\u001b[1m 682/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7488 - loss: 0.4860\n",
            "\u001b[1m2428/2428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.8015 - loss: 0.3903\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m2422/2428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8015 - loss: 0.3903\u001b[32m [repeated 21x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   7/2428\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7922 - loss: 0.3512   \n",
            "\u001b[1m 757/2428\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.8026 - loss: 0.3863 \u001b[32m [repeated 78x across cluster]\u001b[0m\n",
            "\u001b[1m 706/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7488 - loss: 0.4860\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m1972/2428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.8038 - loss: 0.3853\u001b[32m [repeated 76x across cluster]\u001b[0m\n",
            "\u001b[1m1018/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7489 - loss: 0.4866 \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m2642/2642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7478 - loss: 0.4882\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 43ms/step - accuracy: 0.7500 - loss: 0.4877\n",
            "\u001b[1m  33/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7573 - loss: 0.4855\n",
            "\u001b[1m  18/2428\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7983 - loss: 0.4043   \n",
            "\u001b[1m1090/2428\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.8083 - loss: 0.3787\u001b[32m [repeated 94x across cluster]\u001b[0m\n",
            "\u001b[1m1276/2642\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7473 - loss: 0.4883\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m1446/2428\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8076 - loss: 0.3789\n",
            "\u001b[1m1499/2428\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.8075 - loss: 0.3789\n",
            "\u001b[1m1878/2428\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.8071 - loss: 0.3789\u001b[32m [repeated 88x across cluster]\u001b[0m\n",
            "\u001b[1m1900/2642\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7470 - loss: 0.4887\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  2 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 30\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  30 Training...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \r\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:24:16\u001b[0m 2s/step - accuracy: 0.0312 - loss: 3.5539\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  18/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.0820 - loss: 3.4419    \n",
            "\u001b[1m2610/2642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7470 - loss: 0.4886\u001b[32m [repeated 49x across cluster]\u001b[0m\n",
            "\u001b[1m2642/2642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7470 - loss: 0.4886\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  7 Training complete...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 9\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  9 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n",
            "\u001b[1m  15/2633\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.0641 - loss: 3.5105        \n",
            "\u001b[1m1553/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6461 - loss: 1.1062\u001b[32m [repeated 87x across cluster]\u001b[0m\n",
            "\u001b[1m1575/2633\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6458 - loss: 1.1056\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[1m2383/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6701 - loss: 0.9542\u001b[32m [repeated 92x across cluster]\u001b[0m\n",
            "\u001b[1m2015/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6614 - loss: 1.0091\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m  16/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7276 - loss: 0.5494   \n",
            "\u001b[1m  75/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7341 - loss: 0.5290 \n",
            "\u001b[1m   1/2633\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:09\u001b[0m 49ms/step - accuracy: 0.8438 - loss: 0.3854\n",
            "\u001b[1m  29/2633\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7655 - loss: 0.4759 \n",
            "\u001b[1m 110/2633\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7431 - loss: 0.5093\n",
            "\u001b[1m 996/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7417 - loss: 0.5145\u001b[32m [repeated 89x across cluster]\u001b[0m\n",
            "\u001b[1m 630/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7424 - loss: 0.5141\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m2438/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7411 - loss: 0.5133\u001b[32m [repeated 96x across cluster]\u001b[0m\n",
            "\u001b[1m2458/2633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7344 - loss: 0.5194\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 48ms/step - accuracy: 0.7188 - loss: 0.5533\n",
            "\u001b[1m  31/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7189 - loss: 0.5345\n",
            "\u001b[1m  98/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7312 - loss: 0.5148\n",
            "\u001b[1m 113/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7334 - loss: 0.5133\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7411 - loss: 0.5131\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m 136/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7360 - loss: 0.5119\n",
            "\u001b[1m 156/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7375 - loss: 0.5114\n",
            "\u001b[1m 178/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7388 - loss: 0.5105\n",
            "\u001b[1m   7/2633\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 0.8305 - loss: 0.4031   \n",
            "\u001b[1m 196/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7396 - loss: 0.5101\n",
            "\u001b[1m 269/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7413 - loss: 0.5087\n",
            "\u001b[1m 394/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7420 - loss: 0.5071\n",
            "\u001b[1m2629/2633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7346 - loss: 0.5191\u001b[32m [repeated 20x across cluster]\u001b[0m\n",
            "\u001b[1m 481/2633\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7500 - loss: 0.4953\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m 818/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7417 - loss: 0.5044\u001b[32m [repeated 80x across cluster]\u001b[0m\n",
            "\u001b[1m2633/2633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7346 - loss: 0.5190\n",
            "\u001b[1m1065/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7416 - loss: 0.5034\n",
            "\u001b[1m 488/2633\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7499 - loss: 0.4954\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m1889/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7420 - loss: 0.5025\u001b[32m [repeated 47x across cluster]\u001b[0m\n",
            "\u001b[1m1960/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7420 - loss: 0.5025\n",
            "\u001b[1m1002/2633\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7466 - loss: 0.4986\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
            "\u001b[1m2225/2633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7440 - loss: 0.4999\n",
            "\u001b[1m1034/2633\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7465 - loss: 0.4986\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 42ms/step - accuracy: 0.7188 - loss: 0.5210\n",
            "\u001b[1m  29/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7285 - loss: 0.5199\n",
            "\u001b[1m  20/2633\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7181 - loss: 0.5090   \n",
            "\u001b[1m 749/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7426 - loss: 0.5000\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[1m 499/2633\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7351 - loss: 0.4970\n",
            "\u001b[1m 570/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7415 - loss: 0.5014\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m1382/2633\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7377 - loss: 0.4952\n",
            "\u001b[1m1726/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7461 - loss: 0.4956\u001b[32m [repeated 92x across cluster]\u001b[0m\n",
            "\u001b[1m1807/2633\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7385 - loss: 0.4947\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m2481/2633\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7393 - loss: 0.4947\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 45ms/step - accuracy: 0.8125 - loss: 0.3660\n",
            "\u001b[1m  29/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7538 - loss: 0.4804 \n",
            "\u001b[1m  20/2633\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7096 - loss: 0.5357   \n",
            "\u001b[1m 148/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7520 - loss: 0.4880\u001b[32m [repeated 87x across cluster]\u001b[0m\n",
            "\u001b[1m 624/2633\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7461 - loss: 0.4940\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m1569/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7484 - loss: 0.4912\u001b[32m [repeated 95x across cluster]\u001b[0m\n",
            "\u001b[1m1641/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7484 - loss: 0.4911\n",
            "\u001b[1m1815/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7483 - loss: 0.4910\n",
            "\u001b[1m1778/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7483 - loss: 0.4910\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[1m2550/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7483 - loss: 0.4903\u001b[32m [repeated 88x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  9 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 8\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  8 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7482 - loss: 0.4902\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[1m  17/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.0719 - loss: 3.4857    \n",
            "\u001b[1m   1/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:31:50\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 3.5777\n",
            "\u001b[1m  31/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.1230 - loss: 3.3736\n",
            "\u001b[1m 330/2404\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.4954 - loss: 1.9931\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  30 Training complete...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 4\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  4 Training...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m 557/2404\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5692 - loss: 1.6036\n",
            "\u001b[1m1884/2404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6805 - loss: 0.9836\u001b[32m [repeated 97x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m  19/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8060 - loss: 0.4242   \n",
            "\u001b[1m  35/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8016 - loss: 0.4296\n",
            "\u001b[1m 115/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7800 - loss: 0.4589\n",
            "\u001b[1m   1/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 46ms/step - accuracy: 0.6562 - loss: 0.5409\n",
            "\u001b[1m  36/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7285 - loss: 0.5423\n",
            "\u001b[1m 263/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7366 - loss: 0.5290\n",
            "\u001b[1m 284/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7370 - loss: 0.5281\n",
            "\u001b[1m 298/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7372 - loss: 0.5275\n",
            "\u001b[1m 314/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7374 - loss: 0.5268\n",
            "\u001b[1m 326/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7376 - loss: 0.5264\n",
            "\u001b[1m 344/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7377 - loss: 0.5257\n",
            "\u001b[1m 362/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7378 - loss: 0.5251\n",
            "\u001b[1m 375/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7380 - loss: 0.5246\n",
            "\u001b[1m 388/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7380 - loss: 0.5242\n",
            "\u001b[1m 401/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7381 - loss: 0.5238\n",
            "\u001b[1m 416/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7382 - loss: 0.5233\n",
            "\u001b[1m 429/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7383 - loss: 0.5230\n",
            "\u001b[1m 446/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7384 - loss: 0.5226\n",
            "\u001b[1m 469/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7386 - loss: 0.5221\n",
            "\u001b[1m 484/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7386 - loss: 0.5218\n",
            "\u001b[1m 693/2404\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7717 - loss: 0.4595\u001b[32m [repeated 74x across cluster]\u001b[0m\n",
            "\u001b[1m 499/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7387 - loss: 0.5215\n",
            "\u001b[1m 515/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7388 - loss: 0.5213\n",
            "\u001b[1m 531/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7389 - loss: 0.5210\n",
            "\u001b[1m 547/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7390 - loss: 0.5207\n",
            "\u001b[1m 564/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7391 - loss: 0.5205\n",
            "\u001b[1m 579/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7392 - loss: 0.5203\n",
            "\u001b[1m 593/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7392 - loss: 0.5202\n",
            "\u001b[1m 608/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7393 - loss: 0.5200\n",
            "\u001b[1m 630/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7394 - loss: 0.5198\n",
            "\u001b[1m 641/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7394 - loss: 0.5197\n",
            "\u001b[1m 664/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7395 - loss: 0.5194\n",
            "\u001b[1m 672/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7396 - loss: 0.5193\n",
            "\u001b[1m 695/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7397 - loss: 0.5190\n",
            "\u001b[1m 725/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7398 - loss: 0.5187\n",
            "\u001b[1m 736/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7399 - loss: 0.5185\n",
            "\u001b[1m 753/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7399 - loss: 0.5184\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m 764/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7399 - loss: 0.5183\n",
            "\u001b[1m 778/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7399 - loss: 0.5182\n",
            "\u001b[1m 907/2404\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7722 - loss: 0.4588\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m 790/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7399 - loss: 0.5181\n",
            "\u001b[1m 812/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7399 - loss: 0.5180\n",
            "\u001b[1m 823/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7399 - loss: 0.5179\n",
            "\u001b[1m 835/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7399 - loss: 0.5179\n",
            "\u001b[1m 851/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7398 - loss: 0.5178\n",
            "\u001b[1m 866/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7398 - loss: 0.5178\n",
            "\u001b[1m 880/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7398 - loss: 0.5177\n",
            "\u001b[1m 896/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7398 - loss: 0.5176\n",
            "\u001b[1m 908/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7398 - loss: 0.5176\n",
            "\u001b[1m 928/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7398 - loss: 0.5175\n",
            "\u001b[1m 940/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7397 - loss: 0.5174\n",
            "\u001b[1m 946/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7397 - loss: 0.5174\n",
            "\u001b[1m 968/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7397 - loss: 0.5173\n",
            "\u001b[1m 985/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7397 - loss: 0.5173\n",
            "\u001b[1m1004/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7397 - loss: 0.5172\n",
            "\u001b[1m1023/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7396 - loss: 0.5171\n",
            "\u001b[1m1194/2404\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.7727 - loss: 0.4585\n",
            "\u001b[1m1646/2404\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7733 - loss: 0.4581\u001b[32m [repeated 59x across cluster]\u001b[0m\n",
            "\u001b[1m1942/2642\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7395 - loss: 0.5154\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 49ms/step - accuracy: 0.8438 - loss: 0.3637\n",
            "\u001b[1m  28/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.8083 - loss: 0.4160\n",
            "\u001b[1m 664/2404\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7794 - loss: 0.4500\u001b[32m [repeated 92x across cluster]\u001b[0m\n",
            "\u001b[1m2642/2642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7396 - loss: 0.5149\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 45ms/step - accuracy: 0.7188 - loss: 0.4867\n",
            "\u001b[1m1189/2642\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7376 - loss: 0.5046\n",
            "\u001b[1m  35/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7086 - loss: 0.5111\n",
            "\u001b[1m1282/2642\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7379 - loss: 0.5045\n",
            "\u001b[1m1441/2642\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7382 - loss: 0.5044\n",
            "\u001b[1m1952/2404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7784 - loss: 0.4483\u001b[32m [repeated 87x across cluster]\u001b[0m\n",
            "\u001b[1m2200/2404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7782 - loss: 0.4481\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   5/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 13ms/step - accuracy: 0.8194 - loss: 0.3600 \n",
            "\u001b[1m  17/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.8089 - loss: 0.3765\n",
            "\u001b[1m  29/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 10ms/step - accuracy: 0.7985 - loss: 0.3930\n",
            "\u001b[1m  44/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.7917 - loss: 0.4077\n",
            "\u001b[1m  57/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.7872 - loss: 0.4154\n",
            "\u001b[1m  72/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7825 - loss: 0.4229\n",
            "\u001b[1m  85/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7800 - loss: 0.4267\n",
            "\u001b[1m 114/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7776 - loss: 0.4299\n",
            "\u001b[1m 145/2404\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7767 - loss: 0.4323\n",
            "\u001b[1m 178/2404\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7765 - loss: 0.4338\n",
            "\u001b[1m 206/2404\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7765 - loss: 0.4348\n",
            "\u001b[1m 239/2404\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7764 - loss: 0.4359\n",
            "\u001b[1m 271/2404\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7762 - loss: 0.4368\n",
            "\u001b[1m 305/2404\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7761 - loss: 0.4377 \n",
            "\u001b[1m 369/2404\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7759 - loss: 0.4391\u001b[32m [repeated 77x across cluster]\u001b[0m\n",
            "\u001b[1m   1/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:44\u001b[0m 39ms/step - accuracy: 0.6875 - loss: 0.8647\n",
            "\u001b[1m  33/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7104 - loss: 0.5593\n",
            "\u001b[1m 582/2404\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7756 - loss: 0.4416\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m1404/2642\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7403 - loss: 0.4979\u001b[32m [repeated 95x across cluster]\u001b[0m\n",
            "\u001b[1m1497/2642\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7407 - loss: 0.4977\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m  14/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7593 - loss: 0.4527   \n",
            "\u001b[1m1550/2642\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7408 - loss: 0.4975\n",
            "\u001b[1m  75/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7518 - loss: 0.5003\n",
            "\u001b[1m  98/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7504 - loss: 0.5018\n",
            "\u001b[1m 785/2404\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7811 - loss: 0.4314\n",
            "\u001b[1m 112/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7501 - loss: 0.5020\n",
            "\u001b[1m 125/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7496 - loss: 0.5024\n",
            "\u001b[1m 138/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7494 - loss: 0.5024\n",
            "\u001b[1m 151/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7495 - loss: 0.5023\n",
            "\u001b[1m 167/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7495 - loss: 0.5023\n",
            "\u001b[1m 856/2404\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7812 - loss: 0.4317\u001b[32m [repeated 85x across cluster]\u001b[0m\n",
            "\u001b[1m 180/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7497 - loss: 0.5020\n",
            "\u001b[1m 196/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7499 - loss: 0.5017\n",
            "\u001b[1m 211/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7499 - loss: 0.5015\n",
            "\u001b[1m 226/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7499 - loss: 0.5013\n",
            "\u001b[1m 232/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7499 - loss: 0.5013\n",
            "\u001b[1m 247/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7497 - loss: 0.5013\n",
            "\u001b[1m 262/2642\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7496 - loss: 0.5012\n",
            "\u001b[1m 276/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7496 - loss: 0.5010\n",
            "\u001b[1m 287/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7495 - loss: 0.5008\n",
            "\u001b[1m 979/2404\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7813 - loss: 0.4323\n",
            "\u001b[1m 305/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7495 - loss: 0.5004\n",
            "\u001b[1m1004/2404\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7814 - loss: 0.4324\n",
            "\u001b[1m 322/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7496 - loss: 0.5000\n",
            "\u001b[1m1012/2404\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7814 - loss: 0.4324\n",
            "\u001b[1m 339/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7496 - loss: 0.4996\n",
            "\u001b[1m 352/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7495 - loss: 0.4994\n",
            "\u001b[1m 370/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7495 - loss: 0.4992\n",
            "\u001b[1m 383/2642\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4991\n",
            "\u001b[1m 397/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4989\n",
            "\u001b[1m 409/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4987\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m  15/2642\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7513 - loss: 0.5256   \n",
            "\u001b[1m 423/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4985\n",
            "\u001b[1m 436/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4983\n",
            "\u001b[1m 451/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4981\n",
            "\u001b[1m 466/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4979\n",
            "\u001b[1m 480/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4977\n",
            "\u001b[1m 499/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4975\n",
            "\u001b[1m 520/2642\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4973\n",
            "\u001b[1m 536/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4972\n",
            "\u001b[1m 554/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4971\n",
            "\u001b[1m 570/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7494 - loss: 0.4971\n",
            "\u001b[1m 585/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7493 - loss: 0.4970\n",
            "\u001b[1m 596/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7493 - loss: 0.4970\n",
            "\u001b[1m 611/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7492 - loss: 0.4970\n",
            "\u001b[1m 624/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7492 - loss: 0.4970\n",
            "\u001b[1m 645/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7491 - loss: 0.4969\n",
            "\u001b[1m 660/2642\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7490 - loss: 0.4969\n",
            "\u001b[1m 675/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7490 - loss: 0.4968\n",
            "\u001b[1m 701/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7489 - loss: 0.4967\n",
            "\u001b[1m 710/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7489 - loss: 0.4967\n",
            "\u001b[1m 724/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7489 - loss: 0.4966\n",
            "\u001b[1m 742/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7489 - loss: 0.4965\n",
            "\u001b[1m 763/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7489 - loss: 0.4964\n",
            "\u001b[1m1415/2404\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7814 - loss: 0.4338\n",
            "\u001b[1m 777/2642\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7489 - loss: 0.4964\n",
            "\u001b[1m 801/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7488 - loss: 0.4963\n",
            "\u001b[1m 816/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7488 - loss: 0.4963\n",
            "\u001b[1m 835/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7488 - loss: 0.4963\n",
            "\u001b[1m 852/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7487 - loss: 0.4962\n",
            "\u001b[1m 869/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7487 - loss: 0.4962\n",
            "\u001b[1m 892/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7487 - loss: 0.4962\n",
            "\u001b[1m 923/2642\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7487 - loss: 0.4961\n",
            "\u001b[1m 953/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7486 - loss: 0.4961\n",
            "\u001b[1m1597/2404\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7814 - loss: 0.4341\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
            "\u001b[1m 982/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7486 - loss: 0.4960\n",
            "\u001b[1m1013/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7486 - loss: 0.4960\n",
            "\u001b[1m1046/2642\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7485 - loss: 0.4959\n",
            "\u001b[1m1842/2404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7813 - loss: 0.4344\n",
            "\u001b[1m1860/2404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7813 - loss: 0.4345\n",
            "\u001b[1m1877/2404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7813 - loss: 0.4345\n",
            "\u001b[1m1406/2642\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7479 - loss: 0.4957\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  4 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 27\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  27 Training...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m  17/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.0633 - loss: 3.4970    \n",
            "\u001b[1m2410/2642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7474 - loss: 0.4941\u001b[32m [repeated 66x across cluster]\u001b[0m\n",
            "\u001b[1m2642/2642\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7474 - loss: 0.4937\n",
            "\u001b[1m1808/2642\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7476 - loss: 0.4950\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m 233/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.4184 - loss: 2.3725\n",
            "\u001b[1m 264/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.4393 - loss: 2.2675 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  8 Training complete...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 0\n",
            "\u001b[1m  62/2429\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.1705 - loss: 3.2572\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  0 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n",
            "\u001b[1m 556/2429\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.5264 - loss: 1.7305\n",
            "\u001b[1m  18/2429\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.0589 - loss: 3.5212        \n",
            "\u001b[1m1400/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6333 - loss: 1.1656\u001b[32m [repeated 70x across cluster]\u001b[0m\n",
            "\u001b[1m1428/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6348 - loss: 1.1569\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m1987/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6566 - loss: 1.0257\n",
            "\u001b[1m2087/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.6595 - loss: 1.0083\u001b[32m [repeated 86x across cluster]\u001b[0m\n",
            "\u001b[1m2008/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6573 - loss: 1.0220\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m  16/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7651 - loss: 0.4884   \n",
            "\u001b[1m  71/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7491 - loss: 0.5124 \n",
            "\u001b[1m   1/2429\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 42ms/step - accuracy: 0.7812 - loss: 0.4354\n",
            "\u001b[1m  48/2429\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7150 - loss: 0.5737\n",
            "\u001b[1m 102/2429\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7142 - loss: 0.5689\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m 953/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7389 - loss: 0.5169\u001b[32m [repeated 87x across cluster]\u001b[0m\n",
            "\u001b[1m 881/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7390 - loss: 0.5174\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m2439/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7385 - loss: 0.5123\u001b[32m [repeated 96x across cluster]\u001b[0m\n",
            "\u001b[1m1816/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7384 - loss: 0.5136\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m2598/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7385 - loss: 0.5121\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5:56\u001b[0m 134ms/step - accuracy: 0.8125 - loss: 0.3975\n",
            "\u001b[1m  17/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7595 - loss: 0.4545\n",
            "\u001b[1m  29/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7578 - loss: 0.4639\n",
            "\u001b[1m  48/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7549 - loss: 0.4738\n",
            "\u001b[1m  65/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7521 - loss: 0.4803\n",
            "\u001b[1m  78/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7513 - loss: 0.4826\n",
            "\u001b[1m  95/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7507 - loss: 0.4838\n",
            "\u001b[1m 109/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7499 - loss: 0.4847\n",
            "\u001b[1m 125/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7488 - loss: 0.4858\n",
            "\u001b[1m 138/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7480 - loss: 0.4867\n",
            "\u001b[1m 159/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7466 - loss: 0.4882\n",
            "\u001b[1m 174/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7460 - loss: 0.4891\n",
            "\u001b[1m 186/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7456 - loss: 0.4897\n",
            "\u001b[1m 200/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7451 - loss: 0.4904\n",
            "\u001b[1m 215/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7445 - loss: 0.4912\n",
            "\u001b[1m 235/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7439 - loss: 0.4919\n",
            "\u001b[1m 249/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7436 - loss: 0.4922\n",
            "\u001b[1m 263/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7434 - loss: 0.4924\n",
            "\u001b[1m  36/2429\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7044 - loss: 0.5791\n",
            "\u001b[1m 304/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7430 - loss: 0.4929\n",
            "\u001b[1m2416/2429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7133 - loss: 0.5582\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
            "\u001b[1m2429/2429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7133 - loss: 0.5582\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m 579/2429\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7147 - loss: 0.5526\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2429\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:08\u001b[0m 103ms/step - accuracy: 0.7500 - loss: 0.5572\n",
            "\u001b[1m  18/2429\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7035 - loss: 0.5921\n",
            "\u001b[1m 548/2429\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7145 - loss: 0.5528\u001b[32m [repeated 47x across cluster]\u001b[0m\n",
            "\u001b[1m 659/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7425 - loss: 0.4936\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m1993/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7430 - loss: 0.4947\u001b[32m [repeated 68x across cluster]\u001b[0m\n",
            "\u001b[1m1108/2429\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7160 - loss: 0.5504\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m  15/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.6999 - loss: 0.5493   \n",
            "\u001b[1m   1/2429\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 41ms/step - accuracy: 0.6875 - loss: 0.5543\n",
            "\u001b[1m  34/2429\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6931 - loss: 0.5633\n",
            "\u001b[1m 914/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7459 - loss: 0.4911\u001b[32m [repeated 96x across cluster]\u001b[0m\n",
            "\u001b[1m 790/2429\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7200 - loss: 0.5426\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m1552/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7465 - loss: 0.4907\n",
            "\u001b[1m1742/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7465 - loss: 0.4908\u001b[32m [repeated 89x across cluster]\u001b[0m\n",
            "\u001b[1m1437/2429\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7204 - loss: 0.5421\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m  28/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7545 - loss: 0.5005 \n",
            "\u001b[1m   1/2429\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 53ms/step - accuracy: 0.6562 - loss: 0.5691\n",
            "\u001b[1m  21/2429\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7193 - loss: 0.5248\n",
            "\u001b[1m  92/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7397 - loss: 0.5046\n",
            "\u001b[1m 400/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7388 - loss: 0.4984\u001b[32m [repeated 87x across cluster]\u001b[0m\n",
            "\u001b[1m2429/2429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7208 - loss: 0.5408\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[1m 609/2429\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7265 - loss: 0.5307\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m 123/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7373 - loss: 0.5048 \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m1941/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7431 - loss: 0.4921\u001b[32m [repeated 93x across cluster]\u001b[0m\n",
            "\u001b[1m1910/2429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m  3ms/step - accuracy: 0.7252 - loss: 0.5303\n",
            "\u001b[1m1996/2429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7252 - loss: 0.5303\n",
            "\u001b[1m1953/2429\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7252 - loss: 0.5303\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m2238/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7437 - loss: 0.4914\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  0 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 20\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  20 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \r\u001b[1m   1/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:19:48\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 3.6103\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2641/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7443 - loss: 0.4906\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2653/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7443 - loss: 0.4905\u001b[32m [repeated 62x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7443 - loss: 0.4905\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[1m  28/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.0837 - loss: 3.4280 \n",
            "\u001b[1m  18/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.0641 - loss: 3.4832        \n",
            "\u001b[1m 235/2648\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.4395 - loss: 2.3155\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  27 Training complete...\n",
            "\u001b[1m 364/2648\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.5077 - loss: 1.9576\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 24\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  24 Training...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m 511/2648\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.5527 - loss: 1.7047\n",
            "\u001b[1m1390/2648\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6450 - loss: 1.1408\u001b[32m [repeated 81x across cluster]\u001b[0m\n",
            "\u001b[1m1260/2648\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6382 - loss: 1.1842\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[1m1019/2653\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6166 - loss: 1.3008\n",
            "\u001b[1m2567/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6751 - loss: 0.9273\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 42ms/step - accuracy: 0.7500 - loss: 0.5810\n",
            "\u001b[1m  18/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7447 - loss: 0.5311   \n",
            "\u001b[1m  29/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7547 - loss: 0.4881\n",
            "\u001b[1m  53/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7464 - loss: 0.5207\n",
            "\u001b[1m  48/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7475 - loss: 0.4940\u001b[32m [repeated 85x across cluster]\u001b[0m\n",
            "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.6766 - loss: 0.9173\n",
            "\u001b[1m 120/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7389 - loss: 0.5013\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m 752/2648\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7423 - loss: 0.5068\u001b[32m [repeated 94x across cluster]\u001b[0m\n",
            "\u001b[1m 388/2648\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7422 - loss: 0.5086\n",
            "\u001b[1m1067/2648\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7427 - loss: 0.5063\n",
            "\u001b[1m1099/2648\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7427 - loss: 0.5062\n",
            "\u001b[1m1848/2648\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7432 - loss: 0.5058\n",
            "\u001b[1m1070/2653\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7377 - loss: 0.5099 \u001b[32m [repeated 24x across cluster]\u001b[0m\n",
            "\u001b[1m2560/2648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7438 - loss: 0.5047\u001b[32m [repeated 96x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m  15/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7314 - loss: 0.4915   \n",
            "\u001b[1m  31/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7431 - loss: 0.4887\n",
            "\u001b[1m 621/2648\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7446 - loss: 0.4882\n",
            "\u001b[1m 582/2648\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7446 - loss: 0.4880\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m 172/2653\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7455 - loss: 0.4840 \u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[1m1139/2648\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7444 - loss: 0.4899\u001b[32m [repeated 79x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   6/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 11ms/step - accuracy: 0.7535 - loss: 0.5038 \n",
            "\u001b[1m1426/2648\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7446 - loss: 0.4901\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[1m1681/2648\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7447 - loss: 0.4902\n",
            "\u001b[1m1924/2648\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7447 - loss: 0.4902\u001b[32m [repeated 87x across cluster]\u001b[0m\n",
            "\u001b[1m2126/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7453 - loss: 0.4908\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 44ms/step - accuracy: 0.6875 - loss: 0.6232\n",
            "\u001b[1m  28/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7301 - loss: 0.5012\n",
            "\u001b[1m  57/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7337 - loss: 0.4966\n",
            "\u001b[1m  49/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7426 - loss: 0.4729\n",
            "\u001b[1m  77/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7425 - loss: 0.4768\n",
            "\u001b[1m1853/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7453 - loss: 0.4905\n",
            "\u001b[1m 108/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7421 - loss: 0.4808 \n",
            "\u001b[1m 652/2648\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7423 - loss: 0.4927\u001b[32m [repeated 85x across cluster]\u001b[0m\n",
            "\u001b[1m1295/2648\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7453 - loss: 0.4887\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 42ms/step - accuracy: 0.7812 - loss: 0.4260\n",
            "\u001b[1m  26/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7441 - loss: 0.4713\n",
            "\u001b[1m1653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7456 - loss: 0.4864\n",
            "\u001b[1m2046/2648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7470 - loss: 0.4864\u001b[32m [repeated 91x across cluster]\u001b[0m\n",
            "\u001b[1m2080/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7462 - loss: 0.4860\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[1m1880/2648\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7467 - loss: 0.4867\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   8/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.7377 - loss: 0.5238  \n",
            "\u001b[1m  22/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.7380 - loss: 0.5076\n",
            "\u001b[1m  33/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 0.7404 - loss: 0.4987\n",
            "\u001b[1m  46/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 0.7436 - loss: 0.4906\n",
            "\u001b[1m2490/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7465 - loss: 0.4857\u001b[32m [repeated 86x across cluster]\u001b[0m\n",
            "\u001b[1m  59/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.7453 - loss: 0.4856\n",
            "\u001b[1m  73/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.7457 - loss: 0.4827\n",
            "\u001b[1m  90/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7455 - loss: 0.4807\n",
            "\u001b[1m 113/2648\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7450 - loss: 0.4792\n",
            "\u001b[1m 137/2648\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7447 - loss: 0.4791\n",
            "\u001b[1m 169/2648\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7446 - loss: 0.4794\n",
            "\u001b[1m 200/2648\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7449 - loss: 0.4792\n",
            "\u001b[1m 229/2648\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7452 - loss: 0.4788\n",
            "\u001b[1m   1/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:14\u001b[0m 51ms/step - accuracy: 0.7812 - loss: 0.4368\n",
            "\u001b[1m 262/2648\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7456 - loss: 0.4782\n",
            "\u001b[1m  26/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7525 - loss: 0.4721\n",
            "\u001b[1m 292/2648\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7460 - loss: 0.4778\n",
            "\u001b[1m 356/2648\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7470 - loss: 0.4770\n",
            "\u001b[1m 456/2648\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7478 - loss: 0.4769\n",
            "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7466 - loss: 0.4856\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m 420/2648\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7476 - loss: 0.4768\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m1516/2648\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7491 - loss: 0.4782\u001b[32m [repeated 77x across cluster]\u001b[0m\n",
            "\u001b[1m2251/2648\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7492 - loss: 0.4787\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m2152/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7509 - loss: 0.4788\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  24 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 31\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  31 Training...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m2572/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7508 - loss: 0.4790\u001b[32m [repeated 82x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \r\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21:52\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 3.5102\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  10/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - accuracy: 0.0522 - loss: 3.5088       \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  20/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.0685 - loss: 3.4653\n",
            "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7508 - loss: 0.4790\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m  38/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.1128 - loss: 3.3667\n",
            "\u001b[1m  53/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.1570 - loss: 3.2800\n",
            "\u001b[1m  71/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.2052 - loss: 3.1727\n",
            "\u001b[1m  87/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.2428 - loss: 3.0758\n",
            "\u001b[1m 104/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.2778 - loss: 2.9741\n",
            "\u001b[1m 118/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.3027 - loss: 2.8926\n",
            "\u001b[1m 139/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.3349 - loss: 2.7760\n",
            "\u001b[1m 154/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.3546 - loss: 2.6985\n",
            "\u001b[1m 182/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.3857 - loss: 2.5664\n",
            "\u001b[1m 203/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.4056 - loss: 2.4766\n",
            "\u001b[1m 219/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4190 - loss: 2.4135\n",
            "\u001b[1m 237/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.4326 - loss: 2.3477\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  20 Training complete...\n",
            "\u001b[1m 251/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4422 - loss: 2.2999\n",
            "\u001b[1m 265/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4510 - loss: 2.2549\n",
            "\u001b[1m 279/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4592 - loss: 2.2122\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 21\n",
            "\u001b[1m 298/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4694 - loss: 2.1580\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  21 Training...\n",
            "\u001b[1m 315/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4777 - loss: 2.1129\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n",
            "\u001b[1m 339/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.4886 - loss: 2.0538\n",
            "\u001b[1m 356/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.4956 - loss: 2.0149\n",
            "\u001b[1m 375/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.5029 - loss: 1.9741\n",
            "\u001b[1m 390/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.5084 - loss: 1.9436\n",
            "\u001b[1m 407/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.5141 - loss: 1.9108\n",
            "\u001b[1m 420/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.5183 - loss: 1.8869\n",
            "\u001b[1m2645/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7508 - loss: 0.4790\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m 434/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.5227 - loss: 1.8622\n",
            "\u001b[1m 453/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.5282 - loss: 1.8303\n",
            "\u001b[1m 459/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.5299 - loss: 1.8205\n",
            "\u001b[1m 477/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.5349 - loss: 1.7923\n",
            "\u001b[1m 839/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.5962 - loss: 1.4231\n",
            "\u001b[1m   1/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:44:12\u001b[0m 4s/step - accuracy: 0.0000e+00 - loss: 3.5766\n",
            "\u001b[1m  20/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.0789 - loss: 3.4154\n",
            "\u001b[1m 498/2653\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5351 - loss: 1.7264\u001b[32m [repeated 35x across cluster]\u001b[0m\n",
            "\u001b[1m1761/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6503 - loss: 1.0719\u001b[32m [repeated 53x across cluster]\u001b[0m\n",
            "\u001b[1m1423/2653\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.6355 - loss: 1.1377\n",
            "\u001b[1m 526/2653\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.5419 - loss: 1.6882\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 4ms/step - accuracy: 0.6715 - loss: 0.9324\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 46ms/step - accuracy: 0.7500 - loss: 0.5799\n",
            "\u001b[1m  28/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7514 - loss: 0.5336\n",
            "\u001b[1m  58/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7486 - loss: 0.5195 \n",
            "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.6723 - loss: 0.9158\n",
            "\u001b[1m  17/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.6831 - loss: 0.5388   \n",
            "\u001b[1m 580/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7427 - loss: 0.5124\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[1m 693/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7422 - loss: 0.5128\n",
            "\u001b[1m1105/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7407 - loss: 0.5129\n",
            "\u001b[1m 726/2653\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7369 - loss: 0.5124\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m1477/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7405 - loss: 0.5122\n",
            "\u001b[1m1499/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7405 - loss: 0.5122\n",
            "\u001b[1m1508/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7405 - loss: 0.5122\n",
            "\u001b[1m1571/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7405 - loss: 0.5121\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[1m1012/2653\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7372 - loss: 0.5119\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7402 - loss: 0.5111\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 41ms/step - accuracy: 0.7188 - loss: 0.3609\n",
            "\u001b[1m  30/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7302 - loss: 0.4950\n",
            "\u001b[1m 151/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7443 - loss: 0.4889\u001b[32m [repeated 94x across cluster]\u001b[0m\n",
            "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7392 - loss: 0.5080\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 41ms/step - accuracy: 0.8438 - loss: 0.3748\n",
            "\u001b[1m  35/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7402 - loss: 0.4867\n",
            "\u001b[1m1727/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7422 - loss: 0.4957\u001b[32m [repeated 96x across cluster]\u001b[0m\n",
            "\u001b[1m1898/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7423 - loss: 0.4955\n",
            "\u001b[1m1932/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7424 - loss: 0.4955\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   8/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - accuracy: 0.7416 - loss: 0.4880  \n",
            "\u001b[1m  24/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7480 - loss: 0.4785\n",
            "\u001b[1m  41/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7546 - loss: 0.4754\n",
            "\u001b[1m  51/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7558 - loss: 0.4738\n",
            "\u001b[1m2317/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7427 - loss: 0.4951\u001b[32m [repeated 86x across cluster]\u001b[0m\n",
            "\u001b[1m  59/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7562 - loss: 0.4734\n",
            "\u001b[1m  74/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7550 - loss: 0.4738\n",
            "\u001b[1m  88/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7542 - loss: 0.4738\n",
            "\u001b[1m 103/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7539 - loss: 0.4737\n",
            "\u001b[1m 114/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7537 - loss: 0.4739\n",
            "\u001b[1m 138/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7536 - loss: 0.4743\n",
            "\u001b[1m 151/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7533 - loss: 0.4747\n",
            "\u001b[1m 167/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7530 - loss: 0.4750\n",
            "\u001b[1m 174/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7527 - loss: 0.4753\n",
            "\u001b[1m 188/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7522 - loss: 0.4759\n",
            "\u001b[1m 199/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7518 - loss: 0.4764\n",
            "\u001b[1m2523/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7427 - loss: 0.4949\n",
            "\u001b[1m 217/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7513 - loss: 0.4770\n",
            "\u001b[1m 228/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7510 - loss: 0.4773\n",
            "\u001b[1m 235/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7508 - loss: 0.4775\n",
            "\u001b[1m 249/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7506 - loss: 0.4779\n",
            "\u001b[1m 256/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7505 - loss: 0.4781\n",
            "\u001b[1m 269/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7504 - loss: 0.4784\n",
            "\u001b[1m 276/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7503 - loss: 0.4786\n",
            "\u001b[1m 284/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7503 - loss: 0.4787\n",
            "\u001b[1m 302/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7501 - loss: 0.4791\n",
            "\u001b[1m 318/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7499 - loss: 0.4795\n",
            "\u001b[1m 338/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7497 - loss: 0.4799\n",
            "\u001b[1m   1/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6:12\u001b[0m 141ms/step - accuracy: 0.7812 - loss: 0.3871\n",
            "\u001b[1m 354/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7496 - loss: 0.4802\n",
            "\u001b[1m  18/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - accuracy: 0.7819 - loss: 0.4360 \n",
            "\u001b[1m 372/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7495 - loss: 0.4804\n",
            "\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7428 - loss: 0.4949\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m 326/2653\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7565 - loss: 0.4686\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m 776/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7465 - loss: 0.4853 \u001b[32m [repeated 24x across cluster]\u001b[0m\n",
            "\u001b[1m1197/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7453 - loss: 0.4873\u001b[32m [repeated 43x across cluster]\u001b[0m\n",
            "\u001b[1m1644/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7451 - loss: 0.4882\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 42ms/step - accuracy: 0.7500 - loss: 0.4034\n",
            "\u001b[1m  34/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7233 - loss: 0.4967\n",
            "\u001b[1m  66/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7289 - loss: 0.4955\u001b[32m [repeated 93x across cluster]\u001b[0m\n",
            "\u001b[1m 505/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7468 - loss: 0.4814\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m1210/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7460 - loss: 0.4834\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 44ms/step - accuracy: 0.7500 - loss: 0.4610\n",
            "\u001b[1m  32/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7317 - loss: 0.5065\n",
            "\u001b[1m1269/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7460 - loss: 0.4834\u001b[32m [repeated 89x across cluster]\u001b[0m\n",
            "\u001b[1m1588/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7460 - loss: 0.4835\u001b[32m [repeated 8x across cluster]\u001b[0m\n",
            "\u001b[1m 955/2653\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7475 - loss: 0.4859\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m2498/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7463 - loss: 0.4835\u001b[32m [repeated 93x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  31 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 18\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  18 Training...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m1975/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7483 - loss: 0.4846\n",
            "\u001b[1m  16/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.0735 - loss: 3.4535        \n",
            "\u001b[1m  74/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.2178 - loss: 3.1358\n",
            "\u001b[1m  96/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.2674 - loss: 3.0006\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 110/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.2939 - loss: 2.9167\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 118/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.3075 - loss: 2.8703\n",
            "\u001b[1m 145/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.3468 - loss: 2.7215\n",
            "\u001b[1m 176/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.3819 - loss: 2.5687\n",
            "\u001b[1m 210/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.4135 - loss: 2.4214\n",
            "\u001b[1m 246/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.4405 - loss: 2.2885\n",
            "\u001b[1m 658/2608\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.5742 - loss: 1.5463\u001b[32m [repeated 48x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  21 Training complete...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 15\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  15 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n",
            "\u001b[1m 610/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5693 - loss: 1.6078\n",
            "\u001b[1m 627/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5723 - loss: 1.5897\n",
            "\u001b[1m 676/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.5802 - loss: 1.5411\n",
            "\u001b[1m  17/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.0722 - loss: 3.4569    \n",
            "\u001b[1m 124/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.3286 - loss: 2.8413 \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m1019/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6180 - loss: 1.3046\n",
            "\u001b[1m1875/2608\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.6554 - loss: 1.0364\u001b[32m [repeated 86x across cluster]\u001b[0m\n",
            "\u001b[1m1286/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.6357 - loss: 1.1913\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:59\u001b[0m 46ms/step - accuracy: 0.7188 - loss: 0.5309\n",
            "\u001b[1m  32/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7171 - loss: 0.5478\n",
            "\u001b[1m 208/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7336 - loss: 0.5322\u001b[32m [repeated 91x across cluster]\u001b[0m\n",
            "\u001b[1m  29/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7422 - loss: 0.5057 \n",
            "\u001b[1m 106/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7358 - loss: 0.5117\n",
            "\u001b[1m  68/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7374 - loss: 0.5101\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[1m 135/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7357 - loss: 0.5127 \n",
            "\u001b[1m1422/2608\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7346 - loss: 0.5263\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 45ms/step - accuracy: 0.6875 - loss: 0.5545\n",
            "\u001b[1m1669/2608\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7347 - loss: 0.5256\u001b[32m [repeated 88x across cluster]\u001b[0m\n",
            "\u001b[1m1383/2608\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7346 - loss: 0.5264\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:02\u001b[0m 47ms/step - accuracy: 0.7188 - loss: 0.9034\n",
            "\u001b[1m  31/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7377 - loss: 0.5449\n",
            "\u001b[1m  76/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7343 - loss: 0.5252\n",
            "\u001b[1m  91/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7332 - loss: 0.5242\n",
            "\u001b[1m 110/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7327 - loss: 0.5231\n",
            "\u001b[1m 126/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7323 - loss: 0.5230\n",
            "\u001b[1m 140/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7319 - loss: 0.5232\n",
            "\u001b[1m 160/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7316 - loss: 0.5229\n",
            "\u001b[1m 177/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7317 - loss: 0.5224\n",
            "\u001b[1m 191/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7318 - loss: 0.5221\n",
            "\u001b[1m 837/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7380 - loss: 0.5178\n",
            "\u001b[1m 205/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7319 - loss: 0.5219\n",
            "\u001b[1m 217/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7321 - loss: 0.5215\n",
            "\u001b[1m 231/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7322 - loss: 0.5212\n",
            "\u001b[1m 245/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7323 - loss: 0.5208\n",
            "\u001b[1m 258/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7324 - loss: 0.5205\n",
            "\u001b[1m 271/2608\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7325 - loss: 0.5203\n",
            "\u001b[1m 285/2608\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7326 - loss: 0.5200\n",
            "\u001b[1m 297/2608\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7326 - loss: 0.5200\n",
            "\u001b[1m2233/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7395 - loss: 0.5143\u001b[32m [repeated 78x across cluster]\u001b[0m\n",
            "\u001b[1m 311/2608\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7326 - loss: 0.5199\n",
            "\u001b[1m 324/2608\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7327 - loss: 0.5199\n",
            "\u001b[1m 345/2608\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7327 - loss: 0.5197\n",
            "\u001b[1m 359/2608\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7328 - loss: 0.5195\n",
            "\u001b[1m 373/2608\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7328 - loss: 0.5193\n",
            "\u001b[1m 385/2608\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7329 - loss: 0.5192\n",
            "\u001b[1m 398/2608\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7330 - loss: 0.5190\n",
            "\u001b[1m 412/2608\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7331 - loss: 0.5189\n",
            "\u001b[1m 426/2608\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7332 - loss: 0.5188\n",
            "\u001b[1m 440/2608\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7333 - loss: 0.5186\n",
            "\u001b[1m 456/2608\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7334 - loss: 0.5184\n",
            "\u001b[1m 472/2608\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7336 - loss: 0.5182\n",
            "\u001b[1m 491/2608\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7337 - loss: 0.5179\n",
            "\u001b[1m 505/2608\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7339 - loss: 0.5176\n",
            "\u001b[1m 511/2608\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7339 - loss: 0.5175\n",
            "\u001b[1m 518/2608\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7340 - loss: 0.5174\n",
            "\u001b[1m 531/2608\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7341 - loss: 0.5171\n",
            "\u001b[1m 539/2608\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7342 - loss: 0.5170\n",
            "\u001b[1m 552/2608\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7342 - loss: 0.5168\n",
            "\u001b[1m 566/2608\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7343 - loss: 0.5166\n",
            "\u001b[1m 578/2608\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7344 - loss: 0.5164\n",
            "\u001b[1m 593/2608\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7345 - loss: 0.5162\n",
            "\u001b[1m2366/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7395 - loss: 0.5140\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m 605/2608\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7346 - loss: 0.5160\n",
            "\u001b[1m 619/2608\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7347 - loss: 0.5158\n",
            "\u001b[1m 631/2608\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7348 - loss: 0.5157\n",
            "\u001b[1m 646/2608\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7348 - loss: 0.5155\n",
            "\u001b[1m 657/2608\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7349 - loss: 0.5155\n",
            "\u001b[1m 672/2608\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7349 - loss: 0.5153\n",
            "\u001b[1m 684/2608\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7350 - loss: 0.5152\n",
            "\u001b[1m 699/2608\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7350 - loss: 0.5151\n",
            "\u001b[1m 712/2608\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7350 - loss: 0.5150\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m 726/2608\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7351 - loss: 0.5149\n",
            "\u001b[1m   1/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:42\u001b[0m 84ms/step - accuracy: 0.7812 - loss: 0.4210\n",
            "\u001b[1m 742/2608\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7351 - loss: 0.5147\n",
            "\u001b[1m  15/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - accuracy: 0.7240 - loss: 0.5092\n",
            "\u001b[1m 760/2608\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7351 - loss: 0.5146\n",
            "\u001b[1m1140/2608\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7355 - loss: 0.5127\u001b[32m [repeated 36x across cluster]\u001b[0m\n",
            "\u001b[1m1222/2608\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7356 - loss: 0.5124\n",
            "\u001b[1m 669/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7357 - loss: 0.5042\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[1m1050/2608\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7354 - loss: 0.5131 \u001b[32m [repeated 24x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m  13/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.6817 - loss: 0.6023  \n",
            "\u001b[1m1969/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7394 - loss: 0.5019\u001b[32m [repeated 88x across cluster]\u001b[0m\n",
            "\u001b[1m  74/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7192 - loss: 0.5291\n",
            "\u001b[1m 590/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7354 - loss: 0.5040\n",
            "\u001b[1m 214/2608\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7283 - loss: 0.5203\n",
            "\u001b[1m 263/2608\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7296 - loss: 0.5183\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m   1/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:03\u001b[0m 47ms/step - accuracy: 0.7500 - loss: 0.4439\n",
            "\u001b[1m  29/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7271 - loss: 0.4824\n",
            "\u001b[1m  84/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7467 - loss: 0.4760 \u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m 613/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7493 - loss: 0.4875\u001b[32m [repeated 83x across cluster]\u001b[0m\n",
            "\u001b[1m 400/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7491 - loss: 0.4858\n",
            "\u001b[1m 691/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7496 - loss: 0.4876\u001b[32m [repeated 11x across cluster]\u001b[0m\n",
            "\u001b[1m1757/2608\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7397 - loss: 0.5031\n",
            "\u001b[1m2187/2608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7401 - loss: 0.5028\u001b[32m [repeated 92x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m  15/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7427 - loss: 0.5107  \n",
            "\u001b[1m  41/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7384 - loss: 0.5113\n",
            "\u001b[1m  72/2608\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7405 - loss: 0.5055 \n",
            "\u001b[1m1764/2608\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7397 - loss: 0.5031\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m 387/2608\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7437 - loss: 0.4940\n",
            "\u001b[1m1023/2608\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7456 - loss: 0.4918\u001b[32m [repeated 93x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m  19/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7372 - loss: 0.4882   \n",
            "\u001b[1m 950/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7478 - loss: 0.4816\n",
            "\u001b[1m1686/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7489 - loss: 0.4817\n",
            "\u001b[1m1706/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7490 - loss: 0.4817\n",
            "\u001b[1m1714/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7490 - loss: 0.4817\n",
            "\u001b[1m1744/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7490 - loss: 0.4818\n",
            "\u001b[1m2410/2608\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7450 - loss: 0.4927\u001b[32m [repeated 91x across cluster]\u001b[0m\n",
            "\u001b[1m1766/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7490 - loss: 0.4818\n",
            "\u001b[1m1786/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7490 - loss: 0.4818\n",
            "\u001b[1m1793/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7490 - loss: 0.4818\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  18 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  26 Training...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m2121/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7491 - loss: 0.4821\n",
            "\u001b[1m2201/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7491 - loss: 0.4821\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m2580/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7490 - loss: 0.4827\u001b[32m [repeated 53x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \r\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:33:50\u001b[0m 3s/step - accuracy: 0.0312 - loss: 3.4693\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  16/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.0749 - loss: 3.4380    \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  15 Training complete...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 29\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  29 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n",
            "\u001b[1m 123/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.3103 - loss: 2.8704\n",
            "\u001b[1m 177/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.3774 - loss: 2.5956\n",
            "\u001b[1m 268/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.4486 - loss: 2.2481\n",
            "\u001b[1m 221/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.4163 - loss: 2.4110\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m 689/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.5744 - loss: 1.5388\n",
            "\u001b[1m1247/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6252 - loss: 1.2117\u001b[32m [repeated 59x across cluster]\u001b[0m\n",
            "\u001b[1m  17/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.0747 - loss: 3.4714        \n",
            "\u001b[1m1300/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.6283 - loss: 1.1927\n",
            "\u001b[1m2158/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6601 - loss: 0.9948\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:04\u001b[0m 47ms/step - accuracy: 0.7812 - loss: 0.4366\n",
            "\u001b[1m  24/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7518 - loss: 0.5226\n",
            "\u001b[1m  45/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7437 - loss: 0.5250\n",
            "\u001b[1m  73/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7393 - loss: 0.5230\n",
            "\u001b[1m  85/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7377 - loss: 0.5237\n",
            "\u001b[1m  93/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7370 - loss: 0.5242\n",
            "\u001b[1m2204/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.6622 - loss: 0.9905\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[1m 118/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7352 - loss: 0.5246\n",
            "\u001b[1m 130/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7349 - loss: 0.5248\n",
            "\u001b[1m 147/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7346 - loss: 0.5252\n",
            "\u001b[1m 162/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7344 - loss: 0.5255\n",
            "\u001b[1m 175/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7342 - loss: 0.5254\n",
            "\u001b[1m 190/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7342 - loss: 0.5252\n",
            "\u001b[1m 203/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7343 - loss: 0.5248\n",
            "\u001b[1m 217/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7345 - loss: 0.5242\n",
            "\u001b[1m 230/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7348 - loss: 0.5237\n",
            "\u001b[1m 242/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7351 - loss: 0.5233\n",
            "\u001b[1m 256/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7353 - loss: 0.5229\n",
            "\u001b[1m 267/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7355 - loss: 0.5226\n",
            "\u001b[1m 294/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7360 - loss: 0.5218\n",
            "\u001b[1m 301/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7362 - loss: 0.5216\n",
            "\u001b[1m 320/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7366 - loss: 0.5212\n",
            "\u001b[1m 326/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7367 - loss: 0.5210\n",
            "\u001b[1m 340/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7370 - loss: 0.5208\n",
            "\u001b[1m 357/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7372 - loss: 0.5206\n",
            "\u001b[1m 373/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7374 - loss: 0.5205\n",
            "\u001b[1m 386/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7376 - loss: 0.5205\n",
            "\u001b[1m 397/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7377 - loss: 0.5204\n",
            "\u001b[1m 411/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7379 - loss: 0.5203\n",
            "\u001b[1m 425/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7380 - loss: 0.5202\n",
            "\u001b[1m 440/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7382 - loss: 0.5200\n",
            "\u001b[1m 454/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7383 - loss: 0.5200\n",
            "\u001b[1m 469/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7385 - loss: 0.5199\n",
            "\u001b[1m 482/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7386 - loss: 0.5198\n",
            "\u001b[1m 500/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7388 - loss: 0.5196\n",
            "\u001b[1m 509/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7388 - loss: 0.5196\n",
            "\u001b[1m 527/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7390 - loss: 0.5193\n",
            "\u001b[1m 542/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7392 - loss: 0.5192\n",
            "\u001b[1m 559/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7393 - loss: 0.5190\n",
            "\u001b[1m 576/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7395 - loss: 0.5188\n",
            "\u001b[1m 589/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7396 - loss: 0.5187\n",
            "\u001b[1m 603/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7397 - loss: 0.5186\n",
            "\u001b[1m2498/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6685 - loss: 0.9503\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:55\u001b[0m 89ms/step - accuracy: 0.9062 - loss: 0.3133\n",
            "\u001b[1m  16/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - accuracy: 0.8034 - loss: 0.4045\n",
            "\u001b[1m 779/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7405 - loss: 0.5176\u001b[32m [repeated 22x across cluster]\u001b[0m\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.6715 - loss: 0.9313\u001b[32m [repeated 31x across cluster]\u001b[0m\n",
            "\u001b[1m 701/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7455 - loss: 0.5054\n",
            "\u001b[1m1734/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7405 - loss: 0.5146\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m 668/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7458 - loss: 0.5048\u001b[32m [repeated 32x across cluster]\u001b[0m\n",
            "\u001b[1m2308/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7402 - loss: 0.5140\u001b[32m [repeated 67x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 45ms/step - accuracy: 0.8438 - loss: 0.4889\n",
            "\u001b[1m2039/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7411 - loss: 0.5112\n",
            "\u001b[1m  38/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7492 - loss: 0.4834\n",
            "\u001b[1m  19/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7831 - loss: 0.4813   \n",
            "\u001b[1m 494/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7451 - loss: 0.4941\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m1180/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7432 - loss: 0.4980\u001b[32m [repeated 88x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m 479/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7379 - loss: 0.5055\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m1201/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7401 - loss: 0.5004\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m2008/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7424 - loss: 0.4994\u001b[32m [repeated 89x across cluster]\u001b[0m\n",
            "\u001b[1m1992/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7424 - loss: 0.4994\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m  16/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7841 - loss: 0.4338   \n",
            "\u001b[1m  67/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7586 - loss: 0.4768\n",
            "\u001b[1m  94/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7537 - loss: 0.4856 \n",
            "\u001b[1m 193/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7471 - loss: 0.4953\n",
            "\u001b[1m 223/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7463 - loss: 0.4961\n",
            "\u001b[1m2133/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7407 - loss: 0.4996\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[1m 575/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7442 - loss: 0.4999\u001b[32m [repeated 87x across cluster]\u001b[0m\n",
            "\u001b[1m 300/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7398 - loss: 0.5021\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m  14/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7132 - loss: 0.5401  \n",
            "\u001b[1m  47/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7350 - loss: 0.5125\n",
            "\u001b[1m1746/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7457 - loss: 0.4967\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[1m2018/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7456 - loss: 0.4964\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[1m2197/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7456 - loss: 0.4962\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:59\u001b[0m 90ms/step - accuracy: 0.7188 - loss: 0.4512\n",
            "\u001b[1m  14/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.7546 - loss: 0.4696\n",
            "\u001b[1m  26/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 9ms/step - accuracy: 0.7576 - loss: 0.4734\n",
            "\u001b[1m  40/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.7578 - loss: 0.4737\n",
            "\u001b[1m  59/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - accuracy: 0.7574 - loss: 0.4713\n",
            "\u001b[1m  71/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7565 - loss: 0.4714\n",
            "\u001b[1m  85/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7556 - loss: 0.4720\n",
            "\u001b[1m  99/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7536 - loss: 0.4739\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7455 - loss: 0.4956\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m 113/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7521 - loss: 0.4754\n",
            "\u001b[1m 131/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7506 - loss: 0.4771\n",
            "\u001b[1m 136/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7504 - loss: 0.4774\n",
            "\u001b[1m 152/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7498 - loss: 0.4783\n",
            "\u001b[1m 164/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7494 - loss: 0.4790\n",
            "\u001b[1m 177/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7492 - loss: 0.4794\n",
            "\u001b[1m2383/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7437 - loss: 0.4928\u001b[32m [repeated 76x across cluster]\u001b[0m\n",
            "\u001b[1m 200/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7490 - loss: 0.4800\n",
            "\u001b[1m 213/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7489 - loss: 0.4805\n",
            "\u001b[1m 226/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7488 - loss: 0.4810\n",
            "\u001b[1m 247/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7486 - loss: 0.4816\n",
            "\u001b[1m 260/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7485 - loss: 0.4821\n",
            "\u001b[1m 276/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7482 - loss: 0.4826\n",
            "\u001b[1m 288/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7480 - loss: 0.4830\n",
            "\u001b[1m 308/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7478 - loss: 0.4835\n",
            "\u001b[1m 321/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7477 - loss: 0.4837\n",
            "\u001b[1m 335/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7476 - loss: 0.4840\n",
            "\u001b[1m 349/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7475 - loss: 0.4843\n",
            "\u001b[1m 365/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7473 - loss: 0.4847\n",
            "\u001b[1m 379/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7472 - loss: 0.4850\n",
            "\u001b[1m 397/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7471 - loss: 0.4852\n",
            "\u001b[1m 414/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.7471 - loss: 0.4854\n",
            "\u001b[1m 443/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7471 - loss: 0.4857\n",
            "\u001b[1m 473/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7471 - loss: 0.4859\n",
            "\u001b[1m 502/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7471 - loss: 0.4860\n",
            "\u001b[1m  32/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7303 - loss: 0.4874\n",
            "\u001b[1m 528/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7471 - loss: 0.4861\n",
            "\u001b[1m 557/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7471 - loss: 0.4862\n",
            "\u001b[1m 582/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7471 - loss: 0.4862\n",
            "\u001b[1m 613/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7472 - loss: 0.4862\n",
            "\u001b[1m 634/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7473 - loss: 0.4862\n",
            "\u001b[1m 670/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7474 - loss: 0.4862\n",
            "\u001b[1m 702/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7474 - loss: 0.4863\n",
            "\u001b[1m 738/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7474 - loss: 0.4864\n",
            "\u001b[1m 772/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7475 - loss: 0.4865\n",
            "\u001b[1m 804/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7475 - loss: 0.4866 \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:19\u001b[0m 52ms/step - accuracy: 0.6250 - loss: 0.6513\n",
            "\u001b[1m 687/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7457 - loss: 0.4850\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m1471/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7487 - loss: 0.4859\u001b[32m [repeated 65x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  26 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 11\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  11 Training...\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7489 - loss: 0.4856\n",
            "\u001b[1m2426/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7460 - loss: 0.4870\u001b[32m [repeated 87x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \r\u001b[1m   1/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:28:14\u001b[0m 2s/step - accuracy: 0.0625 - loss: 3.4703\n",
            "\u001b[1m  32/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.1208 - loss: 3.3570\n",
            "\u001b[1m  91/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.2634 - loss: 3.0175\n",
            "\u001b[1m 104/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.2872 - loss: 2.9425\n",
            "\u001b[1m 121/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.3148 - loss: 2.8464\n",
            "\u001b[1m 137/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.3377 - loss: 2.7595\n",
            "\u001b[1m 157/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.3624 - loss: 2.6567\n",
            "\u001b[1m 176/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.3828 - loss: 2.5656\n",
            "\u001b[1m 186/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.3925 - loss: 2.5206\n",
            "\u001b[1m 204/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.4084 - loss: 2.4443\n",
            "\u001b[1m 223/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.4235 - loss: 2.3697\n",
            "\u001b[1m 237/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.4336 - loss: 2.3187\n",
            "\u001b[1m 256/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.4462 - loss: 2.2544\n",
            "\u001b[1m 279/2656\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.4597 - loss: 2.1831\n",
            "\u001b[1m 302/2656\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.4720 - loss: 2.1181\n",
            "\u001b[1m 317/2656\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.4793 - loss: 2.0788\n",
            "\u001b[1m 338/2656\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.4888 - loss: 2.0273\n",
            "\u001b[1m 355/2656\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.4959 - loss: 1.9884\n",
            "\u001b[1m 374/2656\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5034 - loss: 1.9476\n",
            "\u001b[1m 390/2656\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5093 - loss: 1.9152\n",
            "\u001b[1m 410/2656\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5161 - loss: 1.8772\n",
            "\u001b[1m 428/2656\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5218 - loss: 1.8450\n",
            "\u001b[1m 448/2656\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5277 - loss: 1.8113\n",
            "\u001b[1m 465/2656\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5325 - loss: 1.7841\n",
            "\u001b[1m 488/2656\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5384 - loss: 1.7495\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  29 Training complete...\n",
            "\u001b[1m 502/2656\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5419 - loss: 1.7295\n",
            "\u001b[1m 522/2656\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5465 - loss: 1.7022\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 3\n",
            "\u001b[1m 538/2656\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5501 - loss: 1.6814\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  3 Training...\n",
            "\u001b[1m 558/2656\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5543 - loss: 1.6564\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n",
            "\u001b[1m 577/2656\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5581 - loss: 1.6339\n",
            "\u001b[1m 592/2656\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5610 - loss: 1.6168\n",
            "\u001b[1m 610/2656\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5644 - loss: 1.5970\n",
            "\u001b[1m 629/2656\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5677 - loss: 1.5771\n",
            "\u001b[1m 646/2656\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5705 - loss: 1.5600\n",
            "\u001b[1m 659/2656\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5726 - loss: 1.5474\n",
            "\u001b[1m 678/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5755 - loss: 1.5296\n",
            "\u001b[1m  76/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.2326 - loss: 3.1057\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[1m 696/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5782 - loss: 1.5134\n",
            "\u001b[1m 708/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5799 - loss: 1.5029\n",
            "\u001b[1m   8/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.0537 - loss: 3.4673   \n",
            "\u001b[1m 716/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5810 - loss: 1.4960\n",
            "\u001b[1m 744/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5848 - loss: 1.4729\n",
            "\u001b[1m 765/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5875 - loss: 1.4563\n",
            "\u001b[1m 771/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5883 - loss: 1.4517\n",
            "\u001b[1m 792/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5909 - loss: 1.4360\n",
            "\u001b[1m 813/2656\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5933 - loss: 1.4210\n",
            "\u001b[1m 821/2656\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5942 - loss: 1.4154\n",
            "\u001b[1m 839/2656\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5963 - loss: 1.4031\n",
            "\u001b[1m   1/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:35:55\u001b[0m 4s/step - accuracy: 0.0312 - loss: 3.4706\n",
            "\u001b[1m 861/2656\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.5986 - loss: 1.3886\n",
            "\u001b[1m 892/2656\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6018 - loss: 1.3692\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
            "\u001b[1m 989/2656\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6106 - loss: 1.3146\n",
            "\u001b[1m 516/2352\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.5674 - loss: 1.6385\n",
            "\u001b[1m1846/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.6542 - loss: 1.0413\u001b[32m [repeated 47x across cluster]\u001b[0m\n",
            "\u001b[1m 484/2352\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.5592 - loss: 1.6833\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m  17/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7018 - loss: 0.5231   \n",
            "\u001b[1m2352/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.6949 - loss: 0.9035\n",
            "\u001b[1m   1/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:48\u001b[0m 46ms/step - accuracy: 0.8750 - loss: 0.3897\n",
            "\u001b[1m  29/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.8036 - loss: 0.4307\n",
            "\u001b[1m 620/2656\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7377 - loss: 0.5093\u001b[32m [repeated 93x across cluster]\u001b[0m\n",
            "\u001b[1m 866/2352\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7676 - loss: 0.4772\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m1452/2656\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7383 - loss: 0.5109\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m1265/2352\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7658 - loss: 0.4773\u001b[32m [repeated 87x across cluster]\u001b[0m\n",
            "\u001b[1m2004/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7388 - loss: 0.5112\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m2079/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7389 - loss: 0.5112\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 41ms/step - accuracy: 0.7812 - loss: 0.4429\n",
            "\u001b[1m  33/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7339 - loss: 0.4722\n",
            "\u001b[1m 198/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7356 - loss: 0.4907\u001b[32m [repeated 85x across cluster]\u001b[0m\n",
            "\u001b[1m1135/2352\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7707 - loss: 0.4555\n",
            "\u001b[1m1184/2352\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7707 - loss: 0.4554\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 40ms/step - accuracy: 0.8438 - loss: 0.2230\n",
            "\u001b[1m  35/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7887 - loss: 0.4721\n",
            "\u001b[1m1767/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7426 - loss: 0.4956\u001b[32m [repeated 91x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   9/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - accuracy: 0.7537 - loss: 0.4658  \n",
            "\u001b[1m  25/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - accuracy: 0.7615 - loss: 0.4686\n",
            "\u001b[1m1442/2656\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7424 - loss: 0.4947\n",
            "\u001b[1m2352/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7701 - loss: 0.4567\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m 223/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7736 - loss: 0.4429\n",
            "\u001b[1m 231/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7737 - loss: 0.4428\n",
            "\u001b[1m2346/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7701 - loss: 0.4567\u001b[32m [repeated 56x across cluster]\u001b[0m\n",
            "\u001b[1m 722/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7450 - loss: 0.4943\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m  10/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.7655 - loss: 0.4923  \n",
            "\u001b[1m 776/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7452 - loss: 0.4941\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[1m 679/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7448 - loss: 0.4947\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m1265/2352\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7710 - loss: 0.4440\n",
            "\u001b[1m1306/2656\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7455 - loss: 0.4935\u001b[32m [repeated 24x across cluster]\u001b[0m\n",
            "\u001b[1m1367/2352\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7710 - loss: 0.4441\n",
            "\u001b[1m1405/2352\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7709 - loss: 0.4442\n",
            "\u001b[1m1009/2656\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7455 - loss: 0.4935 \u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:38\u001b[0m 42ms/step - accuracy: 0.7812 - loss: 0.5798\n",
            "\u001b[1m  31/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7935 - loss: 0.4369\n",
            "\u001b[1m  18/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7354 - loss: 0.4937   \n",
            "\u001b[1m2656/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7456 - loss: 0.4935\u001b[32m [repeated 16x across cluster]\u001b[0m\n",
            "\u001b[1m 149/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7439 - loss: 0.4898\u001b[32m [repeated 85x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m 582/2656\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7479 - loss: 0.4841\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m1354/2656\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7480 - loss: 0.4853\u001b[32m [repeated 94x across cluster]\u001b[0m\n",
            "\u001b[1m1502/2656\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7481 - loss: 0.4853\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  3 Training complete...\n",
            "\u001b[1m1683/2656\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7481 - loss: 0.4853\u001b[32m [repeated 6x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 19\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  19 Training...\n",
            "\u001b[1m2291/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7481 - loss: 0.4856\u001b[32m [repeated 87x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \r\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:22:21\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 3.6131\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  18/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.0780 - loss: 3.4687        \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  35/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.1235 - loss: 3.3567\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  11 Training complete...\n",
            "\u001b[1m2656/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7481 - loss: 0.4857\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 32\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  32 Training...\n",
            "\u001b[1m 463/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.5219 - loss: 1.8337\u001b[32m [repeated 55x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m1035/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6107 - loss: 1.2976\n",
            "\u001b[1m1050/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.6118 - loss: 1.2904\n",
            "\u001b[1m   1/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21:09\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 3.7688\n",
            "\u001b[1m  34/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.0867 - loss: 3.4279\n",
            "\u001b[1m1694/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.6464 - loss: 1.0960\u001b[32m [repeated 94x across cluster]\u001b[0m\n",
            "\u001b[1m2369/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.6653 - loss: 0.9606\n",
            "\u001b[1m2652/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6709 - loss: 0.9259\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:54\u001b[0m 111ms/step - accuracy: 0.6562 - loss: 0.4907\n",
            "\u001b[1m   9/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 7ms/step - accuracy: 0.6942 - loss: 0.5695   \n",
            "\u001b[1m  19/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.7125 - loss: 0.5684\n",
            "\u001b[1m  40/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7169 - loss: 0.5675\n",
            "\u001b[1m  54/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 6ms/step - accuracy: 0.7183 - loss: 0.5666\n",
            "\u001b[1m  75/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7192 - loss: 0.5641\n",
            "\u001b[1m  90/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7201 - loss: 0.5622\n",
            "\u001b[1m 110/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7212 - loss: 0.5594\n",
            "\u001b[1m 127/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7222 - loss: 0.5560\n",
            "\u001b[1m 139/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7229 - loss: 0.5537\n",
            "\u001b[1m 168/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7240 - loss: 0.5493\n",
            "\u001b[1m 199/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7251 - loss: 0.5455\n",
            "\u001b[1m 231/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7262 - loss: 0.5420\n",
            "\u001b[1m 260/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7270 - loss: 0.5398\n",
            "\u001b[1m 290/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7278 - loss: 0.5379\n",
            "\u001b[1m 323/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7285 - loss: 0.5361\n",
            "\u001b[1m 349/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7290 - loss: 0.5348\n",
            "\u001b[1m2564/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6695 - loss: 0.9495\u001b[32m [repeated 72x across cluster]\u001b[0m\n",
            "\u001b[1m 384/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7295 - loss: 0.5335\n",
            "\u001b[1m 414/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7298 - loss: 0.5326\n",
            "\u001b[1m  32/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7637 - loss: 0.4796\n",
            "\u001b[1m2657/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - accuracy: 0.6712 - loss: 0.9384\u001b[32m [repeated 9x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:07\u001b[0m 48ms/step - accuracy: 0.7812 - loss: 0.3773\n",
            "\u001b[1m1327/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7427 - loss: 0.5126\u001b[32m [repeated 92x across cluster]\u001b[0m\n",
            "\u001b[1m1582/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7422 - loss: 0.5128\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:22\u001b[0m 54ms/step - accuracy: 0.7500 - loss: 0.4523\n",
            "\u001b[1m  33/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7351 - loss: 0.4997\n",
            "\u001b[1m 490/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7423 - loss: 0.4925\n",
            "\u001b[1m  14/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.7347 - loss: 0.5291\n",
            "\u001b[1m  28/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7383 - loss: 0.5224\n",
            "\u001b[1m  41/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7360 - loss: 0.5212\n",
            "\u001b[1m  54/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7346 - loss: 0.5223\n",
            "\u001b[1m 626/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7420 - loss: 0.4933\u001b[32m [repeated 82x across cluster]\u001b[0m\n",
            "\u001b[1m  69/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7347 - loss: 0.5217\n",
            "\u001b[1m  81/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7344 - loss: 0.5215\n",
            "\u001b[1m  94/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7344 - loss: 0.5206\n",
            "\u001b[1m 104/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.7346 - loss: 0.5194\n",
            "\u001b[1m 113/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7349 - loss: 0.5184\n",
            "\u001b[1m 133/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7356 - loss: 0.5165\n",
            "\u001b[1m 147/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7360 - loss: 0.5156\n",
            "\u001b[1m 159/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7363 - loss: 0.5149\n",
            "\u001b[1m 166/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7365 - loss: 0.5145\n",
            "\u001b[1m 183/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7367 - loss: 0.5137\n",
            "\u001b[1m 190/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7369 - loss: 0.5134\n",
            "\u001b[1m 197/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7370 - loss: 0.5131\n",
            "\u001b[1m 202/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7371 - loss: 0.5129\n",
            "\u001b[1m 210/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 8ms/step - accuracy: 0.7372 - loss: 0.5130\n",
            "\u001b[1m 219/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7373 - loss: 0.5129\n",
            "\u001b[1m 685/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7418 - loss: 0.4939\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[1m 230/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7375 - loss: 0.5129\n",
            "\u001b[1m 243/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7377 - loss: 0.5128\n",
            "\u001b[1m 258/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7379 - loss: 0.5126\n",
            "\u001b[1m 273/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7384 - loss: 0.5122\n",
            "\u001b[1m 287/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7388 - loss: 0.5118\n",
            "\u001b[1m 301/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7391 - loss: 0.5115\n",
            "\u001b[1m 314/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7393 - loss: 0.5114\n",
            "\u001b[1m 329/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7394 - loss: 0.5112\n",
            "\u001b[1m 342/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7395 - loss: 0.5110\n",
            "\u001b[1m 356/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7396 - loss: 0.5109\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:09\u001b[0m 49ms/step - accuracy: 0.7188 - loss: 0.5550\n",
            "\u001b[1m 368/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7397 - loss: 0.5107\n",
            "\u001b[1m 380/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7398 - loss: 0.5105\n",
            "\u001b[1m 390/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7399 - loss: 0.5103\n",
            "\u001b[1m 403/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7401 - loss: 0.5100\n",
            "\u001b[1m 419/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7403 - loss: 0.5097\n",
            "\u001b[1m 434/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7405 - loss: 0.5094\n",
            "\u001b[1m 446/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7407 - loss: 0.5091\n",
            "\u001b[1m 454/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7407 - loss: 0.5090\n",
            "\u001b[1m 475/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7409 - loss: 0.5086\n",
            "\u001b[1m 482/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7410 - loss: 0.5085\n",
            "\u001b[1m 501/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7411 - loss: 0.5082\n",
            "\u001b[1m 519/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7412 - loss: 0.5080\n",
            "\u001b[1m 533/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7413 - loss: 0.5078\n",
            "\u001b[1m 550/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7414 - loss: 0.5076\n",
            "\u001b[1m 566/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7415 - loss: 0.5074\n",
            "\u001b[1m 582/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7416 - loss: 0.5072\n",
            "\u001b[1m 598/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7417 - loss: 0.5071\n",
            "\u001b[1m 626/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7418 - loss: 0.5068\n",
            "\u001b[1m 642/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7419 - loss: 0.5066\n",
            "\u001b[1m 668/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7420 - loss: 0.5063\n",
            "\u001b[1m 679/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7420 - loss: 0.5061\n",
            "\u001b[1m 695/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7421 - loss: 0.5059\n",
            "\u001b[1m 711/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7422 - loss: 0.5057\n",
            "\u001b[1m 723/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7422 - loss: 0.5056\n",
            "\u001b[1m 737/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7423 - loss: 0.5055\n",
            "\u001b[1m1361/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.7415 - loss: 0.4962\u001b[32m [repeated 43x across cluster]\u001b[0m\n",
            "\u001b[1m 753/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7423 - loss: 0.5053\n",
            "\u001b[1m 779/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7424 - loss: 0.5050\n",
            "\u001b[1m 820/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7424 - loss: 0.5047\n",
            "\u001b[1m 849/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7425 - loss: 0.5045\n",
            "\u001b[1m 885/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7425 - loss: 0.5044\n",
            "\u001b[1m 917/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7425 - loss: 0.5042\n",
            "\u001b[1m 951/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7425 - loss: 0.5041\n",
            "\u001b[1m 993/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7425 - loss: 0.5039\n",
            "\u001b[1m1010/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7425 - loss: 0.5038\n",
            "\u001b[1m1027/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7425 - loss: 0.5038\n",
            "\u001b[1m1058/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7425 - loss: 0.5037 \n",
            "\u001b[1m1032/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7415 - loss: 0.4958\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m  16/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7328 - loss: 0.4968   \n",
            "\u001b[1m2332/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7426 - loss: 0.5005\u001b[32m [repeated 85x across cluster]\u001b[0m\n",
            "\u001b[1m   1/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:46\u001b[0m 40ms/step - accuracy: 0.8125 - loss: 0.4618\n",
            "\u001b[1m  25/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7672 - loss: 0.4676\n",
            "\u001b[1m  61/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7563 - loss: 0.4786\n",
            "\u001b[1m2657/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 4ms/step - accuracy: 0.7427 - loss: 0.5001\n",
            "\u001b[1m 935/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7484 - loss: 0.4812\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m1162/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7485 - loss: 0.4819\u001b[32m [repeated 95x across cluster]\u001b[0m\n",
            "\u001b[1m1253/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7484 - loss: 0.4822\n",
            "\u001b[1m1396/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7483 - loss: 0.4827\n",
            "\u001b[1m1444/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7483 - loss: 0.4829\n",
            "\u001b[1m1451/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7483 - loss: 0.4829\n",
            "\u001b[1m2473/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7458 - loss: 0.4886\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m2638/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7458 - loss: 0.4887\n",
            "\u001b[1m1886/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7478 - loss: 0.4845\u001b[32m [repeated 88x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m  16/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7328 - loss: 0.4984   \n",
            "\u001b[1m  87/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7233 - loss: 0.5236\n",
            "\u001b[1m  92/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7240 - loss: 0.5231\n",
            "\u001b[1m 108/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.7255 - loss: 0.5221\n",
            "\u001b[1m 131/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.7271 - loss: 0.5209\n",
            "\u001b[1m 163/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7299 - loss: 0.5184\n",
            "\u001b[1m 191/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7317 - loss: 0.5165\n",
            "\u001b[1m 230/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7337 - loss: 0.5143\n",
            "\u001b[1m 260/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7352 - loss: 0.5126\n",
            "\u001b[1m 293/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7365 - loss: 0.5109 \n",
            "\u001b[1m 727/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7425 - loss: 0.5001\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[1m1299/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7456 - loss: 0.4936\n",
            "\u001b[1m 593/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7431 - loss: 0.4886\u001b[32m [repeated 78x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m  18/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7481 - loss: 0.4444   \n",
            "\u001b[1m 748/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7440 - loss: 0.4880\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  19 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 14\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  14 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n",
            "\u001b[1m2103/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7468 - loss: 0.4855\u001b[32m [repeated 88x across cluster]\u001b[0m\n",
            "\u001b[1m  20/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.0951 - loss: 3.4296        \n",
            "\u001b[1m  51/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.1744 - loss: 3.2531\n",
            "\u001b[1m  68/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.2152 - loss: 3.1514\n",
            "\u001b[1m  82/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.2447 - loss: 3.0655\n",
            "\u001b[1m  94/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.2685 - loss: 2.9909\n",
            "\u001b[1m 119/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.3133 - loss: 2.8375\n",
            "\u001b[1m 137/2507\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.3412 - loss: 2.7326\n",
            "\u001b[1m 147/2507\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.3551 - loss: 2.6774\n",
            "\u001b[1m 157/2507\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.3680 - loss: 2.6245\n",
            "\u001b[1m 158/2507\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.3693 - loss: 2.6193\n",
            "\u001b[1m 164/2507\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.3766 - loss: 2.5886\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 174/2507\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.3882 - loss: 2.5391\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 181/2507\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 7ms/step - accuracy: 0.3958 - loss: 2.5059\n",
            "\u001b[1m 199/2507\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.4140 - loss: 2.4251\n",
            "\u001b[1m 220/2507\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 7ms/step - accuracy: 0.4329 - loss: 2.3389\n",
            "\u001b[1m 241/2507\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.4497 - loss: 2.2606\n",
            "\u001b[1m 263/2507\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.4654 - loss: 2.1861\n",
            "\u001b[1m 284/2507\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.4788 - loss: 2.1210\n",
            "\u001b[1m 308/2507\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 6ms/step - accuracy: 0.4925 - loss: 2.0530\n",
            "\u001b[1m 331/2507\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.5044 - loss: 1.9935\n",
            "\u001b[1m 355/2507\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.5156 - loss: 1.9365\n",
            "\u001b[1m2657/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7474 - loss: 0.4854\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m 373/2507\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5234 - loss: 1.8967\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  32 Training complete...\n",
            "\u001b[1m 393/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5315 - loss: 1.8552\n",
            "\u001b[1m 411/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5382 - loss: 1.8203\n",
            "\u001b[1m 431/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5452 - loss: 1.7837\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 1\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  1 Training...\n",
            "\u001b[1m 448/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.5508 - loss: 1.7544\n",
            "\u001b[1m 474/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5588 - loss: 1.7124\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 1/5\n",
            "\u001b[1m 490/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5634 - loss: 1.6880\n",
            "\u001b[1m 508/2507\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5684 - loss: 1.6619\n",
            "\u001b[1m2645/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7474 - loss: 0.4854\u001b[32m [repeated 26x across cluster]\u001b[0m\n",
            "\u001b[1m 525/2507\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5729 - loss: 1.6383\n",
            "\u001b[1m 543/2507\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5774 - loss: 1.6145\n",
            "\u001b[1m 568/2507\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5832 - loss: 1.5833\n",
            "\u001b[1m 588/2507\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5876 - loss: 1.5597\n",
            "\u001b[1m 605/2507\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5912 - loss: 1.5405\n",
            "\u001b[1m 620/2507\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5942 - loss: 1.5242\n",
            "\u001b[1m 635/2507\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.5972 - loss: 1.5084\n",
            "\u001b[1m 656/2507\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6011 - loss: 1.4873\n",
            "\u001b[1m 673/2507\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6041 - loss: 1.4708\n",
            "\u001b[1m 692/2507\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6074 - loss: 1.4532\n",
            "\u001b[1m 706/2507\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6097 - loss: 1.4406\n",
            "\u001b[1m 726/2507\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6129 - loss: 1.4231\n",
            "\u001b[1m 746/2507\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6160 - loss: 1.4064\n",
            "\u001b[1m 759/2507\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6180 - loss: 1.3959\n",
            "\u001b[1m 766/2507\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6190 - loss: 1.3904\n",
            "\u001b[1m 778/2507\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6207 - loss: 1.3810\n",
            "\u001b[1m 785/2507\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6217 - loss: 1.3757\n",
            "\u001b[1m 814/2507\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.6256 - loss: 1.3543\n",
            "\u001b[1m 846/2507\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.6297 - loss: 1.3320\n",
            "\u001b[1m   1/2426\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:13:24\u001b[0m 3s/step - accuracy: 0.0000e+00 - loss: 3.7694\n",
            "\u001b[1m  31/2426\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.0577 - loss: 3.4573\n",
            "\u001b[1m 231/2426\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.3808 - loss: 2.4125\n",
            "\u001b[1m 190/2426\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.3455 - loss: 2.5761\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m 949/2426\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.5721 - loss: 1.3949\u001b[32m [repeated 55x across cluster]\u001b[0m\n",
            "\u001b[1m1423/2426\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.6064 - loss: 1.1988\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 45ms/step - accuracy: 0.7188 - loss: 0.3837\n",
            "\u001b[1m  29/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7725 - loss: 0.4738\n",
            "\u001b[1m 255/2507\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7795 - loss: 0.4605\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m  20/2426\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7103 - loss: 0.5535   \n",
            "\u001b[1m  78/2426\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7106 - loss: 0.5654\u001b[32m [repeated 90x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 2/5\n",
            "\u001b[1m 781/2426\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7111 - loss: 0.5644\u001b[32m [repeated 7x across cluster]\u001b[0m\n",
            "\u001b[1m1830/2507\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7794 - loss: 0.4457\u001b[32m [repeated 88x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m  17/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7658 - loss: 0.4667   \n",
            "\u001b[1m  93/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7821 - loss: 0.4316\n",
            "\u001b[1m2027/2426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.7142 - loss: 0.5616\u001b[32m [repeated 12x across cluster]\u001b[0m\n",
            "\u001b[1m2322/2426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7147 - loss: 0.5610\u001b[32m [repeated 91x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 3/5\n",
            "\u001b[1m  18/2426\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7388 - loss: 0.4931   \n",
            "\u001b[1m 718/2426\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7189 - loss: 0.5440\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m1991/2507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7813 - loss: 0.4307\n",
            "\u001b[1m1456/2426\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7195 - loss: 0.5449\u001b[32m [repeated 93x across cluster]\u001b[0m\n",
            "\u001b[1m2072/2507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7814 - loss: 0.4306\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:34\u001b[0m 38ms/step - accuracy: 0.9062 - loss: 0.3667\n",
            "\u001b[1m  34/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.8038 - loss: 0.4173\n",
            "\u001b[1m 319/2507\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7903 - loss: 0.4237\n",
            "\u001b[1m 334/2507\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7902 - loss: 0.4236\n",
            "\u001b[1m 350/2507\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7902 - loss: 0.4236\n",
            "\u001b[1m 363/2507\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7901 - loss: 0.4235\n",
            "\u001b[1m 379/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7900 - loss: 0.4235\n",
            "\u001b[1m 393/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7899 - loss: 0.4234\n",
            "\u001b[1m 414/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7899 - loss: 0.4233\n",
            "\u001b[1m 427/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7898 - loss: 0.4232\n",
            "\u001b[1m 442/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7898 - loss: 0.4231\n",
            "\u001b[1m 456/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7898 - loss: 0.4229\n",
            "\u001b[1m2295/2426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7191 - loss: 0.5456\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[1m 465/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7898 - loss: 0.4228\n",
            "\u001b[1m 489/2507\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7898 - loss: 0.4225\n",
            "\u001b[1m 510/2507\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7898 - loss: 0.4224\n",
            "\u001b[1m 530/2507\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7897 - loss: 0.4223\n",
            "\u001b[1m  17/2426\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7598 - loss: 0.5247\n",
            "\u001b[1m 539/2507\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7897 - loss: 0.4222\n",
            "\u001b[1m1736/2426\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7193 - loss: 0.5452\n",
            "\u001b[1m2414/2426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7191 - loss: 0.5456\u001b[32m [repeated 65x across cluster]\u001b[0m\n",
            "\u001b[1m 832/2507\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7889 - loss: 0.4217 \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2426\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:23\u001b[0m 109ms/step - accuracy: 0.7500 - loss: 0.7028\n",
            "\u001b[1m 580/2426\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7276 - loss: 0.5357\u001b[32m [repeated 49x across cluster]\u001b[0m\n",
            "\u001b[1m 752/2426\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7262 - loss: 0.5368\n",
            "\u001b[1m2426/2426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7191 - loss: 0.5456\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[1m 313/2426\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7341 - loss: 0.5317\u001b[32m [repeated 3x across cluster]\u001b[0m\n",
            "\u001b[1m1128/2426\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7255 - loss: 0.5372\u001b[32m [repeated 44x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:41\u001b[0m 40ms/step - accuracy: 0.9375 - loss: 0.1860\n",
            "\u001b[1m  32/2507\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7928 - loss: 0.3929\n",
            "\u001b[1m2096/2426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7242 - loss: 0.5381\n",
            "\u001b[1m 716/2426\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7264 - loss: 0.5367\u001b[32m [repeated 5x across cluster]\u001b[0m\n",
            "\u001b[1m2140/2426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7242 - loss: 0.5381\u001b[32m [repeated 4x across cluster]\u001b[0m\n",
            "\u001b[1m  17/2426\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7157 - loss: 0.5681   \n",
            "\u001b[1m 207/2426\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7118 - loss: 0.5409\u001b[32m [repeated 91x across cluster]\u001b[0m\n",
            "\u001b[1m1093/2426\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7201 - loss: 0.5370\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Epoch 5/5\n",
            "\u001b[1m1588/2507\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7902 - loss: 0.4111\u001b[32m [repeated 10x across cluster]\u001b[0m\n",
            "\u001b[1m1389/2426\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7203 - loss: 0.5371\n",
            "\u001b[1m1806/2507\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7902 - loss: 0.4114\u001b[32m [repeated 87x across cluster]\u001b[0m\n",
            "\u001b[1m1486/2426\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7204 - loss: 0.5372\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  14 Training complete...\n",
            "\u001b[1m2507/2507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.7903 - loss: 0.4121\u001b[32m [repeated 17x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  5 Training...\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 1/5\n",
            "\u001b[1m2202/2507\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7903 - loss: 0.4118\n",
            "\u001b[1m2317/2426\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7216 - loss: 0.5366\u001b[32m [repeated 75x across cluster]\u001b[0m\n",
            "\u001b[1m   1/2252\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11:47\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 3.5875\n",
            "\u001b[1m  48/2252\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.1743 - loss: 3.2397\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  1 Training complete...\n",
            "\u001b[1m 295/2252\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.4511 - loss: 2.1375\n",
            "\u001b[1m1930/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6144 - loss: 1.1016\u001b[32m [repeated 39x across cluster]\u001b[0m\n",
            "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.6220 - loss: 1.0512\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 2/5\n",
            "\u001b[1m  31/2252\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6506 - loss: 0.6606   \n",
            "\u001b[1m1141/2252\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6845 - loss: 0.6164\n",
            "\u001b[1m1826/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6865 - loss: 0.6126\u001b[32m [repeated 47x across cluster]\u001b[0m\n",
            "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6873 - loss: 0.6112\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 3/5\n",
            "\u001b[1m  17/2252\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.6829 - loss: 0.5656   \n",
            "\u001b[1m1374/2252\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.6959 - loss: 0.5912\n",
            "\u001b[1m1511/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6959 - loss: 0.5914\u001b[32m [repeated 47x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2252\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:05\u001b[0m 29ms/step - accuracy: 0.5625 - loss: 0.7949\n",
            "\u001b[1m  57/2252\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.6877 - loss: 0.5911\n",
            "\u001b[1m1850/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6966 - loss: 0.5826\u001b[32m [repeated 48x across cluster]\u001b[0m\n",
            "\u001b[1m1975/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.6966 - loss: 0.5825\n",
            "\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6968 - loss: 0.5825\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2252\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 28ms/step - accuracy: 0.7188 - loss: 0.5912\n",
            "\u001b[1m  56/2252\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7062 - loss: 0.5849\n",
            "\u001b[1m  83/2252\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.6996 - loss: 0.5885\n",
            "\u001b[1m1737/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.6994 - loss: 0.5780\u001b[32m [repeated 47x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 33 results and 0 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2238/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6993 - loss: 0.5779\n",
            "Server Evaluating... Evaluation Count: 1\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2252/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.6993 - loss: 0.5779\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  5 Training complete...\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m52s\u001b[0m 2ms/step - accuracy: 0.7511 - loss: 0.4928\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m33s\u001b[0m 1ms/step\n",
            "Prediction:  [[1.7732433e-06 2.6936066e-06 7.6810871e-08 ... 3.7410270e-09\n",
            "  7.4822758e-08 6.9671486e-08]\n",
            " [7.3506214e-09 7.0982882e-09 9.9988967e-01 ... 1.1676134e-13\n",
            "  1.1016824e-11 1.6430706e-08]\n",
            " [9.3174117e-07 4.8039659e-05 3.0217034e-05 ... 3.1159957e-11\n",
            "  1.2488602e-10 5.2227875e-09]\n",
            " ...\n",
            " [3.2153898e-07 4.8467550e-06 1.4533174e-07 ... 1.2390898e-15\n",
            "  1.0181439e-11 5.5299970e-10]\n",
            " [1.3951811e-06 1.0000317e-06 2.5066685e-05 ... 3.4020921e-11\n",
            "  5.2408700e-10 3.1017702e-09]\n",
            " [3.4170821e-06 1.5797481e-06 5.3791966e-05 ... 1.7338458e-10\n",
            "  2.2391000e-09 1.4792274e-08]] (744790, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (1, 0.49314528703689575, {'accuracy': 0.7516212463378906}, 1025.342713132)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 16 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mServer evaluation complete - Accuracy: 0.7516, Loss: 0.4931\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client ID: 6\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2208/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6993 - loss: 0.5779\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2224/2252\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.6993 - loss: 0.5779\u001b[32m [repeated 15x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m Client  6 Evaluating...\n",
            "\u001b[1m  12/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7197 - loss: 0.4714    \n",
            "\u001b[1m  32/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7478 - loss: 0.4618\n",
            "\u001b[1m  55/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7547 - loss: 0.4609\n",
            "\u001b[1m  80/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7586 - loss: 0.4628\n",
            "\u001b[1m   1/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m44:00\u001b[0m 1s/step - accuracy: 0.6875 - loss: 0.5714\n",
            "\u001b[1m  97/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 5ms/step - accuracy: 0.7611 - loss: 0.4634\n",
            "\u001b[1m  20/2404\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 6ms/step - accuracy: 0.6981 - loss: 0.5696\n",
            "\u001b[1m 112/2454\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 5ms/step - accuracy: 0.7626 - loss: 0.4642\n",
            "\u001b[1m 357/2454\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7667 - loss: 0.4633\n",
            "\u001b[1m 382/2454\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7669 - loss: 0.4631\n",
            "\u001b[1m 428/2404\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7321 - loss: 0.5069\n",
            "\u001b[1m 616/2454\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7688 - loss: 0.4623\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 4\n",
            "\u001b[1m 855/2454\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7698 - loss: 0.4620\u001b[32m [repeated 13x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  4 Evaluating...\n",
            "\u001b[1m 956/2404\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7325 - loss: 0.5049\n",
            "\u001b[1m 513/2404\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7320 - loss: 0.5066\u001b[32m [repeated 46x across cluster]\u001b[0m\n",
            "\u001b[1m1844/2404\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7332 - loss: 0.5032\n",
            "\u001b[1m 406/2404\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7321 - loss: 0.5072\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[33mClient 4 evaluation complete - Accuracy: 0.734741, Loss: 0.500842\u001b[0m\n",
            "\u001b[1m 558/2404\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7318 - loss: 0.5067\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client ID: 23\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m Client  23 Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             This is a deprecated feature. It will be removed\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m             entirely in future versions of Flower.\u001b[32m [repeated 2x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=1715)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m(raylet)\u001b[0m A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: fffffffffffffffff4e6df034bd07adb8b3b785d01000000 Worker ID: 9b9bd4157ace78131d274edc8ac7dabb88e71e87a9a87b865bca450a Node ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b Worker IP address: 172.28.0.12 Worker port: 33681 Worker PID: 1713 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2445/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7700 - loss: 0.4625\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2449/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7700 - loss: 0.4625\u001b[32m [repeated 78x across cluster]\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2454/2454\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7700 - loss: 0.4625\u001b[32m [repeated 5x across cluster]\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ClientAppActor\n",
            "\tactor_id: f4e6df034bd07adb8b3b785d01000000\n",
            "\tpid: 1713\n",
            "\tnamespace: 12ce9bb1-8983-4cfc-96f7-8e017464848b\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ClientAppActor\n",
            "\tactor_id: f4e6df034bd07adb8b3b785d01000000\n",
            "\tpid: 1713\n",
            "\tnamespace: 12ce9bb1-8983-4cfc-96f7-8e017464848b\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ClientAppActor\n",
            "\tactor_id: f4e6df034bd07adb8b3b785d01000000\n",
            "\tpid: 1713\n",
            "\tnamespace: 12ce9bb1-8983-4cfc-96f7-8e017464848b\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\u001b[93mWARNING \u001b[0m:   Actor(f4e6df034bd07adb8b3b785d01000000) will be remove from pool.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 291, in _fetch_future_result\n",
            "    raise ex\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ClientAppActor\n",
            "\tactor_id: f4e6df034bd07adb8b3b785d01000000\n",
            "\tpid: 1713\n",
            "\tnamespace: 12ce9bb1-8983-4cfc-96f7-8e017464848b\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ClientAppActor\n",
            "\tactor_id: f4e6df034bd07adb8b3b785d01000000\n",
            "\tpid: 1713\n",
            "\tnamespace: 12ce9bb1-8983-4cfc-96f7-8e017464848b\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\u001b[93mWARNING \u001b[0m:   REMOVED actor f4e6df034bd07adb8b3b785d01000000 from pool\n",
            "\u001b[93mWARNING \u001b[0m:   Pool size: 1\n",
            "\u001b[93mWARNING \u001b[0m:   Actor(f4e6df034bd07adb8b3b785d01000000) will be remove from pool.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 291, in _fetch_future_result\n",
            "    raise ex\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ClientAppActor\n",
            "\tactor_id: f4e6df034bd07adb8b3b785d01000000\n",
            "\tpid: 1713\n",
            "\tnamespace: 12ce9bb1-8983-4cfc-96f7-8e017464848b\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ClientAppActor\n",
            "\tactor_id: f4e6df034bd07adb8b3b785d01000000\n",
            "\tpid: 1713\n",
            "\tnamespace: 12ce9bb1-8983-4cfc-96f7-8e017464848b\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[93mWARNING \u001b[0m:   Actor(f4e6df034bd07adb8b3b785d01000000) will be remove from pool.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 291, in _fetch_future_result\n",
            "    raise ex\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ClientAppActor\n",
            "\tactor_id: f4e6df034bd07adb8b3b785d01000000\n",
            "\tpid: 1713\n",
            "\tnamespace: 12ce9bb1-8983-4cfc-96f7-8e017464848b\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ClientAppActor\n",
            "\tactor_id: f4e6df034bd07adb8b3b785d01000000\n",
            "\tpid: 1713\n",
            "\tnamespace: 12ce9bb1-8983-4cfc-96f7-8e017464848b\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ClientAppActor\n",
            "\tactor_id: f4e6df034bd07adb8b3b785d01000000\n",
            "\tpid: 1713\n",
            "\tnamespace: 12ce9bb1-8983-4cfc-96f7-8e017464848b\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[93mWARNING \u001b[0m:   Actor(f4e6df034bd07adb8b3b785d01000000) will be remove from pool.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 291, in _fetch_future_result\n",
            "    raise ex\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.ActorDiedError: The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ClientAppActor\n",
            "\tactor_id: f4e6df034bd07adb8b3b785d01000000\n",
            "\tpid: 1713\n",
            "\tnamespace: 12ce9bb1-8983-4cfc-96f7-8e017464848b\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     The actor died unexpectedly before finishing this task.\n",
            "\tclass_name: ClientAppActor\n",
            "\tactor_id: f4e6df034bd07adb8b3b785d01000000\n",
            "\tpid: 1713\n",
            "\tnamespace: 12ce9bb1-8983-4cfc-96f7-8e017464848b\n",
            "\tip: 172.28.0.12\n",
            "The actor is dead because its worker process has died. Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 2 results and 14 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No evaluate_metrics_aggregation_fn provided\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 33 clients (out of 33)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      The cluster expanded. Adding 1 actors to the pool.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[36m(pid=24048)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=24048)\u001b[0m E0000 00:00:1755674943.051594   24048 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=24048)\u001b[0m E0000 00:00:1755674943.105352   24048 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=24048)\u001b[0m W0000 00:00:1755674943.522272   24048 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=24048)\u001b[0m W0000 00:00:1755674943.522338   24048 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=24048)\u001b[0m W0000 00:00:1755674943.522345   24048 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=24048)\u001b[0m W0000 00:00:1755674943.522351   24048 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=1713)\u001b[0m \u001b[33mClient 6 evaluation complete - Accuracy: 0.769664, Loss: 0.463880\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client ID: 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client  26 Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 1/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:01:51\u001b[0m 1s/step - accuracy: 0.7500 - loss: 0.5485\n",
            "\u001b[1m  54/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7635 - loss: 0.4792\n",
            "\u001b[1m 111/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7546 - loss: 0.4917\n",
            "\u001b[1m 162/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7491 - loss: 0.4979\n",
            "\u001b[1m 219/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7464 - loss: 0.4993\n",
            "\u001b[1m 269/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7445 - loss: 0.5004\n",
            "\u001b[1m 322/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7436 - loss: 0.5009\n",
            "\u001b[1m 365/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7432 - loss: 0.5011\n",
            "\u001b[1m 420/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7431 - loss: 0.5007\n",
            "\u001b[1m 459/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7432 - loss: 0.5005\n",
            "\u001b[1m 516/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7433 - loss: 0.5001\n",
            "\u001b[1m 590/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7434 - loss: 0.4997\n",
            "\u001b[1m 644/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7434 - loss: 0.4994\n",
            "\u001b[1m 694/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7434 - loss: 0.4992\n",
            "\u001b[1m 744/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7434 - loss: 0.4990\n",
            "\u001b[1m 792/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7434 - loss: 0.4987\n",
            "\u001b[1m 845/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7434 - loss: 0.4983\n",
            "\u001b[1m 897/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7436 - loss: 0.4979\n",
            "\u001b[1m 943/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7436 - loss: 0.4977\n",
            "\u001b[1m 988/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7437 - loss: 0.4974\n",
            "\u001b[1m1036/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7438 - loss: 0.4971\n",
            "\u001b[1m1091/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7439 - loss: 0.4968\n",
            "\u001b[1m1138/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7440 - loss: 0.4966\n",
            "\u001b[1m1193/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7441 - loss: 0.4963\n",
            "\u001b[1m1241/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7441 - loss: 0.4961\n",
            "\u001b[1m1297/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7441 - loss: 0.4959\n",
            "\u001b[1m1348/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7441 - loss: 0.4957\n",
            "\u001b[1m1397/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7441 - loss: 0.4956\n",
            "\u001b[1m1445/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7441 - loss: 0.4955\n",
            "\u001b[1m1491/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7442 - loss: 0.4953\n",
            "\u001b[1m1546/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7442 - loss: 0.4952\n",
            "\u001b[1m1597/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7442 - loss: 0.4951\n",
            "\u001b[1m1650/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7442 - loss: 0.4949\n",
            "\u001b[1m1699/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7442 - loss: 0.4948\n",
            "\u001b[1m1753/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7443 - loss: 0.4947\n",
            "\u001b[1m1803/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7443 - loss: 0.4946\n",
            "\u001b[1m1852/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7443 - loss: 0.4945\n",
            "\u001b[1m1904/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7444 - loss: 0.4945\n",
            "\u001b[1m1959/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7444 - loss: 0.4945\n",
            "\u001b[1m2001/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7444 - loss: 0.4945\n",
            "\u001b[1m2052/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7445 - loss: 0.4946\n",
            "\u001b[1m2101/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7445 - loss: 0.4946\n",
            "\u001b[1m2127/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7445 - loss: 0.4946\n",
            "\u001b[1m2179/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7446 - loss: 0.4946\n",
            "\u001b[1m2226/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7446 - loss: 0.4946\n",
            "\u001b[1m2282/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7446 - loss: 0.4947\n",
            "\u001b[1m2331/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7446 - loss: 0.4947\n",
            "\u001b[1m2382/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7446 - loss: 0.4947\n",
            "\u001b[1m2430/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7446 - loss: 0.4947\n",
            "\u001b[1m2486/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7447 - loss: 0.4947\n",
            "\u001b[1m2527/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7447 - loss: 0.4948\n",
            "\u001b[1m2582/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7447 - loss: 0.4948\n",
            "\u001b[1m2627/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7447 - loss: 0.4948\n",
            "\u001b[1m2654/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7447 - loss: 0.4948\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 2ms/step - accuracy: 0.7447 - loss: 0.4948\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 45ms/step - accuracy: 0.7500 - loss: 0.3619\n",
            "\u001b[1m  29/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.7072 - loss: 0.5198\n",
            "\u001b[1m  61/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7105 - loss: 0.5118\n",
            "\u001b[1m  93/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7165 - loss: 0.5049\n",
            "\u001b[1m 122/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7208 - loss: 0.5008\n",
            "\u001b[1m 153/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7243 - loss: 0.4976\n",
            "\u001b[1m 176/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7265 - loss: 0.4960\n",
            "\u001b[1m 208/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 4ms/step - accuracy: 0.7287 - loss: 0.4944\n",
            "\u001b[1m 239/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7302 - loss: 0.4934\n",
            "\u001b[1m 272/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7316 - loss: 0.4923\n",
            "\u001b[1m 300/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7327 - loss: 0.4914\n",
            "\u001b[1m 334/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7338 - loss: 0.4906\n",
            "\u001b[1m 363/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7346 - loss: 0.4901\n",
            "\u001b[1m 394/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7353 - loss: 0.4896\n",
            "\u001b[1m 421/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7358 - loss: 0.4892\n",
            "\u001b[1m 457/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7365 - loss: 0.4887\n",
            "\u001b[1m 485/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7370 - loss: 0.4883\n",
            "\u001b[1m 508/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7373 - loss: 0.4881\n",
            "\u001b[1m 536/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 4ms/step - accuracy: 0.7377 - loss: 0.4877\n",
            "\u001b[1m 567/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7381 - loss: 0.4874\n",
            "\u001b[1m 601/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7386 - loss: 0.4871\n",
            "\u001b[1m 639/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7391 - loss: 0.4868\n",
            "\u001b[1m 668/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7394 - loss: 0.4866\n",
            "\u001b[1m 700/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7398 - loss: 0.4865\n",
            "\u001b[1m 727/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7401 - loss: 0.4864\n",
            "\u001b[1m 762/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7404 - loss: 0.4863\n",
            "\u001b[1m 793/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7406 - loss: 0.4862\n",
            "\u001b[1m 821/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7408 - loss: 0.4861\n",
            "\u001b[1m 848/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7410 - loss: 0.4861\n",
            "\u001b[1m 876/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7412 - loss: 0.4860\n",
            "\u001b[1m 901/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7414 - loss: 0.4860\n",
            "\u001b[1m 930/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7415 - loss: 0.4859\n",
            "\u001b[1m 955/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7417 - loss: 0.4859\n",
            "\u001b[1m 988/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7419 - loss: 0.4858\n",
            "\u001b[1m1017/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7420 - loss: 0.4857\n",
            "\u001b[1m1046/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7422 - loss: 0.4856\n",
            "\u001b[1m1078/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7424 - loss: 0.4856\n",
            "\u001b[1m1111/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7425 - loss: 0.4855\n",
            "\u001b[1m1129/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7426 - loss: 0.4855\n",
            "\u001b[1m1159/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7428 - loss: 0.4854\n",
            "\u001b[1m1185/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7429 - loss: 0.4853\n",
            "\u001b[1m1211/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7430 - loss: 0.4853\n",
            "\u001b[1m1238/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7431 - loss: 0.4853\n",
            "\u001b[1m1271/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7432 - loss: 0.4853\n",
            "\u001b[1m1292/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7433 - loss: 0.4853\n",
            "\u001b[1m1325/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7434 - loss: 0.4853\n",
            "\u001b[1m1357/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7435 - loss: 0.4853\n",
            "\u001b[1m1390/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7436 - loss: 0.4852\n",
            "\u001b[1m1419/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7436 - loss: 0.4852\n",
            "\u001b[1m1449/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7437 - loss: 0.4852\n",
            "\u001b[1m1479/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7438 - loss: 0.4852\n",
            "\u001b[1m1519/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7439 - loss: 0.4852\n",
            "\u001b[1m1565/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7440 - loss: 0.4851\n",
            "\u001b[1m1616/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7441 - loss: 0.4851\n",
            "\u001b[1m1642/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7441 - loss: 0.4851\n",
            "\u001b[1m1695/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7442 - loss: 0.4851\n",
            "\u001b[1m1750/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7442 - loss: 0.4852\n",
            "\u001b[1m1801/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7443 - loss: 0.4852\n",
            "\u001b[1m1855/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7444 - loss: 0.4852\n",
            "\u001b[1m1888/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7444 - loss: 0.4853\n",
            "\u001b[1m1941/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7445 - loss: 0.4853\n",
            "\u001b[1m1983/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7445 - loss: 0.4853\n",
            "\u001b[1m2040/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7446 - loss: 0.4853\n",
            "\u001b[1m2088/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7447 - loss: 0.4853\n",
            "\u001b[1m2141/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7447 - loss: 0.4853\n",
            "\u001b[1m2189/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7448 - loss: 0.4853\n",
            "\u001b[1m2239/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7449 - loss: 0.4854\n",
            "\u001b[1m2286/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7449 - loss: 0.4854\n",
            "\u001b[1m2339/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7450 - loss: 0.4853\n",
            "\u001b[1m2379/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7450 - loss: 0.4853\n",
            "\u001b[1m2431/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7451 - loss: 0.4854\n",
            "\u001b[1m2476/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7452 - loss: 0.4854\n",
            "\u001b[1m2530/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7452 - loss: 0.4854\n",
            "\u001b[1m2557/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7453 - loss: 0.4854\n",
            "\u001b[1m2610/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7454 - loss: 0.4854\n",
            "\u001b[1m2637/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7454 - loss: 0.4854\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7454 - loss: 0.4854\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 3/5\n",
            "\u001b[1m  28/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7454 - loss: 0.4758   \n",
            "\u001b[1m  79/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7536 - loss: 0.4725\n",
            "\u001b[1m 127/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7547 - loss: 0.4736\n",
            "\u001b[1m 175/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7555 - loss: 0.4740\n",
            "\u001b[1m 213/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7552 - loss: 0.4743\n",
            "\u001b[1m 265/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7546 - loss: 0.4754\n",
            "\u001b[1m 307/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4760\n",
            "\u001b[1m 359/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7538 - loss: 0.4762\n",
            "\u001b[1m 406/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7538 - loss: 0.4767\n",
            "\u001b[1m 462/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7539 - loss: 0.4772\n",
            "\u001b[1m 487/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7539 - loss: 0.4774\n",
            "\u001b[1m 533/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4776\n",
            "\u001b[1m 581/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4777\n",
            "\u001b[1m 622/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4778\n",
            "\u001b[1m 677/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7539 - loss: 0.4779\n",
            "\u001b[1m 713/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7538 - loss: 0.4780\n",
            "\u001b[1m 764/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.4781\n",
            "\u001b[1m 814/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.4780\n",
            "\u001b[1m 866/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.4780\n",
            "\u001b[1m 915/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.4780\n",
            "\u001b[1m 969/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.4779\n",
            "\u001b[1m1016/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.4778\n",
            "\u001b[1m1067/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.4777\n",
            "\u001b[1m1110/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7536 - loss: 0.4777\n",
            "\u001b[1m1160/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7536 - loss: 0.4777\n",
            "\u001b[1m1193/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7536 - loss: 0.4776\n",
            "\u001b[1m1243/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.4776\n",
            "\u001b[1m1266/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.4775\n",
            "\u001b[1m1316/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.4775\n",
            "\u001b[1m1363/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7538 - loss: 0.4774\n",
            "\u001b[1m1420/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7539 - loss: 0.4773\n",
            "\u001b[1m1471/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7539 - loss: 0.4772\n",
            "\u001b[1m1521/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4771\n",
            "\u001b[1m1565/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4771\n",
            "\u001b[1m1610/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4771\n",
            "\u001b[1m1646/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4771\n",
            "\u001b[1m1714/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4771\n",
            "\u001b[1m1760/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4771\n",
            "\u001b[1m1809/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4771\n",
            "\u001b[1m1853/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4772\n",
            "\u001b[1m1904/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4773\n",
            "\u001b[1m1953/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4773\n",
            "\u001b[1m2005/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4774\n",
            "\u001b[1m2049/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4774\n",
            "\u001b[1m2097/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4774\n",
            "\u001b[1m2131/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4775\n",
            "\u001b[1m2178/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4775\n",
            "\u001b[1m2223/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4776\n",
            "\u001b[1m2276/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4777\n",
            "\u001b[1m2321/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4777\n",
            "\u001b[1m2370/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4778\n",
            "\u001b[1m2420/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7539 - loss: 0.4779\n",
            "\u001b[1m2473/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7539 - loss: 0.4779\n",
            "\u001b[1m2521/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7539 - loss: 0.4780\n",
            "\u001b[1m2572/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7538 - loss: 0.4780\n",
            "\u001b[1m2614/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7538 - loss: 0.4781\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.4781\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10\u001b[0m 27ms/step - accuracy: 0.8125 - loss: 0.5210\n",
            "\u001b[1m  50/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7439 - loss: 0.4808\n",
            "\u001b[1m 103/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7440 - loss: 0.4815\n",
            "\u001b[1m 153/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7457 - loss: 0.4820\n",
            "\u001b[1m 206/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7464 - loss: 0.4820\n",
            "\u001b[1m 247/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7458 - loss: 0.4828\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[33m(raylet)\u001b[0m [2025-08-20 07:29:35,744 E 1566 1566] (raylet) node_manager.cc:3064: 1 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
            "\u001b[33m(raylet)\u001b[0m \n",
            "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 270/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7455 - loss: 0.4831\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 293/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7454 - loss: 0.4833\n",
            "\u001b[1m 332/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7454 - loss: 0.4837\n",
            "\u001b[1m 379/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7454 - loss: 0.4839\n",
            "\u001b[1m 411/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7456 - loss: 0.4839\n",
            "\u001b[1m 468/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7458 - loss: 0.4839\n",
            "\u001b[1m 520/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7462 - loss: 0.4837\n",
            "\u001b[1m 574/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7467 - loss: 0.4833\n",
            "\u001b[1m 601/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7469 - loss: 0.4832\n",
            "\u001b[1m 647/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7472 - loss: 0.4829\n",
            "\u001b[1m 686/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7474 - loss: 0.4826\n",
            "\u001b[1m 729/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7475 - loss: 0.4824\n",
            "\u001b[1m 765/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7476 - loss: 0.4822\n",
            "\u001b[1m 792/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7478 - loss: 0.4820\n",
            "\u001b[1m 823/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7479 - loss: 0.4819\n",
            "\u001b[1m 848/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7479 - loss: 0.4818\n",
            "\u001b[1m 879/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7480 - loss: 0.4817\n",
            "\u001b[1m 904/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7480 - loss: 0.4816\n",
            "\u001b[1m 935/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7481 - loss: 0.4816\n",
            "\u001b[1m 965/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7482 - loss: 0.4815\n",
            "\u001b[1m 995/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7483 - loss: 0.4815\n",
            "\u001b[1m1020/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7483 - loss: 0.4815\n",
            "\u001b[1m1054/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7484 - loss: 0.4814\n",
            "\u001b[1m1085/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7485 - loss: 0.4814\n",
            "\u001b[1m1115/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7486 - loss: 0.4814\n",
            "\u001b[1m1140/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7487 - loss: 0.4813\n",
            "\u001b[1m1170/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7487 - loss: 0.4813\n",
            "\u001b[1m1200/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7488 - loss: 0.4813\n",
            "\u001b[1m1232/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7489 - loss: 0.4813\n",
            "\u001b[1m1262/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7489 - loss: 0.4812\n",
            "\u001b[1m1290/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7490 - loss: 0.4812\n",
            "\u001b[1m1319/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7490 - loss: 0.4812\n",
            "\u001b[1m1349/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7491 - loss: 0.4811\n",
            "\u001b[1m1375/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7491 - loss: 0.4811\n",
            "\u001b[1m1408/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7492 - loss: 0.4810\n",
            "\u001b[1m1428/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7492 - loss: 0.4810\n",
            "\u001b[1m1452/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7493 - loss: 0.4810\n",
            "\u001b[1m1480/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7493 - loss: 0.4810\n",
            "\u001b[1m1511/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7494 - loss: 0.4809\n",
            "\u001b[1m1542/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7494 - loss: 0.4809\n",
            "\u001b[1m1569/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7495 - loss: 0.4808\n",
            "\u001b[1m1594/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7495 - loss: 0.4808\n",
            "\u001b[1m1627/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7496 - loss: 0.4808\n",
            "\u001b[1m1651/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7496 - loss: 0.4807\n",
            "\u001b[1m1683/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7497 - loss: 0.4807\n",
            "\u001b[1m1713/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7497 - loss: 0.4807\n",
            "\u001b[1m1738/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7498 - loss: 0.4806\n",
            "\u001b[1m1766/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7498 - loss: 0.4806\n",
            "\u001b[1m1798/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7498 - loss: 0.4805\n",
            "\u001b[1m1825/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7499 - loss: 0.4805\n",
            "\u001b[1m1858/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7499 - loss: 0.4805\n",
            "\u001b[1m1884/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7499 - loss: 0.4804\n",
            "\u001b[1m1913/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7500 - loss: 0.4804\n",
            "\u001b[1m1940/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7500 - loss: 0.4804\n",
            "\u001b[1m1971/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7500 - loss: 0.4804\n",
            "\u001b[1m2002/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7501 - loss: 0.4803\n",
            "\u001b[1m2028/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7501 - loss: 0.4803\n",
            "\u001b[1m2059/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7501 - loss: 0.4803\n",
            "\u001b[1m2094/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7502 - loss: 0.4802\n",
            "\u001b[1m2120/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7502 - loss: 0.4802\n",
            "\u001b[1m2147/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7502 - loss: 0.4802\n",
            "\u001b[1m2168/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7503 - loss: 0.4801\n",
            "\u001b[1m2200/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7503 - loss: 0.4801\n",
            "\u001b[1m2243/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7503 - loss: 0.4801\n",
            "\u001b[1m2297/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7504 - loss: 0.4800\n",
            "\u001b[1m2324/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7504 - loss: 0.4800\n",
            "\u001b[1m2378/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7505 - loss: 0.4799\n",
            "\u001b[1m2420/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7505 - loss: 0.4799\n",
            "\u001b[1m2461/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7506 - loss: 0.4799\n",
            "\u001b[1m2514/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7506 - loss: 0.4798\n",
            "\u001b[1m2557/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7507 - loss: 0.4798\n",
            "\u001b[1m2607/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7507 - loss: 0.4797\n",
            "\u001b[1m2657/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7508 - loss: 0.4797\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7508 - loss: 0.4797\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 5/5\n",
            "\u001b[1m  22/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7741 - loss: 0.4354   \n",
            "\u001b[1m  67/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7580 - loss: 0.4604\n",
            "\u001b[1m 118/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7570 - loss: 0.4681\n",
            "\u001b[1m 167/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7574 - loss: 0.4687\n",
            "\u001b[1m 206/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7579 - loss: 0.4687\n",
            "\u001b[1m 251/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7585 - loss: 0.4684\n",
            "\u001b[1m 302/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7587 - loss: 0.4681\n",
            "\u001b[1m 346/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7587 - loss: 0.4681\n",
            "\u001b[1m 397/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7585 - loss: 0.4684\n",
            "\u001b[1m 450/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7583 - loss: 0.4689\n",
            "\u001b[1m 507/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7579 - loss: 0.4694\n",
            "\u001b[1m 532/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7578 - loss: 0.4695\n",
            "\u001b[1m 585/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7577 - loss: 0.4696\n",
            "\u001b[1m 638/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7576 - loss: 0.4696\n",
            "\u001b[1m 686/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7576 - loss: 0.4695\n",
            "\u001b[1m 727/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7576 - loss: 0.4695\n",
            "\u001b[1m 772/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7576 - loss: 0.4694\n",
            "\u001b[1m 817/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7576 - loss: 0.4694\n",
            "\u001b[1m 860/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7575 - loss: 0.4694\n",
            "\u001b[1m 901/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7575 - loss: 0.4694\n",
            "\u001b[1m 948/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7575 - loss: 0.4695\n",
            "\u001b[1m1000/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7574 - loss: 0.4696\n",
            "\u001b[1m1054/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7573 - loss: 0.4697\n",
            "\u001b[1m1111/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7573 - loss: 0.4698\n",
            "\u001b[1m1161/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7572 - loss: 0.4699\n",
            "\u001b[1m1208/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7572 - loss: 0.4700\n",
            "\u001b[1m1257/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7571 - loss: 0.4701\n",
            "\u001b[1m1314/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7570 - loss: 0.4702\n",
            "\u001b[1m1366/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7569 - loss: 0.4704\n",
            "\u001b[1m1410/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7568 - loss: 0.4705\n",
            "\u001b[1m1483/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7567 - loss: 0.4707\n",
            "\u001b[1m1538/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7566 - loss: 0.4709\n",
            "\u001b[1m1585/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7565 - loss: 0.4709\n",
            "\u001b[1m1638/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7564 - loss: 0.4710\n",
            "\u001b[1m1685/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7563 - loss: 0.4711\n",
            "\u001b[1m1731/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7563 - loss: 0.4711\n",
            "\u001b[1m1778/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7562 - loss: 0.4712\n",
            "\u001b[1m1832/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7562 - loss: 0.4713\n",
            "\u001b[1m1880/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7561 - loss: 0.4713\n",
            "\u001b[1m1935/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7560 - loss: 0.4714\n",
            "\u001b[1m1986/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7560 - loss: 0.4714\n",
            "\u001b[1m2071/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7559 - loss: 0.4715\n",
            "\u001b[1m2125/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7558 - loss: 0.4716\n",
            "\u001b[1m2174/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7558 - loss: 0.4717\n",
            "\u001b[1m2214/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7558 - loss: 0.4717\n",
            "\u001b[1m2260/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7557 - loss: 0.4718\n",
            "\u001b[1m2304/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7557 - loss: 0.4718\n",
            "\u001b[1m2358/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7557 - loss: 0.4718\n",
            "\u001b[1m2409/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7556 - loss: 0.4719\n",
            "\u001b[1m2464/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7556 - loss: 0.4719\n",
            "\u001b[1m2511/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7556 - loss: 0.4719\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 32 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2539/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7556 - loss: 0.4719\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2567/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7555 - loss: 0.4720\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2591/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7555 - loss: 0.4720\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2618/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7555 - loss: 0.4720\n",
            "Server Evaluating... Evaluation Count: 2\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7555 - loss: 0.4720\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client  26 Training complete...\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3ms/step - accuracy: 0.7539 - loss: 0.4817\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2ms/step\n",
            "Prediction:  [[4.8994334e-08 2.2525414e-06 3.0448976e-07 ... 3.5222932e-11\n",
            "  9.2240188e-10 7.3490236e-10]\n",
            " [1.3158612e-09 3.9108596e-09 9.9954790e-01 ... 1.3522811e-17\n",
            "  3.7687841e-17 4.9268062e-10]\n",
            " [2.3424117e-07 2.1797384e-06 8.2406559e-06 ... 2.5749265e-14\n",
            "  4.0675369e-17 7.1993986e-12]\n",
            " ...\n",
            " [4.8035126e-08 1.6898464e-07 4.1381605e-09 ... 1.9324504e-15\n",
            "  4.6557739e-13 7.3036140e-11]\n",
            " [5.9692030e-07 7.8229938e-08 2.3500574e-05 ... 1.0219938e-14\n",
            "  3.4746046e-16 8.6244675e-12]\n",
            " [1.8690309e-06 6.4645718e-07 6.7519999e-05 ... 9.4203704e-14\n",
            "  9.1271010e-15 1.0769770e-10]] (744790, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (2, 0.48286232352256775, {'accuracy': 0.7537144422531128}, 1207.784210543)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 16 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mServer evaluation complete - Accuracy: 0.7537, Loss: 0.4829\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client ID: 20\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client  20 Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \r\u001b[1m   1/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3:16:39\u001b[0m 4s/step - accuracy: 0.6562 - loss: 0.4145\n",
            "\u001b[1m  42/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7703 - loss: 0.4150\n",
            "\u001b[1m  81/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7713 - loss: 0.4337\n",
            "\u001b[1m 114/2653\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4418\n",
            "\u001b[1m 140/2653\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7685 - loss: 0.4463\n",
            "\u001b[1m 169/2653\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7675 - loss: 0.4502\n",
            "\u001b[1m 200/2653\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7662 - loss: 0.4539\n",
            "\u001b[1m 227/2653\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7649 - loss: 0.4572\n",
            "\u001b[1m 252/2653\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7640 - loss: 0.4594\n",
            "\u001b[1m 277/2653\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7633 - loss: 0.4609\n",
            "\u001b[1m 310/2653\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7627 - loss: 0.4623\n",
            "\u001b[1m 345/2653\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7623 - loss: 0.4631\n",
            "\u001b[1m 379/2653\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7620 - loss: 0.4636\n",
            "\u001b[1m 409/2653\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7619 - loss: 0.4638\n",
            "\u001b[1m 425/2653\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7618 - loss: 0.4640\n",
            "\u001b[1m 456/2653\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7616 - loss: 0.4644\n",
            "\u001b[1m 507/2653\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7613 - loss: 0.4649\n",
            "\u001b[1m 565/2653\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7608 - loss: 0.4657\n",
            "\u001b[1m 610/2653\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7604 - loss: 0.4663\n",
            "\u001b[1m 663/2653\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7601 - loss: 0.4671\n",
            "\u001b[1m 720/2653\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7597 - loss: 0.4681\n",
            "\u001b[1m 781/2653\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7592 - loss: 0.4690\n",
            "\u001b[1m 836/2653\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7589 - loss: 0.4697\n",
            "\u001b[1m 902/2653\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7587 - loss: 0.4704\n",
            "\u001b[1m 954/2653\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7585 - loss: 0.4709\n",
            "\u001b[1m1007/2653\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7583 - loss: 0.4713\n",
            "\u001b[1m1064/2653\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7582 - loss: 0.4717\n",
            "\u001b[1m1124/2653\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7581 - loss: 0.4720\n",
            "\u001b[1m1174/2653\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7580 - loss: 0.4722\n",
            "\u001b[1m1217/2653\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7580 - loss: 0.4724\n",
            "\u001b[1m1280/2653\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7579 - loss: 0.4727\n",
            "\u001b[1m1342/2653\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7578 - loss: 0.4729\n",
            "\u001b[1m1409/2653\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7578 - loss: 0.4731\n",
            "\u001b[1m1469/2653\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7578 - loss: 0.4733\n",
            "\u001b[1m1526/2653\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7577 - loss: 0.4734\n",
            "\u001b[1m1555/2653\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7577 - loss: 0.4735\n",
            "\u001b[1m1604/2653\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7576 - loss: 0.4736\n",
            "\u001b[1m1659/2653\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7576 - loss: 0.4737\n",
            "\u001b[1m1714/2653\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7576 - loss: 0.4738\n",
            "\u001b[1m1775/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7575 - loss: 0.4739\n",
            "\u001b[1m1819/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7575 - loss: 0.4740\n",
            "\u001b[1m1888/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7575 - loss: 0.4741\n",
            "\u001b[1m1945/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7574 - loss: 0.4742\n",
            "\u001b[1m2013/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7574 - loss: 0.4743\n",
            "\u001b[1m2068/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7574 - loss: 0.4743\n",
            "\u001b[1m2127/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7574 - loss: 0.4744\n",
            "\u001b[1m2181/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7573 - loss: 0.4745\n",
            "\u001b[1m2245/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7573 - loss: 0.4745\n",
            "\u001b[1m2296/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7573 - loss: 0.4746\n",
            "\u001b[1m2360/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7573 - loss: 0.4747\n",
            "\u001b[1m2406/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7573 - loss: 0.4747\n",
            "\u001b[1m2471/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7572 - loss: 0.4748\n",
            "\u001b[1m2524/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7572 - loss: 0.4749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 15 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 33 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2556/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7572 - loss: 0.4749\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2589/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7572 - loss: 0.4749\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2618/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7572 - loss: 0.4750\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2640/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7572 - loss: 0.4750\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2653/2653\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - accuracy: 0.7572 - loss: 0.4750\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[33mClient 20 evaluation complete - Accuracy: 0.756330, Loss: 0.477863\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client ID: 15\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client  15 Training...\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \r\u001b[1m   1/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:11:37\u001b[0m 6s/step - accuracy: 0.7812 - loss: 0.3787\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m   4/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 23ms/step - accuracy: 0.7565 - loss: 0.4967 \n",
            "\u001b[1m   5/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:30\u001b[0m 34ms/step - accuracy: 0.7577 - loss: 0.5003\n",
            "\u001b[1m  10/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29\u001b[0m 34ms/step - accuracy: 0.7608 - loss: 0.4905\n",
            "\u001b[1m  12/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:33\u001b[0m 35ms/step - accuracy: 0.7635 - loss: 0.4834\n",
            "\u001b[1m  16/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:40\u001b[0m 38ms/step - accuracy: 0.7659 - loss: 0.4764\n",
            "\u001b[1m  18/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:43\u001b[0m 39ms/step - accuracy: 0.7657 - loss: 0.4775\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  19/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 41ms/step - accuracy: 0.7651 - loss: 0.4784\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  20/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 44ms/step - accuracy: 0.7644 - loss: 0.4789\n",
            "\u001b[1m  22/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:53\u001b[0m 43ms/step - accuracy: 0.7637 - loss: 0.4799\n",
            "\u001b[1m  24/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 44ms/step - accuracy: 0.7626 - loss: 0.4811\n",
            "\u001b[1m  25/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:00\u001b[0m 46ms/step - accuracy: 0.7623 - loss: 0.4813\n",
            "\u001b[1m  26/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 49ms/step - accuracy: 0.7622 - loss: 0.4813\n",
            "\u001b[1m  28/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:08\u001b[0m 49ms/step - accuracy: 0.7620 - loss: 0.4808\n",
            "\u001b[1m  29/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:21\u001b[0m 54ms/step - accuracy: 0.7618 - loss: 0.4806\n",
            "\u001b[1m  35/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:05\u001b[0m 48ms/step - accuracy: 0.7599 - loss: 0.4811\n",
            "\u001b[1m  40/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:01\u001b[0m 47ms/step - accuracy: 0.7583 - loss: 0.4822\n",
            "\u001b[1m  43/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:58\u001b[0m 45ms/step - accuracy: 0.7575 - loss: 0.4830\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  47/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 43ms/step - accuracy: 0.7563 - loss: 0.4844\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  48/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 45ms/step - accuracy: 0.7560 - loss: 0.4848\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  50/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 44ms/step - accuracy: 0.7555 - loss: 0.4853\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  52/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 44ms/step - accuracy: 0.7552 - loss: 0.4855\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  53/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:55\u001b[0m 44ms/step - accuracy: 0.7550 - loss: 0.4856\n",
            "\u001b[1m  56/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 44ms/step - accuracy: 0.7545 - loss: 0.4860\n",
            "\u001b[1m  58/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:57\u001b[0m 45ms/step - accuracy: 0.7542 - loss: 0.4862\n",
            "\u001b[1m  60/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:56\u001b[0m 45ms/step - accuracy: 0.7540 - loss: 0.4864\n",
            "\u001b[1m  64/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 43ms/step - accuracy: 0.7538 - loss: 0.4866\n",
            "\u001b[1m  66/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 43ms/step - accuracy: 0.7536 - loss: 0.4868\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  72/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:45\u001b[0m 41ms/step - accuracy: 0.7533 - loss: 0.4873\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  73/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:49\u001b[0m 42ms/step - accuracy: 0.7532 - loss: 0.4874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  74/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:50\u001b[0m 43ms/step - accuracy: 0.7532 - loss: 0.4874\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  75/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:51\u001b[0m 43ms/step - accuracy: 0.7532 - loss: 0.4874\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  77/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:52\u001b[0m 44ms/step - accuracy: 0.7531 - loss: 0.4874\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  78/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:54\u001b[0m 44ms/step - accuracy: 0.7530 - loss: 0.4875\n",
            "\u001b[1m  89/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:42\u001b[0m 40ms/step - accuracy: 0.7529 - loss: 0.4876\n",
            "\u001b[1m 117/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:19\u001b[0m 31ms/step - accuracy: 0.7525 - loss: 0.4871\n",
            "\u001b[1m 145/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:04\u001b[0m 26ms/step - accuracy: 0.7520 - loss: 0.4862\n",
            "\u001b[1m 184/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 21ms/step - accuracy: 0.7511 - loss: 0.4855\n",
            "\u001b[1m 214/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m44s\u001b[0m 18ms/step - accuracy: 0.7508 - loss: 0.4849\n",
            "\u001b[1m 246/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 16ms/step - accuracy: 0.7503 - loss: 0.4848\n",
            "\u001b[1m 268/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 15ms/step - accuracy: 0.7499 - loss: 0.4850\n",
            "\u001b[1m 298/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 14ms/step - accuracy: 0.7498 - loss: 0.4850\n",
            "\u001b[1m 320/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 14ms/step - accuracy: 0.7497 - loss: 0.4850\n",
            "\u001b[1m 352/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 13ms/step - accuracy: 0.7498 - loss: 0.4850\n",
            "\u001b[1m 382/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 12ms/step - accuracy: 0.7500 - loss: 0.4850\n",
            "\u001b[1m 414/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 11ms/step - accuracy: 0.7501 - loss: 0.4849\n",
            "\u001b[1m 440/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - accuracy: 0.7503 - loss: 0.4849\n",
            "\u001b[1m 479/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 10ms/step - accuracy: 0.7505 - loss: 0.4847\n",
            "\u001b[1m 504/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.7507 - loss: 0.4846\n",
            "\u001b[1m 541/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.7509 - loss: 0.4845 \n",
            "\u001b[1m 566/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.7510 - loss: 0.4845\n",
            "\u001b[1m 589/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.7511 - loss: 0.4844\n",
            "\u001b[1m 613/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.7512 - loss: 0.4843\n",
            "\u001b[1m 642/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 9ms/step - accuracy: 0.7514 - loss: 0.4842\n",
            "\u001b[1m 668/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7516 - loss: 0.4841\n",
            "\u001b[1m 694/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7517 - loss: 0.4841\n",
            "\u001b[1m 721/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7519 - loss: 0.4841\n",
            "\u001b[1m 752/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.7520 - loss: 0.4841\n",
            "\u001b[1m 781/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.7521 - loss: 0.4841\n",
            "\u001b[1m 811/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 8ms/step - accuracy: 0.7521 - loss: 0.4841\n",
            "\u001b[1m 837/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7521 - loss: 0.4841\n",
            "\u001b[1m 862/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7522 - loss: 0.4841\n",
            "\u001b[1m 889/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7522 - loss: 0.4841\n",
            "\u001b[1m 939/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7524 - loss: 0.4840\n",
            "\u001b[1m 989/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7525 - loss: 0.4840\n",
            "\u001b[1m1045/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7527 - loss: 0.4839\n",
            "\u001b[1m1094/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7528 - loss: 0.4838 \n",
            "\u001b[1m1145/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7529 - loss: 0.4837\n",
            "\u001b[1m1190/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7530 - loss: 0.4837\n",
            "\u001b[1m1243/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7532 - loss: 0.4836\n",
            "\u001b[1m1295/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.7533 - loss: 0.4836\n",
            "\u001b[1m1341/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7534 - loss: 0.4835\n",
            "\u001b[1m1389/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7535 - loss: 0.4835\n",
            "\u001b[1m1466/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7536 - loss: 0.4835\n",
            "\u001b[1m1511/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7537 - loss: 0.4834\n",
            "\u001b[1m1566/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7538 - loss: 0.4834\n",
            "\u001b[1m1611/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7538 - loss: 0.4834\n",
            "\u001b[1m1662/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7539 - loss: 0.4834\n",
            "\u001b[1m1703/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7540 - loss: 0.4834\n",
            "\u001b[1m1760/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7540 - loss: 0.4834\n",
            "\u001b[1m1804/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7541 - loss: 0.4834\n",
            "\u001b[1m1843/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7541 - loss: 0.4834\n",
            "\u001b[1m1890/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7542 - loss: 0.4834\n",
            "\u001b[1m1937/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7542 - loss: 0.4833\n",
            "\u001b[1m1989/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7543 - loss: 0.4833\n",
            "\u001b[1m2021/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7543 - loss: 0.4833\n",
            "\u001b[1m2055/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7543 - loss: 0.4833\n",
            "\u001b[1m2099/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7543 - loss: 0.4832\n",
            "\u001b[1m2147/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7544 - loss: 0.4832\n",
            "\u001b[1m2194/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7544 - loss: 0.4832\n",
            "\u001b[1m2241/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7544 - loss: 0.4831\n",
            "\u001b[1m2276/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7544 - loss: 0.4831\n",
            "\u001b[1m2323/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7545 - loss: 0.4831\n",
            "\u001b[1m2365/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7545 - loss: 0.4830\n",
            "\u001b[1m2416/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7545 - loss: 0.4830\n",
            "\u001b[1m2458/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7545 - loss: 0.4829\n",
            "\u001b[1m2507/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7546 - loss: 0.4829\n",
            "\u001b[1m2554/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7546 - loss: 0.4828\n",
            "\u001b[1m2606/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7546 - loss: 0.4828\n",
            "\u001b[1m2634/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7546 - loss: 0.4827\n",
            "\u001b[1m2654/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.7546 - loss: 0.4827\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 31ms/step - accuracy: 0.6562 - loss: 0.5587\n",
            "\u001b[1m  56/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7411 - loss: 0.4841\n",
            "\u001b[1m  91/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7453 - loss: 0.4817\n",
            "\u001b[1m 143/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7460 - loss: 0.4811\n",
            "\u001b[1m 188/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7468 - loss: 0.4803\n",
            "\u001b[1m 236/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7472 - loss: 0.4800\n",
            "\u001b[1m 280/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7479 - loss: 0.4793\n",
            "\u001b[1m 333/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7487 - loss: 0.4789\n",
            "\u001b[1m 380/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7491 - loss: 0.4787\n",
            "\u001b[1m 438/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7497 - loss: 0.4785\n",
            "\u001b[1m 480/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7502 - loss: 0.4783\n",
            "\u001b[1m 530/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7507 - loss: 0.4780\n",
            "\u001b[1m 565/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7510 - loss: 0.4778\n",
            "\u001b[1m 614/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7512 - loss: 0.4776\n",
            "\u001b[1m 661/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7514 - loss: 0.4776\n",
            "\u001b[1m 719/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7516 - loss: 0.4776\n",
            "\u001b[1m 767/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7518 - loss: 0.4776\n",
            "\u001b[1m 819/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7520 - loss: 0.4775\n",
            "\u001b[1m 845/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7521 - loss: 0.4774\n",
            "\u001b[1m 896/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7523 - loss: 0.4773\n",
            "\u001b[1m 951/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7524 - loss: 0.4772\n",
            "\u001b[1m 998/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7525 - loss: 0.4772\n",
            "\u001b[1m1051/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7527 - loss: 0.4771\n",
            "\u001b[1m1082/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7527 - loss: 0.4771\n",
            "\u001b[1m1133/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7528 - loss: 0.4771\n",
            "\u001b[1m1183/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7530 - loss: 0.4771\n",
            "\u001b[1m1235/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7531 - loss: 0.4771\n",
            "\u001b[1m1284/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7531 - loss: 0.4770\n",
            "\u001b[1m1332/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7532 - loss: 0.4770\n",
            "\u001b[1m1378/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7533 - loss: 0.4770\n",
            "\u001b[1m1430/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7534 - loss: 0.4770\n",
            "\u001b[1m1455/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7534 - loss: 0.4770\n",
            "\u001b[1m1503/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7535 - loss: 0.4770\n",
            "\u001b[1m1557/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7536 - loss: 0.4770\n",
            "\u001b[1m1591/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7536 - loss: 0.4770\n",
            "\u001b[1m1645/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7537 - loss: 0.4770\n",
            "\u001b[1m1689/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7538 - loss: 0.4770\n",
            "\u001b[1m1745/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7538 - loss: 0.4770\n",
            "\u001b[1m1791/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7539 - loss: 0.4769\n",
            "\u001b[1m1847/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4769\n",
            "\u001b[1m1896/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7540 - loss: 0.4768\n",
            "\u001b[1m1955/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4768\n",
            "\u001b[1m2002/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7542 - loss: 0.4767\n",
            "\u001b[1m2053/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7542 - loss: 0.4767\n",
            "\u001b[1m2088/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7543 - loss: 0.4766\n",
            "\u001b[1m2137/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7543 - loss: 0.4766\n",
            "\u001b[1m2179/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7544 - loss: 0.4766\n",
            "\u001b[1m2233/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7544 - loss: 0.4765\n",
            "\u001b[1m2255/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7544 - loss: 0.4765\n",
            "\u001b[1m2304/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7545 - loss: 0.4765\n",
            "\u001b[1m2357/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7545 - loss: 0.4764\n",
            "\u001b[1m2404/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7546 - loss: 0.4764\n",
            "\u001b[1m2460/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7546 - loss: 0.4764\n",
            "\u001b[1m2501/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7546 - loss: 0.4764\n",
            "\u001b[1m2555/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7547 - loss: 0.4763\n",
            "\u001b[1m2586/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7547 - loss: 0.4763\n",
            "\u001b[1m2637/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7548 - loss: 0.4763\n",
            "\u001b[1m2654/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7548 - loss: 0.4763\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:15\u001b[0m 29ms/step - accuracy: 0.8125 - loss: 0.4840\n",
            "\u001b[1m  43/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7541 - loss: 0.4771\n",
            "\u001b[1m 100/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7528 - loss: 0.4790\n",
            "\u001b[1m 150/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7554 - loss: 0.4763\n",
            "\u001b[1m 207/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7571 - loss: 0.4745\n",
            "\u001b[1m 237/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7570 - loss: 0.4747\n",
            "\u001b[1m 272/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7568 - loss: 0.4750\n",
            "\u001b[1m 296/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7567 - loss: 0.4750\n",
            "\u001b[1m 324/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7568 - loss: 0.4750\n",
            "\u001b[1m 352/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7569 - loss: 0.4750\n",
            "\u001b[1m 389/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7571 - loss: 0.4749\n",
            "\u001b[1m 422/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7572 - loss: 0.4748\n",
            "\u001b[1m 454/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7571 - loss: 0.4747\n",
            "\u001b[1m 481/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7571 - loss: 0.4747\n",
            "\u001b[1m 515/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7570 - loss: 0.4746\n",
            "\u001b[1m 545/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7570 - loss: 0.4746\n",
            "\u001b[1m 574/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7570 - loss: 0.4747\n",
            "\u001b[1m 598/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7570 - loss: 0.4747\n",
            "\u001b[1m 631/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7570 - loss: 0.4746\n",
            "\u001b[1m 650/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7570 - loss: 0.4745\n",
            "\u001b[1m 685/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7571 - loss: 0.4744\n",
            "\u001b[1m 712/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7572 - loss: 0.4743\n",
            "\u001b[1m 744/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7572 - loss: 0.4743\n",
            "\u001b[1m 772/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7572 - loss: 0.4743\n",
            "\u001b[1m 805/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7572 - loss: 0.4742\n",
            "\u001b[1m 826/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7572 - loss: 0.4742\n",
            "\u001b[1m 860/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7572 - loss: 0.4742\n",
            "\u001b[1m 892/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7573 - loss: 0.4742\n",
            "\u001b[1m 929/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7573 - loss: 0.4741\n",
            "\u001b[1m 953/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7573 - loss: 0.4740\n",
            "\u001b[1m 989/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7573 - loss: 0.4740\n",
            "\u001b[1m1014/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7574 - loss: 0.4739\n",
            "\u001b[1m1051/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7574 - loss: 0.4738\n",
            "\u001b[1m1076/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7574 - loss: 0.4737\n",
            "\u001b[1m1101/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7575 - loss: 0.4737\n",
            "\u001b[1m1130/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7575 - loss: 0.4736\n",
            "\u001b[1m1159/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4735\n",
            "\u001b[1m1182/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4735\n",
            "\u001b[1m1211/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4735\n",
            "\u001b[1m1238/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4735\n",
            "\u001b[1m1260/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4735\n",
            "\u001b[1m1279/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7575 - loss: 0.4735\n",
            "\u001b[1m1310/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7575 - loss: 0.4735\n",
            "\u001b[1m1340/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7575 - loss: 0.4735\n",
            "\u001b[1m1364/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4735\n",
            "\u001b[1m1386/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4735\n",
            "\u001b[1m1415/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4734\n",
            "\u001b[1m1438/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4734\n",
            "\u001b[1m1466/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4734\n",
            "\u001b[1m1493/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4734\n",
            "\u001b[1m1517/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4733\n",
            "\u001b[1m1540/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4733\n",
            "\u001b[1m1571/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4733\n",
            "\u001b[1m1601/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4733\n",
            "\u001b[1m1631/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4732\n",
            "\u001b[1m1656/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4732\n",
            "\u001b[1m1683/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4732\n",
            "\u001b[1m1715/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4731\n",
            "\u001b[1m1765/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4731\n",
            "\u001b[1m1788/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7576 - loss: 0.4731\n",
            "\u001b[1m1840/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7577 - loss: 0.4730\n",
            "\u001b[1m1861/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7577 - loss: 0.4730\n",
            "\u001b[1m1883/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7577 - loss: 0.4730\n",
            "\u001b[1m1930/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7577 - loss: 0.4729\n",
            "\u001b[1m1979/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7577 - loss: 0.4729\n",
            "\u001b[1m2024/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7578 - loss: 0.4728\n",
            "\u001b[1m2075/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7578 - loss: 0.4728\n",
            "\u001b[1m2126/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7578 - loss: 0.4727\n",
            "\u001b[1m2177/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7579 - loss: 0.4727\n",
            "\u001b[1m2223/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7579 - loss: 0.4726\n",
            "\u001b[1m2276/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7579 - loss: 0.4726\n",
            "\u001b[1m2326/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7580 - loss: 0.4725\n",
            "\u001b[1m2367/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7580 - loss: 0.4725\n",
            "\u001b[1m2416/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7580 - loss: 0.4725\n",
            "\u001b[1m2472/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7581 - loss: 0.4725\n",
            "\u001b[1m2517/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7581 - loss: 0.4724\n",
            "\u001b[1m2573/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7581 - loss: 0.4724\n",
            "\u001b[1m2600/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7581 - loss: 0.4724\n",
            "\u001b[1m2650/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7581 - loss: 0.4723\n",
            "\u001b[1m2654/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7582 - loss: 0.4723\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21\u001b[0m 31ms/step - accuracy: 0.9062 - loss: 0.3298\n",
            "\u001b[1m  21/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7687 - loss: 0.4649   \n",
            "\u001b[1m  73/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4639\n",
            "\u001b[1m 130/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4675\n",
            "\u001b[1m 173/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7618 - loss: 0.4684\n",
            "\u001b[1m 223/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7621 - loss: 0.4681\n",
            "\u001b[1m 272/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7622 - loss: 0.4675\n",
            "\u001b[1m 323/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7621 - loss: 0.4676\n",
            "\u001b[1m 364/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7618 - loss: 0.4680\n",
            "\u001b[1m 418/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4681\n",
            "\u001b[1m 465/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4683\n",
            "\u001b[1m 520/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7615 - loss: 0.4687\n",
            "\u001b[1m 569/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7613 - loss: 0.4690\n",
            "\u001b[1m 627/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4693\n",
            "\u001b[1m 677/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4695\n",
            "\u001b[1m 723/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4696\n",
            "\u001b[1m 772/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4698\n",
            "\u001b[1m 825/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4699\n",
            "\u001b[1m 853/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4699\n",
            "\u001b[1m 906/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4700\n",
            "\u001b[1m 963/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4700\n",
            "\u001b[1m1012/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.4700\n",
            "\u001b[1m1065/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.4700\n",
            "\u001b[1m1111/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7613 - loss: 0.4700\n",
            "\u001b[1m1163/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7614 - loss: 0.4699\n",
            "\u001b[1m1204/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7615 - loss: 0.4699\n",
            "\u001b[1m1250/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7615 - loss: 0.4698\n",
            "\u001b[1m1296/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4698\n",
            "\u001b[1m1352/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4698\n",
            "\u001b[1m1404/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4697\n",
            "\u001b[1m1485/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4697\n",
            "\u001b[1m1535/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4697\n",
            "\u001b[1m1592/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4696\n",
            "\u001b[1m1641/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4696\n",
            "\u001b[1m1696/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4696\n",
            "\u001b[1m1733/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4695\n",
            "\u001b[1m1791/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4695\n",
            "\u001b[1m1842/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4694\n",
            "\u001b[1m1897/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4694\n",
            "\u001b[1m1945/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4693\n",
            "\u001b[1m1997/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4693\n",
            "\u001b[1m2044/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4692\n",
            "\u001b[1m2103/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4692\n",
            "\u001b[1m2148/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4692\n",
            "\u001b[1m2203/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4691\n",
            "\u001b[1m2229/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4691\n",
            "\u001b[1m2268/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4691\n",
            "\u001b[1m2321/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4691\n",
            "\u001b[1m2372/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4691\n",
            "\u001b[1m2428/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4690\n",
            "\u001b[1m2478/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4690\n",
            "\u001b[1m2534/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4690\n",
            "\u001b[1m2580/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4690\n",
            "\u001b[1m2637/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4690\n",
            "\u001b[1m2654/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4690\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:12\u001b[0m 27ms/step - accuracy: 0.7500 - loss: 0.4431\n",
            "\u001b[1m  58/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7632 - loss: 0.4707\n",
            "\u001b[1m  97/2654\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7649 - loss: 0.4693\n",
            "\u001b[1m 151/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7644 - loss: 0.4674\n",
            "\u001b[1m 197/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7635 - loss: 0.4667\n",
            "\u001b[1m 244/2654\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7629 - loss: 0.4660\n",
            "\u001b[1m 288/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7628 - loss: 0.4648\n",
            "\u001b[1m 337/2654\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7628 - loss: 0.4643\n",
            "\u001b[1m 415/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7628 - loss: 0.4638\n",
            "\u001b[1m 470/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7626 - loss: 0.4637\n",
            "\u001b[1m 515/2654\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7623 - loss: 0.4637\n",
            "\u001b[1m 570/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7621 - loss: 0.4636\n",
            "\u001b[1m 604/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7621 - loss: 0.4634\n",
            "\u001b[1m 659/2654\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7620 - loss: 0.4633\n",
            "\u001b[1m 702/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7618 - loss: 0.4632\n",
            "\u001b[1m 753/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4630\n",
            "\u001b[1m 795/2654\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4629\n",
            "\u001b[1m 841/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7615 - loss: 0.4628\n",
            "\u001b[1m 883/2654\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7614 - loss: 0.4628\n",
            "\u001b[1m 937/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7613 - loss: 0.4627\n",
            "\u001b[1m 963/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7613 - loss: 0.4626\n",
            "\u001b[1m1007/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.4626\n",
            "\u001b[1m1057/2654\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.4625\n",
            "\u001b[1m1094/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.4624\n",
            "\u001b[1m1146/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4624\n",
            "\u001b[1m1188/2654\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4623\n",
            "\u001b[1m1218/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4623\n",
            "\u001b[1m1248/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4623\n",
            "\u001b[1m1281/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4623\n",
            "\u001b[1m1303/2654\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4623\n",
            "\u001b[1m1338/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4623\n",
            "\u001b[1m1371/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4623\n",
            "\u001b[1m1405/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4623\n",
            "\u001b[1m1429/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.4623\n",
            "\u001b[1m1458/2654\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.4622\n",
            "\u001b[1m1488/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.4622\n",
            "\u001b[1m1519/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.4622\n",
            "\u001b[1m1544/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.4622\n",
            "\u001b[1m1578/2654\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.4622\n",
            "\u001b[1m1610/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4623\n",
            "\u001b[1m1646/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4623\n",
            "\u001b[1m1677/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.4623\n",
            "\u001b[1m1707/2654\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.4623\n",
            "\u001b[1m1740/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.4624\n",
            "\u001b[1m1765/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.4624\n",
            "\u001b[1m1794/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.4625\n",
            "\u001b[1m1826/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.4625\n",
            "\u001b[1m1847/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.4625\n",
            "\u001b[1m1876/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.4625\n",
            "\u001b[1m1903/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.4626\n",
            "\u001b[1m1942/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.4626\n",
            "\u001b[1m1962/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7611 - loss: 0.4626\n",
            "\u001b[1m1994/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7610 - loss: 0.4627\n",
            "\u001b[1m2021/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7610 - loss: 0.4627\n",
            "\u001b[1m2040/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7610 - loss: 0.4627\n",
            "\u001b[1m2070/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7610 - loss: 0.4628\n",
            "\u001b[1m2102/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7610 - loss: 0.4628\n",
            "\u001b[1m2126/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7610 - loss: 0.4628\n",
            "\u001b[1m2159/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7610 - loss: 0.4629\n",
            "\u001b[1m2182/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7610 - loss: 0.4629\n",
            "\u001b[1m2210/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4629\n",
            "\u001b[1m2234/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4630\n",
            "\u001b[1m2265/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4630\n",
            "\u001b[1m2287/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4630\n",
            "\u001b[1m2310/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4630\n",
            "\u001b[1m2330/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4631\n",
            "\u001b[1m2357/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4631\n",
            "\u001b[1m2379/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4631\n",
            "\u001b[1m2408/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4631\n",
            "\u001b[1m2436/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4631\n",
            "\u001b[1m2461/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4631\n",
            "\u001b[1m2469/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4631\n",
            "\u001b[1m2505/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4632\n",
            "\u001b[1m2534/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7609 - loss: 0.4632\n",
            "\u001b[1m2562/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7608 - loss: 0.4632\n",
            "\u001b[1m2582/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7608 - loss: 0.4632\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 32 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2596/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7608 - loss: 0.4632\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2610/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7608 - loss: 0.4632\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2622/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7608 - loss: 0.4633\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2634/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7608 - loss: 0.4633\n",
            "Server Evaluating... Evaluation Count: 3\n",
            "\u001b[1m2654/2654\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7608 - loss: 0.4633\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client  15 Training complete...\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3ms/step - accuracy: 0.7524 - loss: 0.4959\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m37s\u001b[0m 2ms/step\n",
            "Prediction:  [[3.08742045e-08 1.08829045e-05 1.95107987e-06 ... 1.44646317e-11\n",
            "  3.93291790e-13 3.44347727e-12]\n",
            " [2.17596354e-13 1.37907058e-12 9.99966562e-01 ... 8.64539071e-28\n",
            "  0.00000000e+00 1.75984471e-13]\n",
            " [8.16941821e-08 7.44211150e-07 1.29655655e-06 ... 1.65032477e-14\n",
            "  1.99441332e-27 4.05978487e-14]\n",
            " ...\n",
            " [2.32564344e-06 9.33764976e-09 2.43158693e-09 ... 3.65070360e-13\n",
            "  1.67411681e-16 9.67141958e-08]\n",
            " [3.48064873e-07 9.85741266e-09 3.90411151e-06 ... 1.89979474e-15\n",
            "  3.49557086e-26 9.81191277e-15]\n",
            " [1.59367323e-06 4.55561974e-07 8.21343783e-05 ... 1.38073934e-14\n",
            "  9.11040077e-24 5.25202874e-13]] (744790, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (3, 0.4959028363227844, {'accuracy': 0.7527961134910583}, 1427.4561499440001)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 16 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mServer evaluation complete - Accuracy: 0.7528, Loss: 0.4959\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client ID: 3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client  3 Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \r\u001b[1m   1/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:09:33\u001b[0m 2s/step - accuracy: 0.8125 - loss: 0.3813\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m   7/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.7591 - loss: 0.4602   \n",
            "\u001b[1m  21/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 8ms/step - accuracy: 0.7458 - loss: 0.4851\n",
            "\u001b[1m  28/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.7467 - loss: 0.4860\n",
            "\u001b[1m  42/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.7438 - loss: 0.4923\n",
            "\u001b[1m  50/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.7413 - loss: 0.4956\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  55/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.7399 - loss: 0.4970\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  56/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 17ms/step - accuracy: 0.7397 - loss: 0.4972\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  72/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 14ms/step - accuracy: 0.7365 - loss: 0.4990\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  88/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 12ms/step - accuracy: 0.7339 - loss: 0.5000\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 104/2352\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - accuracy: 0.7320 - loss: 0.5015\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 120/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.7312 - loss: 0.5026\n",
            "\u001b[1m 126/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 10ms/step - accuracy: 0.7311 - loss: 0.5030\n",
            "\u001b[1m 127/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.7311 - loss: 0.5031\n",
            "\u001b[1m 128/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 12ms/step - accuracy: 0.7311 - loss: 0.5031\n",
            "\u001b[1m 135/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 12ms/step - accuracy: 0.7310 - loss: 0.5034\n",
            "\u001b[1m 148/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 12ms/step - accuracy: 0.7310 - loss: 0.5036\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 155/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 12ms/step - accuracy: 0.7310 - loss: 0.5037\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 164/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - accuracy: 0.7308 - loss: 0.5039\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 169/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - accuracy: 0.7307 - loss: 0.5041\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 173/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - accuracy: 0.7305 - loss: 0.5043\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 178/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - accuracy: 0.7304 - loss: 0.5045\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 188/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 11ms/step - accuracy: 0.7303 - loss: 0.5050\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 195/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 11ms/step - accuracy: 0.7303 - loss: 0.5054\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 200/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 11ms/step - accuracy: 0.7302 - loss: 0.5056\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 203/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 11ms/step - accuracy: 0.7302 - loss: 0.5057\n",
            "\u001b[1m 215/2352\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 11ms/step - accuracy: 0.7301 - loss: 0.5061\n",
            "\u001b[1m 251/2352\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 10ms/step - accuracy: 0.7301 - loss: 0.5072\n",
            "\u001b[1m 277/2352\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.7299 - loss: 0.5079 \n",
            "\u001b[1m 336/2352\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7294 - loss: 0.5094\n",
            "\u001b[1m 402/2352\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7291 - loss: 0.5100\n",
            "\u001b[1m 445/2352\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7289 - loss: 0.5102\n",
            "\u001b[1m 505/2352\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7288 - loss: 0.5107\n",
            "\u001b[1m 553/2352\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7287 - loss: 0.5107 \n",
            "\u001b[1m 602/2352\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7287 - loss: 0.5106\n",
            "\u001b[1m 630/2352\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7287 - loss: 0.5106\n",
            "\u001b[1m 664/2352\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7287 - loss: 0.5106\n",
            "\u001b[1m 696/2352\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7288 - loss: 0.5107\n",
            "\u001b[1m 732/2352\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7288 - loss: 0.5108\n",
            "\u001b[1m 765/2352\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7288 - loss: 0.5110\n",
            "\u001b[1m 810/2352\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7288 - loss: 0.5112\n",
            "\u001b[1m 828/2352\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7288 - loss: 0.5113\n",
            "\u001b[1m 865/2352\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7288 - loss: 0.5114\n",
            "\u001b[1m 889/2352\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7288 - loss: 0.5115\n",
            "\u001b[1m 929/2352\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7287 - loss: 0.5117\n",
            "\u001b[1m 955/2352\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7286 - loss: 0.5118\n",
            "\u001b[1m 991/2352\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7286 - loss: 0.5120\n",
            "\u001b[1m1029/2352\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7285 - loss: 0.5121\n",
            "\u001b[1m1064/2352\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7284 - loss: 0.5123\n",
            "\u001b[1m1099/2352\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7283 - loss: 0.5124\n",
            "\u001b[1m1136/2352\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7282 - loss: 0.5125\n",
            "\u001b[1m1165/2352\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7282 - loss: 0.5125\n",
            "\u001b[1m1193/2352\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7281 - loss: 0.5126\n",
            "\u001b[1m1220/2352\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7281 - loss: 0.5126\n",
            "\u001b[1m1251/2352\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7281 - loss: 0.5127\n",
            "\u001b[1m1283/2352\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7280 - loss: 0.5127\n",
            "\u001b[1m1316/2352\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7280 - loss: 0.5128\n",
            "\u001b[1m1351/2352\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7280 - loss: 0.5129\n",
            "\u001b[1m1388/2352\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5130\n",
            "\u001b[1m1422/2352\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5130\n",
            "\u001b[1m1454/2352\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5131\n",
            "\u001b[1m1486/2352\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5131\n",
            "\u001b[1m1516/2352\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5131\n",
            "\u001b[1m1540/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5132\n",
            "\u001b[1m1578/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5132\n",
            "\u001b[1m1605/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5132\n",
            "\u001b[1m1638/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5132\n",
            "\u001b[1m1675/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5132\n",
            "\u001b[1m1710/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5133\n",
            "\u001b[1m1743/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5133\n",
            "\u001b[1m1775/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5133\n",
            "\u001b[1m1809/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5133\n",
            "\u001b[1m1842/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5133\n",
            "\u001b[1m1876/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5133\n",
            "\u001b[1m1915/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5133\n",
            "\u001b[1m1945/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5132\n",
            "\u001b[1m1980/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5132\n",
            "\u001b[1m2007/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5132\n",
            "\u001b[1m2051/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5132\n",
            "\u001b[1m2090/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5132\n",
            "\u001b[1m2122/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7278 - loss: 0.5132\n",
            "\u001b[1m2157/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5131\n",
            "\u001b[1m2194/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5131\n",
            "\u001b[1m2219/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5131\n",
            "\u001b[1m2246/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5131\n",
            "\u001b[1m2276/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5131\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 15 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 33 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2293/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5131\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2310/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5131\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2336/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5131\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2352/2352\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 4ms/step - accuracy: 0.7279 - loss: 0.5131\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[33mClient 3 evaluation complete - Accuracy: 0.728785, Loss: 0.511965\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client ID: 32\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client  32 Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \r\u001b[1m   1/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:11:47\u001b[0m 6s/step - accuracy: 0.7188 - loss: 0.6058\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m   5/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 14ms/step - accuracy: 0.7474 - loss: 0.5635  \n",
            "\u001b[1m  14/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.7574 - loss: 0.5755\n",
            "\u001b[1m  18/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m43s\u001b[0m 17ms/step - accuracy: 0.7581 - loss: 0.5642\n",
            "\u001b[1m  28/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 14ms/step - accuracy: 0.7633 - loss: 0.5416\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  32/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 15ms/step - accuracy: 0.7640 - loss: 0.5351\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  37/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 15ms/step - accuracy: 0.7651 - loss: 0.5276\n",
            "\u001b[1m  45/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 15ms/step - accuracy: 0.7673 - loss: 0.5169\n",
            "\u001b[1m  50/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 14ms/step - accuracy: 0.7680 - loss: 0.5125\n",
            "\u001b[1m  56/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 14ms/step - accuracy: 0.7686 - loss: 0.5094\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  58/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 14ms/step - accuracy: 0.7687 - loss: 0.5087\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  59/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 15ms/step - accuracy: 0.7687 - loss: 0.5084\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  64/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 15ms/step - accuracy: 0.7692 - loss: 0.5066\n",
            "\u001b[1m  66/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 16ms/step - accuracy: 0.7693 - loss: 0.5060\n",
            "\u001b[1m  73/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 16ms/step - accuracy: 0.7696 - loss: 0.5045\n",
            "\u001b[1m  79/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m40s\u001b[0m 16ms/step - accuracy: 0.7696 - loss: 0.5036\n",
            "\u001b[1m  92/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m37s\u001b[0m 15ms/step - accuracy: 0.7698 - loss: 0.5015\n",
            "\u001b[1m 104/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 14ms/step - accuracy: 0.7702 - loss: 0.4993\n",
            "\u001b[1m 120/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.7704 - loss: 0.4976\n",
            "\u001b[1m 130/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.7704 - loss: 0.4967\n",
            "\u001b[1m 136/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 13ms/step - accuracy: 0.7702 - loss: 0.4964\n",
            "\u001b[1m 143/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 13ms/step - accuracy: 0.7700 - loss: 0.4962\n",
            "\u001b[1m 151/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 13ms/step - accuracy: 0.7697 - loss: 0.4958\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 154/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 13ms/step - accuracy: 0.7696 - loss: 0.4957\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 158/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 13ms/step - accuracy: 0.7695 - loss: 0.4955\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 163/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 13ms/step - accuracy: 0.7694 - loss: 0.4953\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 168/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 13ms/step - accuracy: 0.7693 - loss: 0.4951\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 171/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 13ms/step - accuracy: 0.7692 - loss: 0.4950\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 172/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 13ms/step - accuracy: 0.7692 - loss: 0.4950\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 173/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.7692 - loss: 0.4949\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 174/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 14ms/step - accuracy: 0.7692 - loss: 0.4949\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m 181/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 14ms/step - accuracy: 0.7690 - loss: 0.4949\n",
            "\u001b[1m 195/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 14ms/step - accuracy: 0.7685 - loss: 0.4948\n",
            "\u001b[1m 197/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 14ms/step - accuracy: 0.7684 - loss: 0.4948\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 198/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 15ms/step - accuracy: 0.7684 - loss: 0.4948\n",
            "\u001b[1m 215/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m34s\u001b[0m 14ms/step - accuracy: 0.7679 - loss: 0.4946\n",
            "\u001b[1m 263/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 12ms/step - accuracy: 0.7671 - loss: 0.4933\n",
            "\u001b[1m 309/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - accuracy: 0.7667 - loss: 0.4917\n",
            "\u001b[1m 364/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 9ms/step - accuracy: 0.7664 - loss: 0.4906 \n",
            "\u001b[1m 400/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.7662 - loss: 0.4902\n",
            "\u001b[1m 436/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7660 - loss: 0.4897\n",
            "\u001b[1m 464/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7659 - loss: 0.4893\n",
            "\u001b[1m 497/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7657 - loss: 0.4888\n",
            "\u001b[1m 541/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7655 - loss: 0.4882\n",
            "\u001b[1m 558/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7654 - loss: 0.4880\n",
            "\u001b[1m 575/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7654 - loss: 0.4877\n",
            "\u001b[1m 599/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7653 - loss: 0.4874\n",
            "\u001b[1m 634/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7652 - loss: 0.4869\n",
            "\u001b[1m 665/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7652 - loss: 0.4866\n",
            "\u001b[1m 698/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7651 - loss: 0.4861\n",
            "\u001b[1m 719/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7651 - loss: 0.4859\n",
            "\u001b[1m 750/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7651 - loss: 0.4855\n",
            "\u001b[1m 777/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7651 - loss: 0.4853\n",
            "\u001b[1m 810/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7650 - loss: 0.4850\n",
            "\u001b[1m 835/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7650 - loss: 0.4849\n",
            "\u001b[1m 867/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7649 - loss: 0.4847\n",
            "\u001b[1m 892/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7648 - loss: 0.4845\n",
            "\u001b[1m 927/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7648 - loss: 0.4842 \n",
            "\u001b[1m 958/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7647 - loss: 0.4840\n",
            "\u001b[1m 988/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7646 - loss: 0.4839\n",
            "\u001b[1m1014/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7646 - loss: 0.4837\n",
            "\u001b[1m1051/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7645 - loss: 0.4836\n",
            "\u001b[1m1082/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7645 - loss: 0.4834\n",
            "\u001b[1m1112/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7645 - loss: 0.4833\n",
            "\u001b[1m1137/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7644 - loss: 0.4832\n",
            "\u001b[1m1170/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7644 - loss: 0.4831\n",
            "\u001b[1m1199/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7644 - loss: 0.4830\n",
            "\u001b[1m1228/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7644 - loss: 0.4829\n",
            "\u001b[1m1263/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4828\n",
            "\u001b[1m1292/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4827\n",
            "\u001b[1m1318/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4826\n",
            "\u001b[1m1345/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4826\n",
            "\u001b[1m1369/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4825\n",
            "\u001b[1m1398/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4824\n",
            "\u001b[1m1427/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4823\n",
            "\u001b[1m1460/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4822\n",
            "\u001b[1m1484/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4821\n",
            "\u001b[1m1514/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4820\n",
            "\u001b[1m1547/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4819\n",
            "\u001b[1m1580/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4818\n",
            "\u001b[1m1603/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4817\n",
            "\u001b[1m1630/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4816\n",
            "\u001b[1m1653/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4815\n",
            "\u001b[1m1679/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4814\n",
            "\u001b[1m1705/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4813\n",
            "\u001b[1m1738/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4812\n",
            "\u001b[1m1760/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4811\n",
            "\u001b[1m1788/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4810\n",
            "\u001b[1m1810/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4809\n",
            "\u001b[1m1842/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4808\n",
            "\u001b[1m1867/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4807\n",
            "\u001b[1m1893/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7643 - loss: 0.4806\n",
            "\u001b[1m1913/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7642 - loss: 0.4806\n",
            "\u001b[1m1951/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7642 - loss: 0.4805\n",
            "\u001b[1m1996/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7642 - loss: 0.4803\n",
            "\u001b[1m2052/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7641 - loss: 0.4802\n",
            "\u001b[1m2098/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7641 - loss: 0.4801\n",
            "\u001b[1m2151/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7641 - loss: 0.4800\n",
            "\u001b[1m2190/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7641 - loss: 0.4799\n",
            "\u001b[1m2242/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7641 - loss: 0.4797\n",
            "\u001b[1m2287/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7640 - loss: 0.4796\n",
            "\u001b[1m2340/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7640 - loss: 0.4795\n",
            "\u001b[1m2383/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7640 - loss: 0.4794\n",
            "\u001b[1m2430/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7640 - loss: 0.4792\n",
            "\u001b[1m2476/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7640 - loss: 0.4791\n",
            "\u001b[1m2529/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7639 - loss: 0.4790\n",
            "\u001b[1m2571/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7639 - loss: 0.4790\n",
            "\u001b[1m2620/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7639 - loss: 0.4789\n",
            "\u001b[1m2657/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m16s\u001b[0m 4ms/step - accuracy: 0.7639 - loss: 0.4788\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 2/5\n",
            "\u001b[1m  26/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7661 - loss: 0.4723   \n",
            "\u001b[1m  69/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7565 - loss: 0.4739\n",
            "\u001b[1m 121/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7542 - loss: 0.4742\n",
            "\u001b[1m 146/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7535 - loss: 0.4752\n",
            "\u001b[1m 193/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7534 - loss: 0.4761\n",
            "\u001b[1m 244/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7546 - loss: 0.4754\n",
            "\u001b[1m 283/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7553 - loss: 0.4748\n",
            "\u001b[1m 332/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7560 - loss: 0.4743\n",
            "\u001b[1m 377/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7565 - loss: 0.4738\n",
            "\u001b[1m 430/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7571 - loss: 0.4732\n",
            "\u001b[1m 461/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7574 - loss: 0.4730\n",
            "\u001b[1m 512/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7580 - loss: 0.4726\n",
            "\u001b[1m 560/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7585 - loss: 0.4721\n",
            "\u001b[1m 609/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7589 - loss: 0.4716\n",
            "\u001b[1m 650/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7592 - loss: 0.4714\n",
            "\u001b[1m 699/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7595 - loss: 0.4712\n",
            "\u001b[1m 746/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7597 - loss: 0.4710\n",
            "\u001b[1m 797/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7599 - loss: 0.4708\n",
            "\u001b[1m 869/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7602 - loss: 0.4706\n",
            "\u001b[1m 923/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7604 - loss: 0.4705\n",
            "\u001b[1m 951/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7605 - loss: 0.4705\n",
            "\u001b[1m1004/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7606 - loss: 0.4704\n",
            "\u001b[1m1039/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7607 - loss: 0.4703\n",
            "\u001b[1m1091/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7609 - loss: 0.4702\n",
            "\u001b[1m1138/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7610 - loss: 0.4701\n",
            "\u001b[1m1188/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7611 - loss: 0.4699\n",
            "\u001b[1m1233/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7612 - loss: 0.4698\n",
            "\u001b[1m1284/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7613 - loss: 0.4697\n",
            "\u001b[1m1310/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7613 - loss: 0.4696\n",
            "\u001b[1m1356/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7614 - loss: 0.4695\n",
            "\u001b[1m1404/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7615 - loss: 0.4695\n",
            "\u001b[1m1438/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7615 - loss: 0.4694\n",
            "\u001b[1m1484/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7615 - loss: 0.4693\n",
            "\u001b[1m1521/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4693\n",
            "\u001b[1m1570/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4692\n",
            "\u001b[1m1610/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7616 - loss: 0.4692\n",
            "\u001b[1m1654/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4691\n",
            "\u001b[1m1697/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7617 - loss: 0.4691\n",
            "\u001b[1m1750/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7618 - loss: 0.4690\n",
            "\u001b[1m1806/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7618 - loss: 0.4689\n",
            "\u001b[1m1850/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7619 - loss: 0.4689\n",
            "\u001b[1m1892/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7619 - loss: 0.4688\n",
            "\u001b[1m1931/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7619 - loss: 0.4688\n",
            "\u001b[1m1983/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7620 - loss: 0.4687\n",
            "\u001b[1m2029/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7620 - loss: 0.4686\n",
            "\u001b[1m2081/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7621 - loss: 0.4685\n",
            "\u001b[1m2131/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7621 - loss: 0.4685\n",
            "\u001b[1m2185/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7622 - loss: 0.4684\n",
            "\u001b[1m2209/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7622 - loss: 0.4683\n",
            "\u001b[1m2259/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7623 - loss: 0.4682\n",
            "\u001b[1m2311/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7624 - loss: 0.4681\n",
            "\u001b[1m2355/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7624 - loss: 0.4681\n",
            "\u001b[1m2396/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7624 - loss: 0.4680\n",
            "\u001b[1m2439/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7625 - loss: 0.4680\n",
            "\u001b[1m2491/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7625 - loss: 0.4679\n",
            "\u001b[1m2528/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7626 - loss: 0.4678\n",
            "\u001b[1m2576/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7626 - loss: 0.4677\n",
            "\u001b[1m2619/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7626 - loss: 0.4677\n",
            "\u001b[1m2657/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7627 - loss: 0.4676\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 30ms/step - accuracy: 0.6875 - loss: 0.5093\n",
            "\u001b[1m  47/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7879 - loss: 0.4332\n",
            "\u001b[1m  97/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7845 - loss: 0.4356\n",
            "\u001b[1m 134/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7828 - loss: 0.4368\n",
            "\u001b[1m 173/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7813 - loss: 0.4386\n",
            "\u001b[1m 217/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7800 - loss: 0.4404\n",
            "\u001b[1m 271/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7786 - loss: 0.4423\n",
            "\u001b[1m 316/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7774 - loss: 0.4436\n",
            "\u001b[1m 369/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7760 - loss: 0.4453\n",
            "\u001b[1m 409/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7752 - loss: 0.4463\n",
            "\u001b[1m 482/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7740 - loss: 0.4481\n",
            "\u001b[1m 526/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7733 - loss: 0.4490\n",
            "\u001b[1m 577/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7728 - loss: 0.4497\n",
            "\u001b[1m 613/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7724 - loss: 0.4501\n",
            "\u001b[1m 660/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 0.4505\n",
            "\u001b[1m 703/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7718 - loss: 0.4509\n",
            "\u001b[1m 754/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4512\n",
            "\u001b[1m 797/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7714 - loss: 0.4515\n",
            "\u001b[1m 847/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7711 - loss: 0.4518\n",
            "\u001b[1m 890/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4521\n",
            "\u001b[1m 943/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7707 - loss: 0.4524\n",
            "\u001b[1m 985/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7705 - loss: 0.4527\n",
            "\u001b[1m1038/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7703 - loss: 0.4529\n",
            "\u001b[1m1077/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7702 - loss: 0.4531\n",
            "\u001b[1m1110/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7701 - loss: 0.4532\n",
            "\u001b[1m1140/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7700 - loss: 0.4533\n",
            "\u001b[1m1165/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7700 - loss: 0.4534\n",
            "\u001b[1m1199/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7699 - loss: 0.4535\n",
            "\u001b[1m1225/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7699 - loss: 0.4536\n",
            "\u001b[1m1257/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7698 - loss: 0.4537\n",
            "\u001b[1m1289/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7698 - loss: 0.4537\n",
            "\u001b[1m1324/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7697 - loss: 0.4538\n",
            "\u001b[1m1349/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7697 - loss: 0.4539\n",
            "\u001b[1m1375/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7697 - loss: 0.4539\n",
            "\u001b[1m1392/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7697 - loss: 0.4539\n",
            "\u001b[1m1419/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4540\n",
            "\u001b[1m1442/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4540\n",
            "\u001b[1m1469/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4541\n",
            "\u001b[1m1499/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4541\n",
            "\u001b[1m1530/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4541\n",
            "\u001b[1m1553/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4542\n",
            "\u001b[1m1581/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4542\n",
            "\u001b[1m1607/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4542\n",
            "\u001b[1m1641/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4542\n",
            "\u001b[1m1671/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4542\n",
            "\u001b[1m1700/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4543\n",
            "\u001b[1m1723/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4543\n",
            "\u001b[1m1755/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4543\n",
            "\u001b[1m1782/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7696 - loss: 0.4543\n",
            "\u001b[1m1819/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7695 - loss: 0.4544\n",
            "\u001b[1m1841/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7695 - loss: 0.4544\n",
            "\u001b[1m1876/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7695 - loss: 0.4544\n",
            "\u001b[1m1895/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7695 - loss: 0.4545\n",
            "\u001b[1m1919/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7695 - loss: 0.4545\n",
            "\u001b[1m1948/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7695 - loss: 0.4545\n",
            "\u001b[1m1975/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7695 - loss: 0.4545\n",
            "\u001b[1m2000/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7695 - loss: 0.4545\n",
            "\u001b[1m2031/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7695 - loss: 0.4545\n",
            "\u001b[1m2062/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4546\n",
            "\u001b[1m2095/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4546\n",
            "\u001b[1m2119/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4546\n",
            "\u001b[1m2147/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4546\n",
            "\u001b[1m2171/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4546\n",
            "\u001b[1m2208/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4546\n",
            "\u001b[1m2239/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4547\n",
            "\u001b[1m2266/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4547\n",
            "\u001b[1m2290/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4547\n",
            "\u001b[1m2322/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4547\n",
            "\u001b[1m2347/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4547\n",
            "\u001b[1m2381/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4548\n",
            "\u001b[1m2404/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7694 - loss: 0.4548\n",
            "\u001b[1m2429/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7693 - loss: 0.4548\n",
            "\u001b[1m2455/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7693 - loss: 0.4548\n",
            "\u001b[1m2489/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7693 - loss: 0.4548\n",
            "\u001b[1m2516/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7693 - loss: 0.4549\n",
            "\u001b[1m2542/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7693 - loss: 0.4549\n",
            "\u001b[1m2567/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7693 - loss: 0.4549\n",
            "\u001b[1m2597/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7693 - loss: 0.4549\n",
            "\u001b[1m2631/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7693 - loss: 0.4550\n",
            "\u001b[1m2657/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7693 - loss: 0.4550\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 28ms/step - accuracy: 0.8438 - loss: 0.3879\n",
            "\u001b[1m  45/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7920 - loss: 0.4366\n",
            "\u001b[1m  95/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7843 - loss: 0.4407\n",
            "\u001b[1m 138/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7814 - loss: 0.4423\n",
            "\u001b[1m 188/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7799 - loss: 0.4430\n",
            "\u001b[1m 234/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7787 - loss: 0.4441\n",
            "\u001b[1m 277/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7779 - loss: 0.4451\n",
            "\u001b[1m 315/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7775 - loss: 0.4454\n",
            "\u001b[1m 364/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7772 - loss: 0.4455\n",
            "\u001b[1m 442/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7765 - loss: 0.4455\n",
            "\u001b[1m 480/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7762 - loss: 0.4456\n",
            "\u001b[1m 531/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7758 - loss: 0.4458\n",
            "\u001b[1m 575/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7755 - loss: 0.4460\n",
            "\u001b[1m 627/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7751 - loss: 0.4463\n",
            "\u001b[1m 668/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7749 - loss: 0.4464\n",
            "\u001b[1m 719/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7746 - loss: 0.4466\n",
            "\u001b[1m 752/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7744 - loss: 0.4467\n",
            "\u001b[1m 803/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7742 - loss: 0.4469\n",
            "\u001b[1m 846/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7740 - loss: 0.4472\n",
            "\u001b[1m 898/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7738 - loss: 0.4474\n",
            "\u001b[1m 946/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7736 - loss: 0.4476\n",
            "\u001b[1m 986/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7734 - loss: 0.4478\n",
            "\u001b[1m1039/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7732 - loss: 0.4481\n",
            "\u001b[1m1082/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7730 - loss: 0.4483\n",
            "\u001b[1m1137/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7727 - loss: 0.4486\n",
            "\u001b[1m1181/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7726 - loss: 0.4488\n",
            "\u001b[1m1228/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7724 - loss: 0.4491\n",
            "\u001b[1m1250/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7723 - loss: 0.4493\n",
            "\u001b[1m1297/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 0.4495\n",
            "\u001b[1m1351/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7719 - loss: 0.4498\n",
            "\u001b[1m1399/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7717 - loss: 0.4500\n",
            "\u001b[1m1450/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4503\n",
            "\u001b[1m1496/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7714 - loss: 0.4505\n",
            "\u001b[1m1551/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7713 - loss: 0.4507\n",
            "\u001b[1m1589/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7712 - loss: 0.4508\n",
            "\u001b[1m1638/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7711 - loss: 0.4510\n",
            "\u001b[1m1678/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7710 - loss: 0.4511\n",
            "\u001b[1m1724/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4513\n",
            "\u001b[1m1771/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7708 - loss: 0.4514\n",
            "\u001b[1m1825/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7707 - loss: 0.4516\n",
            "\u001b[1m1870/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7706 - loss: 0.4517\n",
            "\u001b[1m1923/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7706 - loss: 0.4518\n",
            "\u001b[1m1965/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7705 - loss: 0.4519\n",
            "\u001b[1m2042/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7704 - loss: 0.4520\n",
            "\u001b[1m2088/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7703 - loss: 0.4521\n",
            "\u001b[1m2141/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7703 - loss: 0.4522\n",
            "\u001b[1m2182/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7703 - loss: 0.4522\n",
            "\u001b[1m2224/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7702 - loss: 0.4523\n",
            "\u001b[1m2266/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7702 - loss: 0.4523\n",
            "\u001b[1m2318/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7701 - loss: 0.4524\n",
            "\u001b[1m2360/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7701 - loss: 0.4524\n",
            "\u001b[1m2406/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7701 - loss: 0.4524\n",
            "\u001b[1m2449/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7701 - loss: 0.4525\n",
            "\u001b[1m2494/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7700 - loss: 0.4525\n",
            "\u001b[1m2538/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7700 - loss: 0.4525\n",
            "\u001b[1m2595/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7700 - loss: 0.4526\n",
            "\u001b[1m2630/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7700 - loss: 0.4526\n",
            "\u001b[1m2657/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7700 - loss: 0.4526\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 5/5\n",
            "\u001b[1m   1/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:11\u001b[0m 27ms/step - accuracy: 0.8125 - loss: 0.4453\n",
            "\u001b[1m  44/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7723 - loss: 0.4397\n",
            "\u001b[1m  95/2657\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7687 - loss: 0.4486\n",
            "\u001b[1m 136/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7693 - loss: 0.4500\n",
            "\u001b[1m 189/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7702 - loss: 0.4511\n",
            "\u001b[1m 231/2657\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7706 - loss: 0.4511\n",
            "\u001b[1m 280/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7714 - loss: 0.4503\n",
            "\u001b[1m 324/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7719 - loss: 0.4499\n",
            "\u001b[1m 379/2657\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7723 - loss: 0.4493\n",
            "\u001b[1m 423/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7724 - loss: 0.4489\n",
            "\u001b[1m 463/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7724 - loss: 0.4487\n",
            "\u001b[1m 506/2657\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7722 - loss: 0.4486\n",
            "\u001b[1m 577/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7720 - loss: 0.4486\n",
            "\u001b[1m 620/2657\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7719 - loss: 0.4486\n",
            "\u001b[1m 671/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7719 - loss: 0.4485\n",
            "\u001b[1m 713/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7717 - loss: 0.4485\n",
            "\u001b[1m 768/2657\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4485\n",
            "\u001b[1m 808/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4484\n",
            "\u001b[1m 864/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4484\n",
            "\u001b[1m 904/2657\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4483\n",
            "\u001b[1m 943/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4483\n",
            "\u001b[1m 985/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4482\n",
            "\u001b[1m1035/2657\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4482\n",
            "\u001b[1m1088/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4483\n",
            "\u001b[1m1137/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4485\n",
            "\u001b[1m1192/2657\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7717 - loss: 0.4486\n",
            "\u001b[1m1236/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7717 - loss: 0.4487\n",
            "\u001b[1m1286/2657\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4488\n",
            "\u001b[1m1332/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7716 - loss: 0.4490\n",
            "\u001b[1m1387/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7715 - loss: 0.4492\n",
            "\u001b[1m1425/2657\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7714 - loss: 0.4493\n",
            "\u001b[1m1477/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7714 - loss: 0.4495\n",
            "\u001b[1m1520/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7713 - loss: 0.4496\n",
            "\u001b[1m1574/2657\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7712 - loss: 0.4499\n",
            "\u001b[1m1620/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7711 - loss: 0.4501\n",
            "\u001b[1m1671/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7710 - loss: 0.4503\n",
            "\u001b[1m1695/2657\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7710 - loss: 0.4504\n",
            "\u001b[1m1744/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4506\n",
            "\u001b[1m1801/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7708 - loss: 0.4508\n",
            "\u001b[1m1825/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7708 - loss: 0.4509\n",
            "\u001b[1m1858/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7708 - loss: 0.4510\n",
            "\u001b[1m1884/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7708 - loss: 0.4511\n",
            "\u001b[1m1919/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7707 - loss: 0.4512\n",
            "\u001b[1m1932/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7707 - loss: 0.4513\n",
            "\u001b[1m1965/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7707 - loss: 0.4514\n",
            "\u001b[1m1980/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7707 - loss: 0.4514\n",
            "\u001b[1m2014/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7706 - loss: 0.4515\n",
            "\u001b[1m2028/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7706 - loss: 0.4516\n",
            "\u001b[1m2053/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7706 - loss: 0.4516\n",
            "\u001b[1m2085/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7705 - loss: 0.4517\n",
            "\u001b[1m2109/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7705 - loss: 0.4518\n",
            "\u001b[1m2144/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7705 - loss: 0.4519\n",
            "\u001b[1m2173/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7705 - loss: 0.4520\n",
            "\u001b[1m2202/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7704 - loss: 0.4520\n",
            "\u001b[1m2226/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7704 - loss: 0.4521\n",
            "\u001b[1m2259/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7704 - loss: 0.4522\n",
            "\u001b[1m2284/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7703 - loss: 0.4522\n",
            "\u001b[1m2321/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7703 - loss: 0.4523\n",
            "\u001b[1m2346/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7703 - loss: 0.4524\n",
            "\u001b[1m2377/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7703 - loss: 0.4525\n",
            "\u001b[1m2397/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7702 - loss: 0.4525\n",
            "\u001b[1m2434/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7702 - loss: 0.4526\n",
            "\u001b[1m2457/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7702 - loss: 0.4526\n",
            "\u001b[1m2481/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7702 - loss: 0.4527\n",
            "\u001b[1m2510/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7702 - loss: 0.4527\n",
            "\u001b[1m2541/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7701 - loss: 0.4528\n",
            "\u001b[1m2564/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7701 - loss: 0.4528\n",
            "\u001b[1m2595/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7701 - loss: 0.4529\n",
            "\u001b[1m2624/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7701 - loss: 0.4529\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 32 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2640/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7701 - loss: 0.4529\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2656/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7701 - loss: 0.4530\n",
            "Server Evaluating... Evaluation Count: 4\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2657/2657\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7701 - loss: 0.4530\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client  32 Training complete...\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m65s\u001b[0m 3ms/step - accuracy: 0.7636 - loss: 0.4703\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 2ms/step\n",
            "Prediction:  [[1.9720259e-09 9.5085477e-07 5.7921670e-07 ... 6.0668332e-13\n",
            "  4.3888826e-15 2.2962428e-15]\n",
            " [1.0451125e-11 7.5656828e-11 9.9967182e-01 ... 2.2406621e-35\n",
            "  1.4922122e-32 2.7493227e-18]\n",
            " [3.5315331e-08 2.2700619e-06 1.3926743e-07 ... 2.5839187e-18\n",
            "  7.0608559e-28 3.2439408e-18]\n",
            " ...\n",
            " [4.5898009e-06 2.0306469e-10 2.1822691e-12 ... 3.2293316e-17\n",
            "  5.2211256e-18 5.5558750e-15]\n",
            " [1.9740412e-07 5.5196967e-09 1.4995787e-07 ... 9.0557781e-18\n",
            "  2.0538665e-22 4.3008505e-17]\n",
            " [1.0096664e-06 3.1746697e-07 1.2242455e-05 ... 3.4312573e-17\n",
            "  4.7794942e-20 1.0063670e-14]] (744790, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (4, 0.4712642431259155, {'accuracy': 0.7643886208534241}, 1630.811099206)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 16 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mServer evaluation complete - Accuracy: 0.7644, Loss: 0.4713\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client ID: 16\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client  16 Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \r\u001b[1m   1/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:29:17\u001b[0m 2s/step - accuracy: 0.7500 - loss: 0.5158\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  10/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 6ms/step - accuracy: 0.7408 - loss: 0.4926   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  15/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m21s\u001b[0m 8ms/step - accuracy: 0.7490 - loss: 0.4790\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  18/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 0.7510 - loss: 0.4765\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  23/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 0.7529 - loss: 0.4742\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  28/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 0.7541 - loss: 0.4718\n",
            "\u001b[1m  34/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 10ms/step - accuracy: 0.7553 - loss: 0.4685\n",
            "\u001b[1m  36/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m33s\u001b[0m 13ms/step - accuracy: 0.7555 - loss: 0.4675\n",
            "\u001b[1m  41/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m35s\u001b[0m 14ms/step - accuracy: 0.7560 - loss: 0.4661\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  53/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 12ms/step - accuracy: 0.7566 - loss: 0.4636\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  57/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 12ms/step - accuracy: 0.7566 - loss: 0.4628\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  63/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 12ms/step - accuracy: 0.7570 - loss: 0.4615\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  68/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 12ms/step - accuracy: 0.7573 - loss: 0.4607\n",
            "\u001b[1m  72/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 12ms/step - accuracy: 0.7575 - loss: 0.4600\n",
            "\u001b[1m  78/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 17ms/step - accuracy: 0.7578 - loss: 0.4589\n",
            "\u001b[1m 114/2602\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 12ms/step - accuracy: 0.7576 - loss: 0.4578\n",
            "\u001b[1m 139/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 11ms/step - accuracy: 0.7583 - loss: 0.4577\n",
            "\u001b[1m 144/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 11ms/step - accuracy: 0.7585 - loss: 0.4576\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 146/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 12ms/step - accuracy: 0.7585 - loss: 0.4576\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 150/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 12ms/step - accuracy: 0.7586 - loss: 0.4576\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 152/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m29s\u001b[0m 12ms/step - accuracy: 0.7586 - loss: 0.4577\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 154/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 12ms/step - accuracy: 0.7587 - loss: 0.4577\n",
            "\u001b[1m 158/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m31s\u001b[0m 13ms/step - accuracy: 0.7588 - loss: 0.4576\n",
            "\u001b[1m 164/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 13ms/step - accuracy: 0.7590 - loss: 0.4577\n",
            "\u001b[1m 184/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 13ms/step - accuracy: 0.7593 - loss: 0.4584\n",
            "\u001b[1m 201/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m28s\u001b[0m 12ms/step - accuracy: 0.7595 - loss: 0.4588\n",
            "\u001b[1m 218/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 12ms/step - accuracy: 0.7598 - loss: 0.4590\n",
            "\u001b[1m 246/2602\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 11ms/step - accuracy: 0.7604 - loss: 0.4594\n",
            "\u001b[1m 273/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.7610 - loss: 0.4593\n",
            "\u001b[1m 311/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.7617 - loss: 0.4591 \n",
            "\u001b[1m 351/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 8ms/step - accuracy: 0.7623 - loss: 0.4590\n",
            "\u001b[1m 387/2602\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7628 - loss: 0.4591\n",
            "\u001b[1m 414/2602\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7630 - loss: 0.4594\n",
            "\u001b[1m 446/2602\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 7ms/step - accuracy: 0.7632 - loss: 0.4598\n",
            "\u001b[1m 486/2602\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7635 - loss: 0.4600\n",
            "\u001b[1m 526/2602\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7637 - loss: 0.4601\n",
            "\u001b[1m 563/2602\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 6ms/step - accuracy: 0.7637 - loss: 0.4604\n",
            "\u001b[1m 608/2602\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7636 - loss: 0.4608\n",
            "\u001b[1m 653/2602\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 6ms/step - accuracy: 0.7636 - loss: 0.4612\n",
            "\u001b[1m 701/2602\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7635 - loss: 0.4617\n",
            "\u001b[1m 735/2602\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 5ms/step - accuracy: 0.7635 - loss: 0.4620\n",
            "\u001b[1m 769/2602\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7634 - loss: 0.4624\n",
            "\u001b[1m 804/2602\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7633 - loss: 0.4628\n",
            "\u001b[1m 826/2602\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 5ms/step - accuracy: 0.7632 - loss: 0.4631\n",
            "\u001b[1m 856/2602\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7632 - loss: 0.4634\n",
            "\u001b[1m 901/2602\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7631 - loss: 0.4638\n",
            "\u001b[1m 929/2602\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 5ms/step - accuracy: 0.7630 - loss: 0.4641\n",
            "\u001b[1m 972/2602\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7629 - loss: 0.4645\n",
            "\u001b[1m 996/2602\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7629 - loss: 0.4647\n",
            "\u001b[1m1041/2602\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7628 - loss: 0.4651\n",
            "\u001b[1m1064/2602\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 5ms/step - accuracy: 0.7627 - loss: 0.4654\n",
            "\u001b[1m1103/2602\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7626 - loss: 0.4657\n",
            "\u001b[1m1132/2602\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7626 - loss: 0.4660\n",
            "\u001b[1m1165/2602\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7625 - loss: 0.4662\n",
            "\u001b[1m1199/2602\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7624 - loss: 0.4665\n",
            "\u001b[1m1242/2602\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.7624 - loss: 0.4668\n",
            "\u001b[1m1273/2602\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4670\n",
            "\u001b[1m1312/2602\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4672\n",
            "\u001b[1m1349/2602\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4674\n",
            "\u001b[1m1386/2602\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4676\n",
            "\u001b[1m1415/2602\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4677\n",
            "\u001b[1m1452/2602\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4679\n",
            "\u001b[1m1475/2602\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4680\n",
            "\u001b[1m1503/2602\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4681\n",
            "\u001b[1m1529/2602\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4682\n",
            "\u001b[1m1565/2602\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4683\n",
            "\u001b[1m1592/2602\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4684\n",
            "\u001b[1m1628/2602\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4685\n",
            "\u001b[1m1660/2602\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4686\n",
            "\u001b[1m1694/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7623 - loss: 0.4688\n",
            "\u001b[1m1726/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7622 - loss: 0.4689\n",
            "\u001b[1m1766/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7622 - loss: 0.4690\n",
            "\u001b[1m1798/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7622 - loss: 0.4691\n",
            "\u001b[1m1835/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 4ms/step - accuracy: 0.7622 - loss: 0.4692\n",
            "\u001b[1m1861/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7622 - loss: 0.4693\n",
            "\u001b[1m1893/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7622 - loss: 0.4706\n",
            "\u001b[1m1940/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7622 - loss: 0.4747\n",
            "\u001b[1m2004/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7621 - loss: 0.4798\n",
            "\u001b[1m2060/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7621 - loss: 0.4839\n",
            "\u001b[1m2127/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7621 - loss: 0.4884\n",
            "\u001b[1m2180/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7621 - loss: 0.4916\n",
            "\u001b[1m2214/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7620 - loss: 0.4936\n",
            "\u001b[1m2243/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7620 - loss: 0.4952\n",
            "\u001b[1m2318/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7620 - loss: 0.4990\n",
            "\u001b[1m2382/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7619 - loss: 0.5020\n",
            "\u001b[1m2422/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7619 - loss: 0.5038\n",
            "\u001b[1m2482/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7619 - loss: 0.5062\n",
            "\u001b[1m2525/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7618 - loss: 0.5078\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 15 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 33 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2555/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7618 - loss: 0.5089\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2591/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7618 - loss: 0.5101\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2602/2602\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - accuracy: 0.7618 - loss: 0.5105\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[33mClient 16 evaluation complete - Accuracy: 0.760271, Loss: 0.596731\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client ID: 26\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client  26 Training...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 1/5\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \r\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4:31:20\u001b[0m 6s/step - accuracy: 0.8438 - loss: 0.4067\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m   8/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 9ms/step - accuracy: 0.7890 - loss: 0.4068   \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  13/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m26s\u001b[0m 10ms/step - accuracy: 0.7912 - loss: 0.4156\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  16/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 12ms/step - accuracy: 0.7921 - loss: 0.4169\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  21/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 12ms/step - accuracy: 0.7913 - loss: 0.4215\n",
            "\u001b[1m  29/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 12ms/step - accuracy: 0.7859 - loss: 0.4310\n",
            "\u001b[1m  31/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 14ms/step - accuracy: 0.7846 - loss: 0.4327\n",
            "\u001b[1m  33/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m46s\u001b[0m 18ms/step - accuracy: 0.7835 - loss: 0.4343\n",
            "\u001b[1m  44/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m42s\u001b[0m 16ms/step - accuracy: 0.7780 - loss: 0.4427\n",
            "\u001b[1m  45/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 17ms/step - accuracy: 0.7776 - loss: 0.4432\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  46/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 19ms/step - accuracy: 0.7772 - loss: 0.4437\n",
            "\u001b[1m  48/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 22ms/step - accuracy: 0.7767 - loss: 0.4447\n",
            "\u001b[1m  53/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m54s\u001b[0m 21ms/step - accuracy: 0.7755 - loss: 0.4465\n",
            "\u001b[1m  63/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 19ms/step - accuracy: 0.7742 - loss: 0.4486\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  67/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 19ms/step - accuracy: 0.7739 - loss: 0.4491\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  71/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m48s\u001b[0m 19ms/step - accuracy: 0.7738 - loss: 0.4494\n",
            "\u001b[1m  74/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 19ms/step - accuracy: 0.7737 - loss: 0.4497\n",
            "\u001b[1m  77/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m50s\u001b[0m 19ms/step - accuracy: 0.7734 - loss: 0.4501\n",
            "\u001b[1m  78/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m51s\u001b[0m 20ms/step - accuracy: 0.7734 - loss: 0.4502\n",
            "\u001b[1m  79/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m52s\u001b[0m 21ms/step - accuracy: 0.7733 - loss: 0.4503\n",
            "\u001b[1m  80/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 21ms/step - accuracy: 0.7732 - loss: 0.4505\n",
            "\u001b[1m  86/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m56s\u001b[0m 22ms/step - accuracy: 0.7726 - loss: 0.4517\n",
            "\u001b[1m  89/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 22ms/step - accuracy: 0.7722 - loss: 0.4525\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  93/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m57s\u001b[0m 22ms/step - accuracy: 0.7718 - loss: 0.4533\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m  94/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m59s\u001b[0m 23ms/step - accuracy: 0.7717 - loss: 0.4535\n",
            "\u001b[1m  97/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 24ms/step - accuracy: 0.7715 - loss: 0.4540\n",
            "\u001b[1m 101/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 24ms/step - accuracy: 0.7712 - loss: 0.4545\n",
            "\u001b[1m 105/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 24ms/step - accuracy: 0.7710 - loss: 0.4551\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 106/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:01\u001b[0m 24ms/step - accuracy: 0.7709 - loss: 0.4552\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m 110/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:00\u001b[0m 24ms/step - accuracy: 0.7706 - loss: 0.4558\n",
            "\u001b[1m 111/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:02\u001b[0m 25ms/step - accuracy: 0.7705 - loss: 0.4559\n",
            "\u001b[1m 129/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m55s\u001b[0m 22ms/step - accuracy: 0.7699 - loss: 0.4578\n",
            "\u001b[1m 148/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m49s\u001b[0m 20ms/step - accuracy: 0.7695 - loss: 0.4593\n",
            "\u001b[1m 166/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m45s\u001b[0m 18ms/step - accuracy: 0.7692 - loss: 0.4600\n",
            "\u001b[1m 187/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m41s\u001b[0m 17ms/step - accuracy: 0.7686 - loss: 0.4609\n",
            "\u001b[1m 202/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m39s\u001b[0m 16ms/step - accuracy: 0.7682 - loss: 0.4613\n",
            "\u001b[1m 214/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m38s\u001b[0m 16ms/step - accuracy: 0.7681 - loss: 0.4613\n",
            "\u001b[1m 231/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m36s\u001b[0m 15ms/step - accuracy: 0.7680 - loss: 0.4614\n",
            "\u001b[1m 261/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m32s\u001b[0m 14ms/step - accuracy: 0.7682 - loss: 0.4618\n",
            "\u001b[1m 289/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m30s\u001b[0m 13ms/step - accuracy: 0.7684 - loss: 0.4618\n",
            "\u001b[1m 319/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m27s\u001b[0m 12ms/step - accuracy: 0.7685 - loss: 0.4619\n",
            "\u001b[1m 348/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m25s\u001b[0m 11ms/step - accuracy: 0.7685 - loss: 0.4620\n",
            "\u001b[1m 368/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m24s\u001b[0m 11ms/step - accuracy: 0.7685 - loss: 0.4621\n",
            "\u001b[1m 396/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m23s\u001b[0m 10ms/step - accuracy: 0.7685 - loss: 0.4637\n",
            "\u001b[1m 424/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m22s\u001b[0m 10ms/step - accuracy: 0.7684 - loss: 0.4669\n",
            "\u001b[1m 454/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.7683 - loss: 0.4696 \n",
            "\u001b[1m 474/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m20s\u001b[0m 9ms/step - accuracy: 0.7682 - loss: 0.4711\n",
            "\u001b[1m 505/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m19s\u001b[0m 9ms/step - accuracy: 0.7681 - loss: 0.4730\n",
            "\u001b[1m 530/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 9ms/step - accuracy: 0.7680 - loss: 0.4742\n",
            "\u001b[1m 556/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7679 - loss: 0.4753\n",
            "\u001b[1m 577/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m17s\u001b[0m 8ms/step - accuracy: 0.7678 - loss: 0.4761\n",
            "\u001b[1m 610/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m16s\u001b[0m 8ms/step - accuracy: 0.7676 - loss: 0.4773\n",
            "\u001b[1m 640/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7674 - loss: 0.4782\n",
            "\u001b[1m 671/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m15s\u001b[0m 8ms/step - accuracy: 0.7673 - loss: 0.4791\n",
            "\u001b[1m 697/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 8ms/step - accuracy: 0.7671 - loss: 0.4798\n",
            "\u001b[1m 727/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 7ms/step - accuracy: 0.7669 - loss: 0.4804\n",
            "\u001b[1m 752/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7668 - loss: 0.4809\n",
            "\u001b[1m 781/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m13s\u001b[0m 7ms/step - accuracy: 0.7667 - loss: 0.4814\n",
            "\u001b[1m 813/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7666 - loss: 0.4819\n",
            "\u001b[1m 839/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7665 - loss: 0.4822\n",
            "\u001b[1m 868/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m12s\u001b[0m 7ms/step - accuracy: 0.7665 - loss: 0.4825\n",
            "\u001b[1m 902/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7664 - loss: 0.4829\n",
            "\u001b[1m 927/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 7ms/step - accuracy: 0.7663 - loss: 0.4831\n",
            "\u001b[1m 957/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7662 - loss: 0.4833\n",
            "\u001b[1m 987/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7662 - loss: 0.4834\n",
            "\u001b[1m1017/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7661 - loss: 0.4836\n",
            "\u001b[1m1044/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m10s\u001b[0m 6ms/step - accuracy: 0.7660 - loss: 0.4838\n",
            "\u001b[1m1075/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7660 - loss: 0.4839\n",
            "\u001b[1m1093/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7659 - loss: 0.4839\n",
            "\u001b[1m1121/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7659 - loss: 0.4840\n",
            "\u001b[1m1146/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 6ms/step - accuracy: 0.7659 - loss: 0.4840\n",
            "\u001b[1m1185/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7658 - loss: 0.4840\n",
            "\u001b[1m1204/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7658 - loss: 0.4840\n",
            "\u001b[1m1232/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7658 - loss: 0.4839\n",
            "\u001b[1m1247/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7658 - loss: 0.4839\n",
            "\u001b[1m1272/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 6ms/step - accuracy: 0.7657 - loss: 0.4839\n",
            "\u001b[1m1290/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.7657 - loss: 0.4838\n",
            "\u001b[1m1319/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.7657 - loss: 0.4838\n",
            "\u001b[1m1345/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.7657 - loss: 0.4838\n",
            "\u001b[1m1374/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.7657 - loss: 0.4837\n",
            "\u001b[1m1394/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 6ms/step - accuracy: 0.7657 - loss: 0.4837\n",
            "\u001b[1m1448/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 6ms/step - accuracy: 0.7656 - loss: 0.4836\n",
            "\u001b[1m1490/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4835\n",
            "\u001b[1m1543/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4834\n",
            "\u001b[1m1587/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4833\n",
            "\u001b[1m1633/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4833\n",
            "\u001b[1m1657/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4832\n",
            "\u001b[1m1703/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4831\n",
            "\u001b[1m1761/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4830\n",
            "\u001b[1m1802/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4829\n",
            "\u001b[1m1853/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4828\n",
            "\u001b[1m1886/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4827\n",
            "\u001b[1m1940/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4826\n",
            "\u001b[1m1979/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4825\n",
            "\u001b[1m2031/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4823\n",
            "\u001b[1m2073/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 5ms/step - accuracy: 0.7656 - loss: 0.4822\n",
            "\u001b[1m2124/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.4821\n",
            "\u001b[1m2171/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m2s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.4820\n",
            "\u001b[1m2221/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.4818\n",
            "\u001b[1m2261/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.4817\n",
            "\u001b[1m2312/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.4816\n",
            "\u001b[1m2348/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.4815\n",
            "\u001b[1m2396/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m1s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.4813\n",
            "\u001b[1m2435/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.4812\n",
            "\u001b[1m2486/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.4811\n",
            "\u001b[1m2534/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.4809\n",
            "\u001b[1m2586/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.4808\n",
            "\u001b[1m2627/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7656 - loss: 0.4806\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m17s\u001b[0m 4ms/step - accuracy: 0.7657 - loss: 0.4806\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 2/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:13\u001b[0m 28ms/step - accuracy: 0.7500 - loss: 0.5241\n",
            "\u001b[1m  44/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7606 - loss: 0.4720\n",
            "\u001b[1m  99/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7637 - loss: 0.4599\n",
            "\u001b[1m 136/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7639 - loss: 0.4590\n",
            "\u001b[1m 184/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7645 - loss: 0.4568\n",
            "\u001b[1m 238/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7652 - loss: 0.4543\n",
            "\u001b[1m 276/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7656 - loss: 0.4531\n",
            "\u001b[1m 324/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7658 - loss: 0.4524\n",
            "\u001b[1m 371/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7659 - loss: 0.4526\n",
            "\u001b[1m 427/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7660 - loss: 0.4530\n",
            "\u001b[1m 454/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7660 - loss: 0.4531\n",
            "\u001b[1m 504/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7662 - loss: 0.4531\n",
            "\u001b[1m 560/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7663 - loss: 0.4531\n",
            "\u001b[1m 605/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7665 - loss: 0.4531\n",
            "\u001b[1m 642/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7666 - loss: 0.4531\n",
            "\u001b[1m 681/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7667 - loss: 0.4532\n",
            "\u001b[1m 731/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7668 - loss: 0.4532\n",
            "\u001b[1m 774/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7668 - loss: 0.4534\n",
            "\u001b[1m 827/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7668 - loss: 0.4536\n",
            "\u001b[1m 870/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7668 - loss: 0.4538\n",
            "\u001b[1m 925/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7668 - loss: 0.4539\n",
            "\u001b[1m 970/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7668 - loss: 0.4541\n",
            "\u001b[1m1021/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7669 - loss: 0.4542\n",
            "\u001b[1m1064/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7669 - loss: 0.4543\n",
            "\u001b[1m1106/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7669 - loss: 0.4543\n",
            "\u001b[1m1152/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7670 - loss: 0.4543\n",
            "\u001b[1m1199/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7670 - loss: 0.4543\n",
            "\u001b[1m1240/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4543\n",
            "\u001b[1m1290/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4544\n",
            "\u001b[1m1329/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7672 - loss: 0.4544\n",
            "\u001b[1m1378/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7672 - loss: 0.4544\n",
            "\u001b[1m1421/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4545\n",
            "\u001b[1m1473/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4546\n",
            "\u001b[1m1497/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4547\n",
            "\u001b[1m1539/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4548\n",
            "\u001b[1m1576/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4548\n",
            "\u001b[1m1614/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4549\n",
            "\u001b[1m1664/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4552\n",
            "\u001b[1m1707/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4555\n",
            "\u001b[1m1758/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4558\n",
            "\u001b[1m1796/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4560\n",
            "\u001b[1m1846/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4563\n",
            "\u001b[1m1884/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4565\n",
            "\u001b[1m1930/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4567\n",
            "\u001b[1m1965/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4568\n",
            "\u001b[1m2004/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4570\n",
            "\u001b[1m2039/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4571\n",
            "\u001b[1m2093/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4573\n",
            "\u001b[1m2139/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7671 - loss: 0.4575\n",
            "\u001b[1m2189/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7672 - loss: 0.4577\n",
            "\u001b[1m2215/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7672 - loss: 0.4578\n",
            "\u001b[1m2260/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7672 - loss: 0.4579\n",
            "\u001b[1m2310/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7672 - loss: 0.4581\n",
            "\u001b[1m2346/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7672 - loss: 0.4582\n",
            "\u001b[1m2397/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7672 - loss: 0.4583\n",
            "\u001b[1m2438/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7673 - loss: 0.4584\n",
            "\u001b[1m2469/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7673 - loss: 0.4584\n",
            "\u001b[1m2507/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7673 - loss: 0.4585\n",
            "\u001b[1m2549/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7673 - loss: 0.4586\n",
            "\u001b[1m2586/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7673 - loss: 0.4587\n",
            "\u001b[1m2633/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7674 - loss: 0.4588\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7674 - loss: 0.4588\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 3/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 30ms/step - accuracy: 0.8438 - loss: 0.4227\n",
            "\u001b[1m  55/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7509 - loss: 0.4736\n",
            "\u001b[1m  98/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7526 - loss: 0.4699\n",
            "\u001b[1m 151/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7574 - loss: 0.4662\n",
            "\u001b[1m 196/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7602 - loss: 0.4637\n",
            "\u001b[1m 233/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7613 - loss: 0.4625\n",
            "\u001b[1m 273/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7623 - loss: 0.4610\n",
            "\u001b[1m 328/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7634 - loss: 0.4594\n",
            "\u001b[1m 372/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7641 - loss: 0.4585\n",
            "\u001b[1m 426/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7648 - loss: 0.4575\n",
            "\u001b[1m 466/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7653 - loss: 0.4570\n",
            "\u001b[1m 498/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7657 - loss: 0.4565\n",
            "\u001b[1m 522/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7660 - loss: 0.4560\n",
            "\u001b[1m 555/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7665 - loss: 0.4554\n",
            "\u001b[1m 583/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7667 - loss: 0.4550\n",
            "\u001b[1m 608/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7669 - loss: 0.4547\n",
            "\u001b[1m 641/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7671 - loss: 0.4544\n",
            "\u001b[1m 672/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7673 - loss: 0.4542\n",
            "\u001b[1m 696/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7673 - loss: 0.4541\n",
            "\u001b[1m 723/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7674 - loss: 0.4541\n",
            "\u001b[1m 747/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7675 - loss: 0.4540\n",
            "\u001b[1m 782/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7675 - loss: 0.4540\n",
            "\u001b[1m 805/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7676 - loss: 0.4540\n",
            "\u001b[1m 841/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7676 - loss: 0.4541\n",
            "\u001b[1m 864/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7676 - loss: 0.4541\n",
            "\u001b[1m 889/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7676 - loss: 0.4541\n",
            "\u001b[1m 919/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7676 - loss: 0.4542\n",
            "\u001b[1m 956/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7676 - loss: 0.4542\n",
            "\u001b[1m 982/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7676 - loss: 0.4542\n",
            "\u001b[1m1007/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7677 - loss: 0.4542\n",
            "\u001b[1m1040/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7677 - loss: 0.4543\n",
            "\u001b[1m1059/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7677 - loss: 0.4543\n",
            "\u001b[1m1088/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7677 - loss: 0.4543\n",
            "\u001b[1m1107/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7677 - loss: 0.4543\n",
            "\u001b[1m1133/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7677 - loss: 0.4543\n",
            "\u001b[1m1161/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7678 - loss: 0.4543\n",
            "\u001b[1m1190/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7678 - loss: 0.4543\n",
            "\u001b[1m1212/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7678 - loss: 0.4543\n",
            "\u001b[1m1246/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7678 - loss: 0.4543\n",
            "\u001b[1m1274/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7678 - loss: 0.4543\n",
            "\u001b[1m1307/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7678 - loss: 0.4543\n",
            "\u001b[1m1333/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7679 - loss: 0.4544\n",
            "\u001b[1m1360/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7679 - loss: 0.4544\n",
            "\u001b[1m1383/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7679 - loss: 0.4545\n",
            "\u001b[1m1410/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7679 - loss: 0.4545\n",
            "\u001b[1m1435/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7679 - loss: 0.4546\n",
            "\u001b[1m1462/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7679 - loss: 0.4546\n",
            "\u001b[1m1481/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7680 - loss: 0.4547\n",
            "\u001b[1m1514/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7680 - loss: 0.4547\n",
            "\u001b[1m1542/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7680 - loss: 0.4547\n",
            "\u001b[1m1577/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7681 - loss: 0.4547\n",
            "\u001b[1m1604/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7681 - loss: 0.4548\n",
            "\u001b[1m1630/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7681 - loss: 0.4548\n",
            "\u001b[1m1654/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7682 - loss: 0.4548\n",
            "\u001b[1m1681/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7682 - loss: 0.4548\n",
            "\u001b[1m1713/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7682 - loss: 0.4548\n",
            "\u001b[1m1746/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7682 - loss: 0.4549\n",
            "\u001b[1m1774/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7683 - loss: 0.4549\n",
            "\u001b[1m1802/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7683 - loss: 0.4549\n",
            "\u001b[1m1829/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7683 - loss: 0.4549\n",
            "\u001b[1m1858/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7683 - loss: 0.4549\n",
            "\u001b[1m1880/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7684 - loss: 0.4549\n",
            "\u001b[1m1905/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7684 - loss: 0.4550\n",
            "\u001b[1m1929/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7684 - loss: 0.4550\n",
            "\u001b[1m1959/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7684 - loss: 0.4550\n",
            "\u001b[1m1991/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7684 - loss: 0.4550\n",
            "\u001b[1m2036/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7684 - loss: 0.4551\n",
            "\u001b[1m2064/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7685 - loss: 0.4551\n",
            "\u001b[1m2115/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7685 - loss: 0.4551\n",
            "\u001b[1m2159/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7685 - loss: 0.4551\n",
            "\u001b[1m2212/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7685 - loss: 0.4551\n",
            "\u001b[1m2236/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7685 - loss: 0.4551\n",
            "\u001b[1m2274/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7686 - loss: 0.4551\n",
            "\u001b[1m2322/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7686 - loss: 0.4551\n",
            "\u001b[1m2365/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7686 - loss: 0.4551\n",
            "\u001b[1m2418/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7686 - loss: 0.4551\n",
            "\u001b[1m2456/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7686 - loss: 0.4551\n",
            "\u001b[1m2507/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7687 - loss: 0.4551\n",
            "\u001b[1m2539/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7687 - loss: 0.4551\n",
            "\u001b[1m2589/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7687 - loss: 0.4551\n",
            "\u001b[1m2629/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7687 - loss: 0.4551\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.7687 - loss: 0.4551\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 4/5\n",
            "\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:20\u001b[0m 30ms/step - accuracy: 0.8125 - loss: 0.5061\n",
            "\u001b[1m  39/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7819 - loss: 0.4283\n",
            "\u001b[1m  84/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7852 - loss: 0.4269\n",
            "\u001b[1m 126/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7840 - loss: 0.4291\n",
            "\u001b[1m 174/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7827 - loss: 0.4313\n",
            "\u001b[1m 213/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7822 - loss: 0.4322\n",
            "\u001b[1m 260/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7813 - loss: 0.4341\n",
            "\u001b[1m 292/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7806 - loss: 0.4353\n",
            "\u001b[1m 333/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7798 - loss: 0.4366\n",
            "\u001b[1m 371/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 3ms/step - accuracy: 0.7791 - loss: 0.4378\n",
            "\u001b[1m 422/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7783 - loss: 0.4391\n",
            "\u001b[1m 459/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7778 - loss: 0.4398\n",
            "\u001b[1m 503/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7772 - loss: 0.4407\n",
            "\u001b[1m 539/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7768 - loss: 0.4413\n",
            "\u001b[1m 583/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7764 - loss: 0.4419\n",
            "\u001b[1m 622/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7761 - loss: 0.4423\n",
            "\u001b[1m 665/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7757 - loss: 0.4426\n",
            "\u001b[1m 693/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7755 - loss: 0.4428\n",
            "\u001b[1m 735/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7752 - loss: 0.4432\n",
            "\u001b[1m 768/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7749 - loss: 0.4438\n",
            "\u001b[1m 805/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7746 - loss: 0.4443\n",
            "\u001b[1m 836/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7744 - loss: 0.4448\n",
            "\u001b[1m 900/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7739 - loss: 0.4457\n",
            "\u001b[1m 944/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7735 - loss: 0.4462\n",
            "\u001b[1m 997/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7732 - loss: 0.4468\n",
            "\u001b[1m1043/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7730 - loss: 0.4472\n",
            "\u001b[1m1092/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7727 - loss: 0.4476\n",
            "\u001b[1m1119/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7726 - loss: 0.4478\n",
            "\u001b[1m1166/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7724 - loss: 0.4481\n",
            "\u001b[1m1195/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7724 - loss: 0.4482\n",
            "\u001b[1m1239/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7722 - loss: 0.4484\n",
            "\u001b[1m1280/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7721 - loss: 0.4486\n",
            "\u001b[1m1328/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7720 - loss: 0.4487\n",
            "\u001b[1m1365/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7719 - loss: 0.4489\n",
            "\u001b[1m1421/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7718 - loss: 0.4491\n",
            "\u001b[1m1463/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7717 - loss: 0.4492\n",
            "\u001b[1m1518/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7715 - loss: 0.4494\n",
            "\u001b[1m1549/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7715 - loss: 0.4494\n",
            "\u001b[1m1601/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7714 - loss: 0.4496\n",
            "\u001b[1m1641/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7713 - loss: 0.4497\n",
            "\u001b[1m1690/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7713 - loss: 0.4498\n",
            "\u001b[1m1718/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7712 - loss: 0.4498\n",
            "\u001b[1m1760/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7712 - loss: 0.4499\n",
            "\u001b[1m1813/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7711 - loss: 0.4500\n",
            "\u001b[1m1853/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7711 - loss: 0.4500\n",
            "\u001b[1m1900/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7711 - loss: 0.4501\n",
            "\u001b[1m1941/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7710 - loss: 0.4502\n",
            "\u001b[1m1986/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7710 - loss: 0.4502\n",
            "\u001b[1m2013/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7710 - loss: 0.4503\n",
            "\u001b[1m2065/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7710 - loss: 0.4503\n",
            "\u001b[1m2105/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4503\n",
            "\u001b[1m2155/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4503\n",
            "\u001b[1m2195/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4504\n",
            "\u001b[1m2245/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4504\n",
            "\u001b[1m2266/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4504\n",
            "\u001b[1m2311/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4504\n",
            "\u001b[1m2362/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4504\n",
            "\u001b[1m2403/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4505\n",
            "\u001b[1m2454/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4505\n",
            "\u001b[1m2485/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4505\n",
            "\u001b[1m2530/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4505\n",
            "\u001b[1m2569/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4506\n",
            "\u001b[1m2617/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4506\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 2ms/step - accuracy: 0.7709 - loss: 0.4506\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Epoch 5/5\n",
            "\u001b[1m  28/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7735 - loss: 0.4341   \n",
            "\u001b[1m  67/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7717 - loss: 0.4436\n",
            "\u001b[1m 120/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7708 - loss: 0.4464\n",
            "\u001b[1m 157/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7717 - loss: 0.4448\n",
            "\u001b[1m 209/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7730 - loss: 0.4420\n",
            "\u001b[1m 236/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7735 - loss: 0.4408\n",
            "\u001b[1m 264/2658\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7739 - loss: 0.4402\n",
            "\u001b[1m 315/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7748 - loss: 0.4392\n",
            "\u001b[1m 359/2658\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7752 - loss: 0.4388\n",
            "\u001b[1m 412/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7756 - loss: 0.4386\n",
            "\u001b[1m 452/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7756 - loss: 0.4384\n",
            "\u001b[1m 506/2658\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7756 - loss: 0.4382\n",
            "\u001b[1m 552/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7755 - loss: 0.4382\n",
            "\u001b[1m 604/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7753 - loss: 0.4383\n",
            "\u001b[1m 648/2658\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7751 - loss: 0.4385\n",
            "\u001b[1m 702/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7748 - loss: 0.4388\n",
            "\u001b[1m 731/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7746 - loss: 0.4390\n",
            "\u001b[1m 779/2658\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7744 - loss: 0.4392\n",
            "\u001b[1m 822/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7742 - loss: 0.4395\n",
            "\u001b[1m 858/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7741 - loss: 0.4398\n",
            "\u001b[1m 885/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7740 - loss: 0.4400\n",
            "\u001b[1m 903/2658\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7739 - loss: 0.4401\n",
            "\u001b[1m 939/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7737 - loss: 0.4402\n",
            "\u001b[1m 959/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7737 - loss: 0.4404\n",
            "\u001b[1m 990/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7735 - loss: 0.4405\n",
            "\u001b[1m1014/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7735 - loss: 0.4406\n",
            "\u001b[1m1043/2658\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7734 - loss: 0.4408\n",
            "\u001b[1m1065/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7733 - loss: 0.4410\n",
            "\u001b[1m1098/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7733 - loss: 0.4412\n",
            "\u001b[1m1116/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7732 - loss: 0.4413\n",
            "\u001b[1m1147/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7732 - loss: 0.4415\n",
            "\u001b[1m1174/2658\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 3ms/step - accuracy: 0.7731 - loss: 0.4416\n",
            "\u001b[1m1207/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7730 - loss: 0.4419\n",
            "\u001b[1m1235/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7730 - loss: 0.4420\n",
            "\u001b[1m1266/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7729 - loss: 0.4423\n",
            "\u001b[1m1291/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7729 - loss: 0.4424\n",
            "\u001b[1m1319/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7728 - loss: 0.4426\n",
            "\u001b[1m1342/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7727 - loss: 0.4428\n",
            "\u001b[1m1368/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7726 - loss: 0.4430\n",
            "\u001b[1m1391/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7726 - loss: 0.4431\n",
            "\u001b[1m1424/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7725 - loss: 0.4433\n",
            "\u001b[1m1457/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7725 - loss: 0.4434\n",
            "\u001b[1m1494/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7724 - loss: 0.4436\n",
            "\u001b[1m1528/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7723 - loss: 0.4437\n",
            "\u001b[1m1561/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7723 - loss: 0.4439\n",
            "\u001b[1m1592/2658\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7722 - loss: 0.4440\n",
            "\u001b[1m1622/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 3ms/step - accuracy: 0.7722 - loss: 0.4441\n",
            "\u001b[1m1652/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7721 - loss: 0.4442\n",
            "\u001b[1m1674/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7721 - loss: 0.4443\n",
            "\u001b[1m1700/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7721 - loss: 0.4444\n",
            "\u001b[1m1727/2658\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7720 - loss: 0.4445\n",
            "\u001b[1m1746/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7720 - loss: 0.4445\n",
            "\u001b[1m1775/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7720 - loss: 0.4446\n",
            "\u001b[1m1795/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7719 - loss: 0.4447\n",
            "\u001b[1m1821/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7719 - loss: 0.4448\n",
            "\u001b[1m1841/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7719 - loss: 0.4449\n",
            "\u001b[1m1858/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7719 - loss: 0.4449\n",
            "\u001b[1m1877/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7719 - loss: 0.4450\n",
            "\u001b[1m1906/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7718 - loss: 0.4451\n",
            "\u001b[1m1931/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7718 - loss: 0.4452\n",
            "\u001b[1m1965/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7718 - loss: 0.4453\n",
            "\u001b[1m1987/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7718 - loss: 0.4453\n",
            "\u001b[1m2011/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7717 - loss: 0.4454\n",
            "\u001b[1m2031/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m2s\u001b[0m 3ms/step - accuracy: 0.7717 - loss: 0.4455\n",
            "\u001b[1m2056/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7717 - loss: 0.4455\n",
            "\u001b[1m2078/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7717 - loss: 0.4456\n",
            "\u001b[1m2109/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7717 - loss: 0.4456\n",
            "\u001b[1m2130/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7717 - loss: 0.4457\n",
            "\u001b[1m2159/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7717 - loss: 0.4458\n",
            "\u001b[1m2180/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7717 - loss: 0.4458\n",
            "\u001b[1m2208/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7716 - loss: 0.4459\n",
            "\u001b[1m2232/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7716 - loss: 0.4459\n",
            "\u001b[1m2258/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7716 - loss: 0.4460\n",
            "\u001b[1m2280/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7716 - loss: 0.4460\n",
            "\u001b[1m2311/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7716 - loss: 0.4461\n",
            "\u001b[1m2354/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m1s\u001b[0m 3ms/step - accuracy: 0.7716 - loss: 0.4462\n",
            "\u001b[1m2406/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7715 - loss: 0.4463\n",
            "\u001b[1m2446/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7715 - loss: 0.4463\n",
            "\u001b[1m2495/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7715 - loss: 0.4464\n",
            "\u001b[1m2533/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7715 - loss: 0.4465\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 1 results and 32 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2556/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7715 - loss: 0.4465\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2578/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7715 - loss: 0.4465\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2602/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7715 - loss: 0.4466\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2620/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.7715 - loss: 0.4466\n",
            "Server Evaluating... Evaluation Count: 5\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 3ms/step - accuracy: 0.7715 - loss: 0.4467\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client  26 Training complete...\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m69s\u001b[0m 3ms/step - accuracy: 0.7702 - loss: 0.4588\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 2ms/step\n",
            "Prediction:  [[7.57813279e-09 4.62537719e-06 6.89939668e-07 ... 4.60214169e-13\n",
            "  2.77784137e-15 3.29298817e-13]\n",
            " [2.74532411e-11 2.91973723e-10 9.99994099e-01 ... 0.00000000e+00\n",
            "  0.00000000e+00 1.27727435e-14]\n",
            " [2.60533124e-08 9.70325331e-08 1.42973818e-07 ... 1.82934405e-21\n",
            "  2.80145394e-30 2.83311292e-16]\n",
            " ...\n",
            " [2.00136992e-06 2.32744705e-13 5.85333377e-13 ... 6.48100404e-20\n",
            "  3.28991960e-19 3.37160230e-11]\n",
            " [5.02606554e-07 1.02983193e-12 6.61759501e-08 ... 1.28180580e-19\n",
            "  4.94801416e-25 1.61280714e-14]\n",
            " [1.74907746e-06 5.36764855e-10 6.82194295e-05 ... 1.79283767e-18\n",
            "  3.47144817e-22 3.87261881e-12]] (744790, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (5, 0.45878881216049194, {'accuracy': 0.7704305648803711}, 1815.596716141)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 16 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mServer evaluation complete - Accuracy: 0.7704, Loss: 0.4588\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client ID: 11\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m Client  11 Evaluating...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: b75d6ce37bed3ca9d98c188367c422de558a7b86d92f43ac3aaba15b) where the task (actor ID: bb4af0ddb7381d01c4b4a0e501000000, name=ClientAppActor.__init__, pid=1715, memory used=0.54GB) was running was 12.11GB / 12.67GB (0.955826), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-78feccce2a83d7d4e8587b092b2cd072e54ea27599522358a73669de*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "412\t7.02\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-5526268e-b11b...\n",
            "1713\t0.61\t\n",
            "1715\t0.54\tray::ClientAppActor.run\n",
            "802\t0.29\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:e4a14748fafa6ea13...\n",
            "88\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "1531\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "1611\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "1004\t0.05\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "1526\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "1567\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \r\u001b[1m   1/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m2:46:04\u001b[0m 4s/step - accuracy: 0.6875 - loss: 0.5096\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m   9/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m18s\u001b[0m 7ms/step - accuracy: 0.7579 - loss: 0.4490   \n",
            "\u001b[1m  30/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m14s\u001b[0m 5ms/step - accuracy: 0.7789 - loss: 0.4292\n",
            "\u001b[1m  87/2656\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.7818 - loss: 0.4290\n",
            "\u001b[1m 133/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 3ms/step - accuracy: 0.7807 - loss: 0.4319\n",
            "\u001b[1m 200/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7787 - loss: 0.4354\n",
            "\u001b[1m 248/2656\u001b[0m \u001b[32m━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7777 - loss: 0.4366\n",
            "\u001b[1m 313/2656\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 2ms/step - accuracy: 0.7765 - loss: 0.4381\n",
            "\u001b[1m 359/2656\u001b[0m \u001b[32m━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7758 - loss: 0.4392\n",
            "\u001b[1m 428/2656\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7749 - loss: 0.4407\n",
            "\u001b[1m 464/2656\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7746 - loss: 0.4413\n",
            "\u001b[1m 526/2656\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7743 - loss: 0.4421\n",
            "\u001b[1m 557/2656\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7742 - loss: 0.4424\n",
            "\u001b[1m 608/2656\u001b[0m \u001b[32m━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m4s\u001b[0m 2ms/step - accuracy: 0.7741 - loss: 0.4427\n",
            "\u001b[1m 666/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7740 - loss: 0.4432\n",
            "\u001b[1m 718/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7738 - loss: 0.4435\n",
            "\u001b[1m 776/2656\u001b[0m \u001b[32m━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7736 - loss: 0.4440\n",
            "\u001b[1m 832/2656\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7735 - loss: 0.4444\n",
            "\u001b[1m 897/2656\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7734 - loss: 0.4448\n",
            "\u001b[1m 949/2656\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7733 - loss: 0.4452\n",
            "\u001b[1m1012/2656\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7732 - loss: 0.4457\n",
            "\u001b[1m1051/2656\u001b[0m \u001b[32m━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━\u001b[0m \u001b[1m3s\u001b[0m 2ms/step - accuracy: 0.7731 - loss: 0.4460\n",
            "\u001b[1m1122/2656\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7730 - loss: 0.4465\n",
            "\u001b[1m1168/2656\u001b[0m \u001b[32m━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7730 - loss: 0.4467\n",
            "\u001b[1m1233/2656\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7729 - loss: 0.4471\n",
            "\u001b[1m1286/2656\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7729 - loss: 0.4474\n",
            "\u001b[1m1345/2656\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7728 - loss: 0.4477\n",
            "\u001b[1m1399/2656\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7728 - loss: 0.4479\n",
            "\u001b[1m1497/2656\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7727 - loss: 0.4483\n",
            "\u001b[1m1552/2656\u001b[0m \u001b[32m━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━\u001b[0m \u001b[1m2s\u001b[0m 2ms/step - accuracy: 0.7727 - loss: 0.4485\n",
            "\u001b[1m1610/2656\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7726 - loss: 0.4487\n",
            "\u001b[1m1643/2656\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7726 - loss: 0.4488\n",
            "\u001b[1m1705/2656\u001b[0m \u001b[32m━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7726 - loss: 0.4491\n",
            "\u001b[1m1752/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7725 - loss: 0.4492\n",
            "\u001b[1m1810/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7724 - loss: 0.4494\n",
            "\u001b[1m1857/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7724 - loss: 0.4496\n",
            "\u001b[1m1911/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7723 - loss: 0.4498\n",
            "\u001b[1m1969/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7723 - loss: 0.4500\n",
            "\u001b[1m2020/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7722 - loss: 0.4501\n",
            "\u001b[1m2088/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━━\u001b[0m \u001b[1m1s\u001b[0m 2ms/step - accuracy: 0.7722 - loss: 0.4503\n",
            "\u001b[1m2137/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7722 - loss: 0.4504\n",
            "\u001b[1m2191/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7722 - loss: 0.4505\n",
            "\u001b[1m2241/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7722 - loss: 0.4506\n",
            "\u001b[1m2308/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7722 - loss: 0.4508\n",
            "\u001b[1m2355/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 0.4508\n",
            "\u001b[1m2444/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 0.4510\n",
            "\u001b[1m2499/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 0.4511\n",
            "\u001b[1m2561/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 0.4512\n",
            "\u001b[1m2607/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 0.4513\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 1 results and 15 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
            "\u001b[92mINFO \u001b[0m:      Run finished 5 round(s) in 1826.95s\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (loss, distributed):\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.482171038819803\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.47786322236061096\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.5119647979736328\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 4: 0.5967308878898621\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 5: 0.4555927515029907\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (loss, centralized):\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 0: 3.5717809200286865\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.49314528703689575\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.48286232352256775\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.4959028363227844\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 4: 0.4712642431259155\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 5: 0.45878881216049194\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, centralized):\n",
            "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(0, 0.0199546180665493),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (1, 0.7516212463378906),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (2, 0.7537144422531128),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (3, 0.7527961134910583),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (4, 0.7643886208534241),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (5, 0.7704305648803711)]}\n",
            "\u001b[92mINFO \u001b[0m:      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2632/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 0.4513\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r\u001b[1m2656/2656\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 2ms/step - accuracy: 0.7721 - loss: 0.4514\n",
            "Total time taken:  0:32:25.300304\n",
            "\u001b[33mSIMULATION COMPLETE. Method = LEAVE_ONE_OUT - Individual (34) Classifier\n",
            "Number of Clients = 33\u001b[0m\n",
            "\n",
            "\u001b[36m(ClientAppActor pid=24048)\u001b[0m \u001b[33mClient 11 evaluation complete - Accuracy: 0.771041, Loss: 0.455593\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAA04AAAGJCAYAAAC90mOkAAAAOnRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjEwLjAsIGh0dHBzOi8vbWF0cGxvdGxpYi5vcmcvlHJYcgAAAAlwSFlzAAAPYQAAD2EBqD+naQAASt5JREFUeJzt3XlYVHXDxvF72EXBJRdUcEnLHVwzMlNLNC3LFiutXMpW9cncCisWl7TMLTWXrGjzzSW19/XJBRf0MbXc8BFLW11S3CpB0WBkzvvHxCQBwiBwZuD7ua65OnPmzJx78KfNze+cMxbDMAwBAAAAAPLkYXYAAAAAAHB1FCcAAAAAyAfFCQAAAADyQXECAAAAgHxQnAAAAAAgHxQnAAAAAMgHxQkAAAAA8kFxAgAAAIB8UJwAAAAAIB8UJwAAUKzq1aungQMHmh0DAK4JxQkAXMj+/fv14IMPqm7duvLz81Pt2rUVERGhWbNmmR2tyMXExMhisejs2bNmR3F5nTt3lsVicdzKlSun0NBQzZgxQzabzex4AFAmeJkdAABgt23bNnXp0kV16tTRU089paCgIB07dkw7duzQzJkzNWzYMLMjwkTBwcGaNGmSJOns2bNatGiRXnzxRZ05c0YTJ040OR0AlH4UJwBwERMnTlTFihW1c+dOVapUKdtjp0+fLrL9XLx4Uf7+/kX2eq6yL3dms9mUkZEhPz+/PLepWLGiHnvsMcf9Z599Vo0bN9asWbM0btw4eXp6lkRUACizOFQPAFzETz/9pGbNmuUoTZJUvXr1HOs++eQTtWnTRuXKlVOVKlX0yCOP6NixY9m26dy5s5o3b67du3frtttuk7+/v8aOHau7775b119/fa45wsPD1bZt2yLb17XauHGjOnbsqPLly6tSpUq699579d1332Xb5vz58xo+fLjq1asnX19fVa9eXREREdqzZ49jmx9++EEPPPCAgoKC5Ofnp+DgYD3yyCNKSUm56v6vfF+33HKLypUrp/r162vevHk5tk1PT1d0dLQaNmwoX19fhYSEaMyYMUpPT8+2ncVi0dChQ/Xpp5+qWbNm8vX11Zo1a5z6ufj5+aldu3Y6f/58tmJ9+fJljR8/Xg0aNJCvr6/q1aunsWPH5pohJiYmx+v+83ykuLg4WSwWffXVVxoxYoSqVaum8uXL67777tOZM2eyPdcwDE2YMEHBwcHy9/dXly5ddODAAafeFwC4KmacAMBF1K1bV9u3b1dSUpKaN29+1W0nTpyo1157TQ899JAGDx6sM2fOaNasWbrtttu0d+/ebOXrt99+U48ePfTII4/oscceU40aNdSmTRv1799fO3fuVLt27RzbHjlyRDt27NCUKVOKbF/XYv369erRo4euv/56xcTE6NKlS5o1a5Y6dOigPXv2qF69epLssy/Lli3T0KFD1bRpU/3222/aunWrvvvuO7Vu3VoZGRnq3r270tPTNWzYMAUFBen48eNatWqVzp07p4oVK141xx9//KGePXvqoYceUt++fbVkyRI999xz8vHx0RNPPCHJPmt0zz33aOvWrXr66afVpEkT7d+/X9OnT9f333+vlStXZnvNjRs3asmSJRo6dKiqVq3qeC/OOHz4sCwWS7Y/g8GDB+vDDz/Ugw8+qJEjR+rrr7/WpEmT9N1332nFihVO7yPLsGHDVLlyZUVHR+vw4cOaMWOGhg4dqsWLFzu2iYqK0oQJE9SzZ0/17NlTe/bsUbdu3ZSRkVHo/QKAyzAAAC5h3bp1hqenp+Hp6WmEh4cbY8aMMdauXWtkZGRk2+7w4cOGp6enMXHixGzr9+/fb3h5eWVb36lTJ0OSMW/evGzbpqSkGL6+vsbIkSOzrX/zzTcNi8ViHDlypMj2lZfo6GhDknHmzJk8t2nZsqVRvXp147fffnOs27dvn+Hh4WH079/fsa5ixYrGkCFD8nydvXv3GpKMpUuXFijblbLe19SpUx3r0tPTHdmy/nw+/vhjw8PDw/jPf/6T7fnz5s0zJBlfffWVY50kw8PDwzhw4ECBMzRu3Ng4c+aMcebMGePgwYPG6NGjDUnGXXfd5dguMTHRkGQMHjw42/NHjRplSDI2btyYLUN0dHSOfdWtW9cYMGCA4/4HH3xgSDK6du1q2Gw2x/oXX3zR8PT0NM6dO2cYhmGcPn3a8PHxMe66665s240dO9aQlO01AcAdcageALiIiIgIbd++Xffcc4/27dunN998U927d1ft2rX1v//7v47tli9fLpvNpoceekhnz5513IKCgnTDDTdo06ZN2V7X19dXgwYNyrYuMDBQPXr00JIlS2QYhmP94sWLdfPNN6tOnTpFtq/CSk5OVmJiogYOHKgqVao41oeGhioiIkJffvmlY12lSpX09ddf68SJE7m+VtaM0tq1a3Xx4kWns3h5eemZZ55x3Pfx8dEzzzyj06dPa/fu3ZKkpUuXqkmTJmrcuHG2n9Xtt98uSTl+Vp06dVLTpk0LnOHgwYOqVq2aqlWrpsaNG2vKlCm65557FBcX59gm62cyYsSIbM8dOXKkJOnf//53wd/0Pzz99NOyWCyO+x07dlRmZqaOHDkiyT47mJGRoWHDhmXbbvjw4YXeJwC4EooTALiQdu3aafny5frjjz/0zTffKDIyUufPn9eDDz6ob7/9VpL9XB3DMHTDDTc4Pkhn3b777rscF5KoXbu2fHx8cuzr4Ycf1rFjx7R9+3ZJ9nOsdu/erYcfftixTVHtqzCyPpA3atQox2NNmjTR2bNnlZaWJkl68803lZSUpJCQEN10002KiYnRzz//7Ni+fv36GjFihBYuXKiqVauqe/fumjNnTr7nN2WpVauWypcvn23djTfeKMl+uJxk/1kdOHAgx88pa7t//qzq169foH1nqVevnuLj47V27Vq98847ql27ts6cOZPtghJHjhyRh4eHGjZsmO25QUFBqlSpkuNnWhhZZTpL5cqVJdkPY8zatyTdcMMN2barVq2aY1sAcGec4wQALsjHx0ft2rVTu3btdOONN2rQoEFaunSpoqOjZbPZZLFYtHr16lyvpFahQoVs98uVK5frPnr16iV/f38tWbJEt9xyi5YsWSIPDw/16dPHsU1R7au4PfTQQ+rYsaNWrFihdevWacqUKXrjjTe0fPly9ejRQ5I0depUDRw4UF988YXWrVunf/3rX5o0aZJ27Nih4ODga85gs9nUokULTZs2LdfHQ0JCst139mdVvnx5de3a1XG/Q4cOat26tcaOHau3334727ZXzvg4KzMzM9f1eV2178oZSwAozShOAODisq5wl5ycLElq0KCBDMNQ/fr1HbMZhVG+fHndfffdWrp0qaZNm6bFixerY8eOqlWrlmObotpXYdStW1eSdOjQoRyPHTx4UFWrVs02C1SzZk09//zzev7553X69Gm1bt1aEydOdBQnSWrRooVatGihV199Vdu2bVOHDh00b948TZgw4apZTpw4obS0tGz7+/777yXJcVGHBg0aaN++fbrjjjuuqbgUVGhoqB577DHNnz9fo0aNUp06dVS3bl3ZbDb98MMPatKkiWPbU6dO6dy5c46fqWSfMTp37ly218zIyHCMM2dlvfYPP/yQ7YqNZ86cccxKAYA741A9AHARmzZtyvW391nnrWQdsnb//ffL09NTsbGxObY3DEO//fZbgff58MMP68SJE1q4cKH27duX7TC9ot6Xs2rWrKmWLVvqww8/zPYBPykpSevWrVPPnj0l2WdI/nnIXfXq1VWrVi3HJbhTU1N1+fLlbNu0aNFCHh4eOS7TnZvLly9r/vz5jvsZGRmaP3++qlWrpjZt2kiyz3odP35c7777bo7nX7p0yXFYYVEaM2aMrFarY5Yr62cyY8aMbNtlPX7XXXc51jVo0EBbtmzJtt2CBQvynHHKT9euXeXt7a1Zs2ZlGyv/zAIA7ooZJwBwEcOGDdPFixd13333qXHjxsrIyNC2bdu0ePFi1atXz3HRhQYNGmjChAmKjIzU4cOH1bt3bwUEBOiXX37RihUr9PTTT2vUqFEF2mfPnj0VEBCgUaNGydPTUw888EC2x4tyX3mZNm1aji/J9fDw0NixYzVlyhT16NFD4eHhevLJJx2XI69YsaLjO4jOnz+v4OBgPfjggwoLC1OFChW0fv167dy5U1OnTpVkv/T30KFD1adPH9144426fPmyPv7441zfc25q1aqlN954Q4cPH9aNN96oxYsXKzExUQsWLJC3t7ck6fHHH9eSJUv07LPPatOmTerQoYMyMzN18OBBLVmyRGvXrs3x/VjXqmnTpurZs6cWLlyo1157TWFhYRowYIAWLFigc+fOqVOnTvrmm2/04Ycfqnfv3urSpYvjuYMHD9azzz6rBx54QBEREdq3b5/Wrl2rqlWrFipLtWrVNGrUKE2aNEl33323evbsqb1792r16tWFfk0AcCnmXMwPAPBPq1evNp544gmjcePGRoUKFQwfHx+jYcOGxrBhw4xTp07l2P7zzz83br31VqN8+fJG+fLljcaNGxtDhgwxDh065NimU6dORrNmza6630cffdRxuem8FNW+rpR1OfLcbp6eno7t1q9fb3To0MEoV66cERgYaPTq1cv49ttvHY+np6cbo0ePNsLCwoyAgACjfPnyRlhYmPHOO+84tvn555+NJ554wmjQoIHh5+dnVKlSxejSpYuxfv36fHNmva9du3YZ4eHhhp+fn1G3bl1j9uzZObbNyMgw3njjDaNZs2aGr6+vUblyZaNNmzZGbGyskZKS4thO0lUvn55XhtwkJCRku7S41Wo1YmNjjfr16xve3t5GSEiIERkZafz555/ZnpeZmWm89NJLRtWqVQ1/f3+je/fuxo8//pjn5ch37tyZ7fmbNm0yJBmbNm3K9pqxsbFGzZo1jXLlyhmdO3c2kpKScrwmALgji2FwVicAAHnp3Lmzzp49q6SkJLOjAABMxDlOAAAAAJAPihMAAAAA5IPiBAAAAAD54BwnAAAAAMgHM04AAAAAkA+KEwAAAADko8x9Aa7NZtOJEycUEBAgi8VidhwAAAAAJjEMQ+fPn1etWrXk4XH1OaUyV5xOnDihkJAQs2MAAAAAcBHHjh1TcHDwVbcpc8UpICBAkv2HExgYaHIayWq1at26derWrZu8vb3NjgMXx3iBsxgzcBZjBs5izMBZrjRmUlNTFRIS4ugIV1PmilPW4XmBgYEuU5z8/f0VGBho+sCB62O8wFmMGTiLMQNnMWbgLFccMwU5hYeLQwAAAABAPihOAAAAAJAPihMAAAAA5IPiBAAAAAD5oDgBAAAAQD4oTgAAAACQD4qTmTIzZdm8WbW3bJFl82YpM9PsRAAAAAByQXEyy/LlUr168oqIUNtp0+QVESHVq2dfDwAAAMClUJzMsHy59OCD0q+/Zl9//Lh9PeUJAAAAcCkUp5KWmSm98IJkGDkfy1o3fDiH7QEAAAAuhOJU0v7zn5wzTVcyDOnYMft2AAAAAFwCxamkJScX7XYAAAAAih3FqaTVrFm02wEAAAAodqYWp7lz5yo0NFSBgYEKDAxUeHi4Vq9enef2cXFxslgs2W5+fn4lmLgIdOwoBQdLFkvuj1ssUkiIfTsAAAAALsHU4hQcHKzJkydr9+7d2rVrl26//Xbde++9OnDgQJ7PCQwMVHJysuN25MiREkxcBDw9pZkz7ct5lacZM+zbAQAAAHAJXmbuvFevXtnuT5w4UXPnztWOHTvUrFmzXJ9jsVgUFBRUEvGKz/33S8uW2a+u988LRcyZY38cAAAAgMswtThdKTMzU0uXLlVaWprCw8Pz3O7ChQuqW7eubDabWrdurddffz3PkiVJ6enpSk9Pd9xPTU2VJFmtVlmt1qJ7A87q1Uvq2VOZCQlKio9Xq02b5Ll3r2zffKPMwYPNywWXljVmTR27cCuMGTiLMQNnMWbgLFcaM85ksBhGbl8oVHL279+v8PBw/fnnn6pQoYIWLVqknj175rrt9u3b9cMPPyg0NFQpKSl66623tGXLFh04cEDBwcG5PicmJkaxsbE51i9atEj+/v5F+l6uReVDh3TbSy/J5uGhjXPmKI2LQwAAAADF6uLFi+rXr59SUlIUGBh41W1NL04ZGRk6evSoUlJStGzZMi1cuFCbN29W06ZN832u1WpVkyZN1LdvX40fPz7XbXKbcQoJCdHZs2fz/eGUBKvVqvj4eEVERMjvwQflsXq1bI89psz33zc7GlzQlePF29vb7DhwA4wZOIsxA2cxZuAsVxozqampqlq1aoGKk+mH6vn4+Khhw4aSpDZt2mjnzp2aOXOm5s+fn+9zvb291apVK/344495buPr6ytfX99cn2v2H9SVvL295TFunLR6tTwWLZLHK69IjRubHQsuytXGL1wfYwbOYszAWYwZOMsVxowz+3e573Gy2WzZZoiuJjMzU/v371fN0nJYW9u20r33SjabNG6c2WkAAAAA/MXU4hQZGaktW7bo8OHD2r9/vyIjI5WQkKBHH31UktS/f39FRkY6th83bpzWrVunn3/+WXv27NFjjz2mI0eOaHBpuphCTIz9v599Jl3lsuwAAAAASo6ph+qdPn1a/fv3V3JysipWrKjQ0FCtXbtWERERkqSjR4/Kw+PvbvfHH3/oqaee0smTJ1W5cmW1adNG27ZtK9D5UG6jZUvpgQekzz+3l6ilS81OBAAAAJR5phan995776qPJyQkZLs/ffp0TZ8+vRgTuYiYGGn5cvt3Pe3bJ4WFmZ0IAAAAKNNc7hwnSGreXHr4Yfty1qF7AAAAAExDcXJV0dGSh4e0cqW0e7fZaQAAAIAyjeLkqho3lvr1sy9HR5ubBQAAACjjKE6uLCpK8vSU/v1v6euvzU4DAAAAlFkUJ1d2ww1S//72ZWadAAAAANNQnFzdq69KXl7S2rXSV1+ZnQYAAAAokyhOru7666VBg+zLUVHmZgEAAADKKIqTO3jlFcnbW9q4UfrHd1sBAAAAKH4UJ3dQt6701FP25ehoyTDMzQMAAACUMRQndxEZKfn6Slu22GeeAAAAAJQYipO7CA6WnnnGvvzaa8w6AQAAACWI4uROIiOlcuWk7dvtV9kDAAAAUCIoTu4kKEh6/nn7clQUs04AAABACaE4uZsxYyR/f2nnTmnVKrPTAAAAAGUCxcndVK8uDRtmX2bWCQAAACgRFCd3NHq0VKGClJgorVxpdhoAAACg1KM4uaPrrpOGD7cvR0dLNpupcQAAAIDSjuLkrkaMkAIDpf37pWXLzE4DAAAAlGoUJ3dVubK9PElSTIyUmWlqHAAAAKA0ozi5s+HD7QXqu++kxYvNTgMAAACUWhQnd1axojRqlH05Nla6fNncPAAAAEApRXFyd8OG2S8W8f330qefmp0GAAAAKJUoTu4uIMD+pbiSNG6cZLWamwcAAAAohShOpcGQIfYvxv35Z+mjj8xOAwAAAJQ6FKfSoHx56eWX7cvjx0sZGebmAQAAAEoZilNp8eyzUs2a0pEj0vvvm50GAAAAKFUoTqVFuXLS2LH25YkTpT//NDcPAAAAUIpQnEqTwYOl4GDp11+lhQvNTgMAAACUGhSn0sTPT3rlFfvy669Lly6ZmwcAAAAoJShOpc0TT0h160rJydK8eWanAQAAAEoFilNp4+MjvfaafXnyZCktzdw8AAAAQClAcSqN+veXrr9eOn1aeucds9MAAAAAbo/iVBp5e0tRUfblN96Qzp83Nw8AAADg5ihOpdWjj0o33ij99ps0a5bZaQAAAAC3Zmpxmjt3rkJDQxUYGKjAwECFh4dr9erVV33O0qVL1bhxY/n5+alFixb68ssvSyitm/HykqKj7ctvvSWlpJibBwAAAHBjphan4OBgTZ48Wbt379auXbt0++23695779WBAwdy3X7btm3q27evnnzySe3du1e9e/dW7969lZSUVMLJ3cTDD0tNmkh//CHNnGl2GgAAAMBtmVqcevXqpZ49e+qGG27QjTfeqIkTJ6pChQrasWNHrtvPnDlTd955p0aPHq0mTZpo/Pjxat26tWbPnl3Cyd2Ep6cUE2NfnjbNXqAAAAAAOM3L7ABZMjMztXTpUqWlpSk8PDzXbbZv364RI0ZkW9e9e3etXLkyz9dNT09Xenq6435qaqokyWq1ymq1Xnvwa5SVodiy3HuvvJo3lyUpSZlTpsgWG1s8+0GJKPbxglKHMQNnMWbgLMYMnOVKY8aZDKYXp/379ys8PFx//vmnKlSooBUrVqhp06a5bnvy5EnVqFEj27oaNWro5MmTeb7+pEmTFJtLWVi3bp38/f2vLXwRio+PL7bXrnn33bopKUnG9OmKb9JE1sDAYtsXSkZxjheUTowZOIsxA2cxZuAsVxgzFy9eLPC2phenRo0aKTExUSkpKVq2bJkGDBigzZs351menBUZGZltlio1NVUhISHq1q2bAl2gQFitVsXHxysiIkLe3t7Fs5MePWSsWSOvxER1/+9/ZXv99eLZD4pdiYwXlCqMGTiLMQNnMWbgLFcaM1lHoxWE6cXJx8dHDRs2lCS1adNGO3fu1MyZMzV//vwc2wYFBenUqVPZ1p06dUpBQUF5vr6vr698fX1zrPf29jb9D+pKxZ5n3Djpnnvk+c478hw9Wqpevfj2hWLnauMXro8xA2cxZuAsxgyc5Qpjxpn9u9z3ONlstmznJF0pPDxcGzZsyLYuPj4+z3OicIW775batZMuXpTefNPsNAAAAIBbMbU4RUZGasuWLTp8+LD279+vyMhIJSQk6NFHH5Uk9e/fX5GRkY7tX3jhBa1Zs0ZTp07VwYMHFRMTo127dmno0KFmvQX3YbHYZ50kac4cKTnZ3DwAAACAGzG1OJ0+fVr9+/dXo0aNdMcdd2jnzp1au3atIiIiJElHjx5V8hUf8G+55RYtWrRICxYsUFhYmJYtW6aVK1eqefPmZr0F99K9uxQeLv35pzR5stlpAAAAALdh6jlO77333lUfT0hIyLGuT58+6tOnTzElKuWyZp0iIqT586XRo6XgYLNTAQAAAC7P5c5xQjG74w7pttuk9HRp0iSz0wAAAABugeJU1lx5rtO770pHj5qbBwAAAHADFKeyqFMn6fbbJatVmjDB7DQAAACAy6M4lVVZs04ffCD9/LO5WQAAAAAXR3Eqqzp0sF9l7/JlZp0AAACAfFCcyrLYWPt/P/pI+uEHc7MAAAAALoziVJa1by/ddZeUmfn3oXsAAAAAcqA4lXVZs06LFknffWduFgAAAMBFUZzKujZtpN69JZuNWScAAAAgDxQnSDEx9v8uXiwlJZkaBQAAAHBFFCdIYWHSgw9KhvF3iQIAAADgQHGCXUyMZLFIn38uJSaanQYAAABwKRQn2DVrJj3yiH2ZWScAAAAgG4oT/hYVJXl4SF98Ie3aZXYaAAAAwGVQnPC3xo2lRx+1L0dHm5sFAAAAcCEUJ2QXFSV5ekpffint2GF2GgAAAMAlUJyQXcOG0oAB9mVmnQAAAABJFCfk5tVXJS8vad06aetWs9MAAAAApqM4Iaf69aUnnrAvR0WZmwUAAABwARQn5O6VVyQfH2nTJvsNAAAAKMMoTshdnTrSU0/Zl6OiJMMwNw8AAABgIooT8hYZKfn62s9zWr/e7DQAAACAaShOyFvt2tKzz9qXmXUCAABAGUZxwtW9/LJUrpz9O53WrDE7DQAAAGAKihOuLihIGjLEvsysEwAAAMooihPyN2aMVL68tGuX9H//Z3YaAAAAoMRRnJC/atWkYcPsy1FRks1mbh4AAACghFGcUDCjRkkBAdK+fdLKlWanAQAAAEoUxQkFc9110vDh9uXoaGadAAAAUKZQnFBwL74oVawoJSVJS5eanQYAAAAoMRQnFFzlytLIkfblmBgpM9PUOAAAAEBJoTjBOS+8YC9QBw9Kn31mdhoAAACgRFCc4JzAQGn0aPtybKx0+bK5eQAAAIASQHGC84YNk6pWlX74QfrkE7PTAAAAAMXO1OI0adIktWvXTgEBAapevbp69+6tQ4cOXfU5cXFxslgs2W5+fn4llBiSpAoVpJdesi+PGydZrebmAQAAAIqZqcVp8+bNGjJkiHbs2KH4+HhZrVZ169ZNaWlpV31eYGCgkpOTHbcjR46UUGI4PP+8VKOG9MsvUlyc2WkAAACAYuVl5s7XrFmT7X5cXJyqV6+u3bt367bbbsvzeRaLRUFBQcUdD1fj7y+9/LL9EuUTJkj9+0u+vmanAgAAAIqFqcXpn1JSUiRJVapUuep2Fy5cUN26dWWz2dS6dWu9/vrratasWa7bpqenKz093XE/NTVVkmS1WmV1gUPMsjK4QhanPfGEvKZMkeXoUWW++65szzxjdqJSz63HC0zBmIGzGDNwFmMGznKlMeNMBothGEYxZikwm82me+65R+fOndPWrVvz3G779u364YcfFBoaqpSUFL311lvasmWLDhw4oODg4Bzbx8TEKDY2Nsf6RYsWyd/fv0jfQ1lU/8svFbpggS5dd53Wz50rm4+P2ZEAAACAArl48aL69eunlJQUBQYGXnVblylOzz33nFavXq2tW7fmWoDyYrVa1aRJE/Xt21fjx4/P8XhuM04hISE6e/Zsvj+ckmC1WhUfH6+IiAh5e3ubHcd56enyatJEll9/Vea0abINHWp2olLN7ccLShxjBs5izMBZjBk4y5XGTGpqqqpWrVqg4uQSh+oNHTpUq1at0pYtW5wqTZLk7e2tVq1a6ccff8z1cV9fX/nmcu6Nt7e36X9QV3K1PAXm7S29+qr07LPyfPNNeT7zjP38JxQrtx0vMA1jBs5izMBZjBk4yxXGjDP7N/WqeoZhaOjQoVqxYoU2btyo+vXrO/0amZmZ2r9/v2rWrFkMCVEggwZJ9epJJ09K8+aZnQYAAAAocqYWpyFDhuiTTz7RokWLFBAQoJMnT+rkyZO6dOmSY5v+/fsrMjLScX/cuHFat26dfv75Z+3Zs0ePPfaYjhw5osGDB5vxFiBJPj7Sa6/ZlydPlvK5nDwAAADgbkwtTnPnzlVKSoo6d+6smjVrOm6LFy92bHP06FElJyc77v/xxx966qmn1KRJE/Xs2VOpqanatm2bmjZtasZbQJbHH5caNJDOnJFmzzY7DQAAAFCkTD3HqSDXpUhISMh2f/r06Zo+fXoxJUKheXtLUVHSgAHSm29Kzz0nucDFNwAAAICiYOqME0qZfv2kRo2k33+XZs0yOw0AAABQZChOKDpeXlJ0tH35rbekc+dMjQMAAAAUFYoTitZDD0lNm9pL04wZZqcBAAAAigTFCUXL01OKibEvT59uP2wPAAAAcHMUJxS9Bx6QQkOl1FRp2jSz0wAAAADXjOKEoufhIcXG2pdnzpTOnjU3DwAAAHCNKE4oHvfeK7VqJV24IE2ZYnYaAAAA4JpQnFA8LBZp3Dj78uzZ0qlT5uYBAAAArgHFCcXnrrukm26SLl60fykuAAAA4KYoTig+V846vfOOdOKEuXkAAACAQqI4oXh16ybdcov055/S5MlmpwEAAAAKheKE4nXlrNP8+dKvv5qbBwAAACgEihOK3+23S506SRkZ0uuvm50GAAAAcBrFCcXPYvn7e50WLpSOHDE3DwAAAOAkihNKRqdO0h13SFarNGGC2WkAAAAAp1CcUHKyznX64APpp5/MzQIAAAA4geKEknPLLdKdd0qZmdL48WanAQAAAAqM4oSSlXWu08cfS99/b24WAAAAoIAoTihZN90k3X23ZLP9fegeAAAA4OIoTih5WYVp0SLpu+/MzQIAAAAUAMUJJa9VK+m++yTDkGJizE4DAAAA5IviBHNkFaYlS6T9+02NAgAAAOSH4gRzhIZKffrYl5l1AgAAgIujOME8MTGSxSItXy7t3Wt2GgAAACBPFCeYp2lTqW9f+3J0tLlZAAAAgKsoVHE6duyYfv31V8f9b775RsOHD9eCBQuKLBjKiKgoycND+r//k3buNDsNAAAAkKtCFad+/fpp06ZNkqSTJ08qIiJC33zzjV555RWN47t54IxGjaTHHrMvM+sEAAAAF1Wo4pSUlKSbbrpJkrRkyRI1b95c27Zt06effqq4uLiizIeyICpK8vSUVq+Wtm83Ow0AAACQQ6GKk9Vqla+vryRp/fr1uueeeyRJjRs3VnJyctGlQ9nQoIE0cKB9OSrK1CgAAABAbgpVnJo1a6Z58+bpP//5j+Lj43XnnXdKkk6cOKHrrruuSAOijHj1VcnbW1q/Xtqyxew0AAAAQDaFKk5vvPGG5s+fr86dO6tv374KCwuTJP3v//6v4xA+wCn16klPPmlf5lwnAAAAuBivwjypc+fOOnv2rFJTU1W5cmXH+qefflr+/v5FFg5lzNix0vvvSwkJ0saN0u23m50IAAAAkFTIGadLly4pPT3dUZqOHDmiGTNm6NChQ6pevXqRBkQZEhIiPf20fTkqSjIMc/MAAAAAfylUcbr33nv10UcfSZLOnTun9u3ba+rUqerdu7fmzp1bpAFRxkRGSn5+0ldfSfHxZqcBAAAAJBWyOO3Zs0cdO3aUJC1btkw1atTQkSNH9NFHH+ntt98u8OtMmjRJ7dq1U0BAgKpXr67evXvr0KFD+T5v6dKlaty4sfz8/NSiRQt9+eWXhXkbcEW1aknPPWdfZtYJAAAALqJQxenixYsKCAiQJK1bt07333+/PDw8dPPNN+vIkSMFfp3NmzdryJAh2rFjh+Lj42W1WtWtWzelpaXl+Zxt27apb9++evLJJ7V371717t1bvXv3VlJSUmHeClzRSy9J5cpJX38tUYoBAADgAgpVnBo2bKiVK1fq2LFjWrt2rbp16yZJOn36tAIDAwv8OmvWrNHAgQPVrFkzhYWFKS4uTkePHtXu3bvzfM7MmTN15513avTo0WrSpInGjx+v1q1ba/bs2YV5K3BFNWpIQ4fal5l1AgAAgAso1FX1oqKi1K9fP7344ou6/fbbFR4eLsk++9SqVatCh0lJSZEkValSJc9ttm/frhEjRmRb1717d61cuTLX7dPT05Wenu64n5qaKsn+Jb5Wq7XQWYtKVgZXyOJShg+X1zvvyLJnjy4vXy7jry9ZLusYL3AWYwbOYszAWYwZOMuVxowzGSyGUbhf5588eVLJyckKCwuTh4d94uqbb75RYGCgGjdu7PTr2Ww23XPPPTp37py2bt2a53Y+Pj768MMP1bdvX8e6d955R7GxsTp16lSO7WNiYhQbG5tj/aJFi7h0uotr8sknunHZMqXUq6eEadMkj0JNkAIAAAC5unjxovr166eUlJR8j5wr1IyTJAUFBSkoKEi//vqrJCk4OPiavvx2yJAhSkpKumppKozIyMhsM1SpqakKCQlRt27dnDqssLhYrVbFx8crIiJC3t7eZsdxLTffLGPtWlU8fFh3pafLeOABsxOZjvECZzFm4CzGDJzFmIGzXGnMZB2NVhCFKk42m00TJkzQ1KlTdeHCBUlSQECARo4cqVdeecUxA1VQQ4cO1apVq7RlyxYFBwdfddugoKAcM0unTp1SUFBQrtv7+vrK19c3x3pvb2/T/6Cu5Gp5XEKNGtKLL0rjxslr/HipTx/J09PsVC6B8QJnMWbgLMYMnMWYgbNcYcw4s/9CHfv0yiuvaPbs2Zo8ebL27t2rvXv36vXXX9esWbP02muvFfh1DMPQ0KFDtWLFCm3cuFH169fP9znh4eHasGFDtnXx8fGO86xQyrz4olSpkvTtt9LSpWanAQAAQBlVqOL04YcfauHChXruuecUGhqq0NBQPf/883r33XcVFxdX4NcZMmSIPvnkEy1atEgBAQE6efKkTp48qUuXLjm26d+/vyIjIx33X3jhBa1Zs0ZTp07VwYMHFRMTo127dmlo1lXYULpUqiSNHGlfjomRLl82Mw0AAADKqEIVp99//z3XC0A0btxYv//+e4FfZ+7cuUpJSVHnzp1Vs2ZNx23x4sWObY4ePark5GTH/VtuuUWLFi3SggULFBYWpmXLlmnlypVq3rx5Yd4K3MG//iVVqSIdOiT9z/+YnQYAAABlUKHOcQoLC9Ps2bP19ttvZ1s/e/ZshYaGFvh1CnJBv4SEhBzr+vTpoz59+hR4P3BzgYHS6NFSZKQUGyv17St5Ffq6JgAAAIDTCvXp880339Rdd92l9evXO84t2r59u44dO6Yvv/yySAMCkuxfiDttmvTTT9LHH0uDBpmdCAAAAGVIoQ7V69Spk77//nvdd999OnfunM6dO6f7779fBw4c0Mcff1zUGQGpQgXppZfsy+PGSRkZ5uYBAABAmVLo451q1aqliRMnZlu3b98+vffee1qwYME1BwNyeO45acoU6fBhKS5OevppsxMBAACgjCjUjBNgCn9/+3lOkjRhgpSebm4eAAAAlBkUJ7iXZ56RatWSjh2T3nvP7DQAAAAoIyhOcC9+ftIrr9iXJ06UrvjOLwAAAKC4OHWO0/3333/Vx8+dO3ctWYCCefJJafJk+6zTggXSCy+YnQgAAAClnFMzThUrVrzqrW7duurfv39xZQXsfH2lV1+1L0+aJF28aG4eAAAAlHpOzTh98MEHxZUDcM6gQfZZp19+kebOlUaONDsRAAAASjHOcYJ78vaWXnvNvjx5snThgrl5AAAAUKpRnOC+Hn9catBAOntWmj3b7DQAAAAoxShOcF9eXlJ0tH15yhQpNdXcPAAAACi1KE5wb/36SY0aSb//Ls2caXYaAAAAlFIUJ7g3T08pJsa+PHWqxCXxAQAAUAwoTnB/ffpIzZpJKSnS9OlmpwEAAEApRHGC+7ty1mn6dPthewAAAEARojihdLj/fiksTDp/XnrrLbPTAAAAoJShOKF08PCQYmPty2+/LZ05Y24eAAAAlCoUJ5Qe99wjtWkjpaXZL08OAAAAFBGKE0oPi0UaN86+PHu2dOqUuXkAAABQalCcULr06CG1by9duiRNnmx2GgAAAJQSFCeULlfOOs2dK504YW4eAAAAlAoUJ5Q+ERFShw5Sero0aZLZaQAAAFAKUJxQ+lgs0vjx9uUFC6Rjx8zNAwAAALdHcULp1KWL1LmzlJEhTZxodhoAAAC4OYoTSq+s73V67z3p8GFTowAAAMC9UZxQet12m9S1q3T5sjRhgtlpAAAA4MYoTijdsq6wFxcn/fijqVEAAADgvihOKN3Cw+3f7ZSZ+fcFIwAAAAAnUZxQ+mWd6/TJJ9KhQ+ZmAQAAgFuiOKH0a9dO6tVLstn+PnQPAAAAcALFCWVD1qzT//yPdOCAuVkAAADgdihOKBtatZLuv18yjL9LFAAAAFBAFCeUHTExksUiLV0q/fe/ZqcBAACAGzG1OG3ZskW9evVSrVq1ZLFYtHLlyqtun5CQIIvFkuN28uTJkgkM99aihfTQQ/blmBhTowAAAMC9mFqc0tLSFBYWpjlz5jj1vEOHDik5Odlxq169ejElRKkTHW2fdVqxQtqzx+w0AAAAcBNeZu68R48e6tGjh9PPq169uipVqlT0gVD6NWki9esnffqpvUT93/+ZnQgAAABuwNTiVFgtW7ZUenq6mjdvrpiYGHXo0CHPbdPT05Wenu64n5qaKkmyWq2yWq3FnjU/WRlcIUuZERkpr//5H1lWrdLlbdtktGtndqICY7zAWYwZOIsxA2cxZuAsVxozzmSwGIZhFGOWArNYLFqxYoV69+6d5zaHDh1SQkKC2rZtq/T0dC1cuFAff/yxvv76a7Vu3TrX58TExCg2l6uoLVq0SP7+/kUVH26m1dtvq87GjTrVqpV2REebHQcAAAAmuHjxovr166eUlBQFBgZedVu3Kk656dSpk+rUqaOPP/4418dzm3EKCQnR2bNn8/3hlASr1ar4+HhFRETI29vb7Dhlx88/y6tZM1kyM3U5IUHGLbeYnahAGC9wFmMGzmLMwFmMGTjLlcZMamqqqlatWqDi5JaH6l3ppptu0tatW/N83NfXV76+vjnWe3t7m/4HdSVXy1PqNWokDRokLVwor/HjpfXrzU7kFMYLnMWYgbMYM3AWYwbOcoUx48z+3f57nBITE1WzZk2zY8Advfqq5O0tbdggbd5sdhoAAAC4MFNnnC5cuKAff/zRcf+XX35RYmKiqlSpojp16igyMlLHjx/XRx99JEmaMWOG6tevr2bNmunPP//UwoULtXHjRq1bt86stwB3VreuNHiwNHeuFBUlJSTYL1UOAAAA/IOpM067du1Sq1at1KpVK0nSiBEj1KpVK0VFRUmSkpOTdfToUcf2GRkZGjlypFq0aKFOnTpp3759Wr9+ve644w5T8qMUGDtW8vGRtmyRNm40Ow0AAABclKkzTp07d9bVrk0RFxeX7f6YMWM0ZsyYYk6FMiU4WHrmGWnWLPus0+23M+sEAACAHNz+HCfgmkVGSn5+0rZtEod9AgAAIBcUJ6BmTen55+3Lr70mucYV+gEAAOBCKE6AJI0ZI/n7Szt3Sv/+t9lpAAAA4GIoToAk1aghDR1qX46KYtYJAAAA2VCcgCyjR0sVKkh790pffGF2GgAAALgQihOQpWpV6YUX7MtRUZLNZm4eAAAAuAyKE3ClESOkwEBp/37p88/NTgMAAAAXQXECrlSlivTii/blmBgpM9PUOAAAAHANFCfgn158UapUSfr2W2nxYrPTAAAAwAVQnIB/qlhRGjXKvhwbK12+bG4eAAAAmI7iBOTmX/+yH7b3/ffSokVmpwEAAIDJKE5AbgIC7F+KK0njxklWq7l5AAAAYCqKE5CXoUOlatWkn36SPvrI7DQAAAAwEcUJyEv58tLLL9uXx4+XMjLMzQMAAADTUJyAq3n2WSkoSDpyRPrgA7PTAAAAwCQUJ+Bq/P2lyEj78oQJUnq6uXkAAABgCooTkJ+nn5Zq15Z+/VV6912z0wAAAMAEFCcgP35+0iuv2Jdff126dMncPAAAAChxFCegIJ54QqpTR0pOlubPNzsNAAAAShjFCSgIX1/p1Vfty5MmSWlp5uYBAABAiaI4AQU1cKBUv750+rT0zjtmpwEAAEAJojgBBeXtLUVF2ZffeEM6f97cPAAAACgxFCfAGY89Jt1wg/Tbb9Ls2WanAQAAQAmhOAHO8PKSoqPty1OmSCkp5uYBAABAiaA4Ac565BGpcWPpjz+kmTPNTgMAAIASQHECnOXpKcXE2JenTbMXKAAAAJRqFCegMPr0kZo3tx+qN3262WkAAABQzChOQGF4eEixsfblGTPsF4sAAABAqUVxAgqrd2+pZUv7ZcnfesvsNAAAAChGFCegsK6cdZo1y/7FuAAAACiVKE7AtejVS2rbVkpLs1+eHAAAAKUSxQm4FhaLNG6cfXnOHOnkSXPzAAAAoFhQnIBrdeed0s03S5cuSZMnm50GAAAAxYDiBFyrK2ed5s2Tjh83Nw8AAACKnKnFacuWLerVq5dq1aoli8WilStX5vuchIQEtW7dWr6+vmrYsKHi4uKKPSeQr65dpVtvldLTpddfNzsNAAAAipipxSktLU1hYWGaM2dOgbb/5ZdfdNddd6lLly5KTEzU8OHDNXjwYK1du7aYkwL5sFik8ePty+++Kx09am4eAAAAFCkvM3feo0cP9ejRo8Dbz5s3T/Xr19fUqVMlSU2aNNHWrVs1ffp0de/evbhiAgXTubPUpYu0aZM0caI0f77ZiQAAAFBETC1Oztq+fbu6du2abV337t01fPjwPJ+Tnp6u9PR0x/3U1FRJktVqldVqLZaczsjK4ApZcO0sUVHy2rRJxvvv6/LIkVL9+kX6+owXOIsxA2cxZuAsxgyc5UpjxpkMblWcTp48qRo1amRbV6NGDaWmpurSpUsqV65cjudMmjRJsVlfUnqFdevWyd/fv9iyOis+Pt7sCCgi4S1bqnpiok48/7wShw0rln0wXuAsxgycxZiBsxgzcJYrjJmLFy8WeFu3Kk6FERkZqREjRjjup6amKiQkRN26dVNgYKCJyeysVqvi4+MVEREhb29vs+OgCFiuu07q2FF1EhJU6+23pRtuKLLXZrzAWYwZOIsxA2cxZuAsVxozWUejFYRbFaegoCCdOnUq27pTp04pMDAw19kmSfL19ZWvr2+O9d7e3qb/QV3J1fLgGtx6q9SzpyxffinvyZOljz4q8l0wXuAsxgycxZiBsxgzcJYrjBln9u9W3+MUHh6uDRs2ZFsXHx+v8PBwkxIBecg6PPTTT6WDB83NAgAAgGtmanG6cOGCEhMTlZiYKMl+ufHExEQd/etSzpGRkerfv79j+2effVY///yzxowZo4MHD+qdd97RkiVL9OKLL5oRH8hb27bSvfdKNtvfJQoAAABuy9TitGvXLrVq1UqtWrWSJI0YMUKtWrVSVFSUJCk5OdlRoiSpfv36+ve//634+HiFhYVp6tSpWrhwIZcih2uKibH/d/FiKSnJ1CgAAAC4Nqae49S5c2cZhpHn43Fxcbk+Z+/evcWYCigiLVtKDzwgff65fdZp6VKzEwEAAKCQ3OocJ8DtxMRIFou0bJm0b5/ZaQAAAFBIFCegODVvLj38sH05OtrcLAAAACg0ihNQ3KKjJQ8P6YsvpN27zU4DAACAQqA4AcWtcWOpXz/7MrNOAAAAboniBJSEqCjJ01P697+lHTvMTgMAAAAnUZyAknDDDVLWd5Ix6wQAAOB2KE5ASXn1VcnLS1q3Ttq61ew0AAAAcALFCSgp118vDRpkX2bWCQAAwK1QnICS9Oqrkre3tHGjlJBgdhoAAAAUEMUJKEl16khPPWVfjoqSDMPcPAAAACgQihNQ0iIjJV9f6T//kTZsMDsNAAAACoDiBJS04GDpmWfsy8w6AQAAuAWKE2CGyEipXDlp+3ZpzRqz0wAAACAfFCfADEFB0vPP25eZdQIAAHB5FCfALGPGSP7+0q5d0qpVZqcBAADAVVCcALNUry4NG2ZfjoqSbDZz8wAAACBPFCfATKNHSxUqSImJ0sqVZqcBAABAHihOgJmuu04aPty+HB3NrBMAAICLojgBZhsxQgoMlJKSpGXLzE4DAACAXFCcALNVrmwvT5IUEyNlZpoaBwAAADlRnABXMHy4vUB995302WdmpwEAAMA/UJwAV1CxojRqlH05Nla6fNncPAAAAMiG4gS4imHD7BeL+OEH6dNPzU4DAACAK1CcAFcRECC99JJ9edw4yWo1Nw8AAAAcKE6AK3n+efsX4/78s/Thh2anAQAAwF8oToArKV9eevll+/L48VJGhrl5AAAAIIniBLieZ5+VataUjh6V3n/f7DQAAAAQxQlwPeXKSWPH2pcnTJD+/NPcPAAAAKA4AS5p8GApOFg6flx6912z0wAAAJR5FCfAFfn5Sa+8Yl9+/XXp0iVz8wAAAJRxFCfAVT3xhFS3rnTypDR3rtlpAAAAyjSKE+CqfHyk116zL0+eLKWlmZsHAACgDKM4Aa6sf3/p+uulM2ekOXPMTgMAAFBmUZwAV+btLUVF2ZfffFM6f97cPABKt8xMWTZvVu0tW2TZvFnKzDQ7EQC4DJcoTnPmzFG9evXk5+en9u3b65tvvslz27i4OFkslmw3Pz+/EkwLlLBHH5VuvFH67Td5jBzJBxoAxWP5cqlePXlFRKjttGnyioiQ6tWzrwcAmF+cFi9erBEjRig6Olp79uxRWFiYunfvrtOnT+f5nMDAQCUnJztuR44cKcHEQAnz8pJ69pQkecbF8YEGBcfsAQpq+XLpwQelX3/Nvv74cft6/q0BAPOL07Rp0/TUU09p0KBBatq0qebNmyd/f3+9//77eT7HYrEoKCjIcatRo0YJJgZK2PLl0syZOdfzgQZXw+wBCiozU3rhBckwcj6WtW74cIo3cscvaFCGeJm584yMDO3evVuRkZGOdR4eHuratau2b9+e5/MuXLigunXrymazqXXr1nr99dfVrFmzXLdNT09Xenq6435qaqokyWq1ymq1FtE7KbysDK6QBS4oM1Ne//qXZBiy/PMxw5AhSYMHK/PsWcnTU7JYJA8P+62olnN5zMhaV8SvW6Bl5MuyYoU8H3kkx7gx/irbmZ99JuO++0zLV2IMQ7LZst8yM3OuK+jjVz5mGLIU5WtnPX5FZktRvHbW6+XyuOP1T5yQxz9nmv75czx2TJkPPGD/igQvL/u/N15eV70ZBdyuUNt7e9v/62H673/LNMuKFfIcMUJex4+rrSRNmyajdm1lTptWNv6NQeFkZiozIUG1t2xRpq+v1Lmz/e++SZz5DG5qcTp79qwyMzNzzBjVqFFDBw8ezPU5jRo10vvvv6/Q0FClpKTorbfe0i233KIDBw4oODg4x/aTJk1SbGxsjvXr1q2Tv79/0byRIhAfH292BLig6/bv163Hj+f5uEWS/vhDXs88U2KZzGZYLDL+Km3GX6Uq2/381mcVP0nGFdvktT7bc/J5rQLto4DPKdD63N6vpPpffplr2bb8VbaNgQN1+LPP7NsYhqMEZFu+ohxcuf7Kx3Osz+W5V64v8DZXPp7bNn/t+8r1V65zPIYi5fnFF2ZHyMGwWGR4esrm4SHD0/Pvm4eHbAW9/9dz/3nfsc6J+//Mkm2fueR09v4/15n5y6Sa27er3Rtv5Hzg+HF5Pvywdr70kpLDw0s+GFxaze3b1WLhQpX77TdH2b503XXaP3iwaePl4sWLBd7WYhi5zc2XjBMnTqh27dratm2bwq/4YY0ZM0abN2/W119/ne9rWK1WNWnSRH379tX48eNzPJ7bjFNISIjOnj2rwMDAonkj18BqtSo+Pl4RERHy9vY2Ow5cjOWzz+TVv3++29latpSCgrL/hrmAy5a8tsm6n7XOydfNc/nKD7uAizCunAn19Mw+M1rQxyyWXB8v8GtnPf7Pmdl8nmtc7fErXyuvfR8+LM/33sv3Z5T56KNS7drS5cv2GazLl6XLl2X567+53q7Y7spbjufksZ1je47KyJXh4ZH/7Nw/ZvGMAs4A5rn9X+PIY+5c6fz5nEdDSPajISpVkm38ePvzrzbmi+DmOArCmb+vzt444uGa5Xk0xF8/W7OOhkhNTVXVqlWVkpKSbzcwdcapatWq8vT01KlTp7KtP3XqlIKCggr0Gt7e3mrVqpV+/PHHXB/39fWVr69vrs9zpaLianngIkJCCrSZx/Tp9qlud5JVyoq4kBX5squ+Vl6v+9130tq1+f/8e/SQmjXL/0N8YR4r7seL+rUtllw//BUVl/+4lZlpHzPHj+d+npPFIgUHy/PDD009nCa/cuUSN6u1eF43j/OGLDablJFhvxVQSYxHiySdOyfPYcNKYG8lKL9yVZRFrSC3ktzfte5Lkl58Mdd/YyyGIVks8ho1SnrggRL/d8aZz9+mFicfHx+1adNGGzZsUO/evSVJNptNGzZs0NChQwv0GpmZmdq/f796/nXVMaBU6dhRCg7O9wONOnYs+WzXKuvwNcncD2OlTUJCwYrTmDHuV7ZRPDw97RegefBB+9/JK/+tyfo7OmOG+X9PPT3tt1x+GVrqGUbRFsdrLXgHDhTs35m2baVata5+ft613PI79y+/m7NHPmQ9D0Xvr3Mp9Z//uPT/m0wtTpI0YsQIDRgwQG3bttVNN92kGTNmKC0tTYMGDZIk9e/fX7Vr19akSZMkSePGjdPNN9+shg0b6ty5c5oyZYqOHDmiwYMHm/k2gOLhLh9o4DpKc9lG8bn/fmnZMvvV9a68UERwsP3fmPvvNy0aZP97m3WonCso6C9opkxx6Q/BOY56KMpSVpyFz8zXL+xrHz8uJSXl/2eSnFz8f+7XwPS/gQ8//LDOnDmjqKgonTx5Ui1bttSaNWscF4w4evSoPLKm+CT98ccfeuqpp3Ty5ElVrlxZbdq00bZt29S0aVOz3gJQvPhAA2dQtlFY998v3XuvLm/apMTVq9WyRw95denCWEFOpeUXNFdeHRbFKyFB6tIl/+1q1iz2KNfC1ItDmCE1NVUVK1Ys0AlgJcFqterLL79Uz549OccJV5eZyQcaFNzy5TnLdkgIZRv54v9LKJCsL02Wcv8FzbJl/FuDv2Vm2r9LML+y/csvJf7ZxpluQMUG3IWnp4xOnXT8tttkdOpEacLV3X+/dPiwLsfHa9eIEbocH2//HxIfZAAUhayjIWrXzr4+OJjShJyyjoaQcl6h0I2OhqA4AUBpRdkGUJz4BQ2cUQrKtunnOAEAAMBNZf2CJi1NYfyCBvlx83MpKU4AAAAASoYbl20O1QMAAACAfFCcAAAAACAfFCcAAAAAyAfFCQAAAADyQXECAAAAgHxQnAAAAAAgH2XucuSGYUiSUlNTTU5iZ7VadfHiRaWmpsrb29vsOHBxjBc4izEDZzFm4CzGDJzlSmMmqxNkdYSrKXPF6fz585KkkJAQk5MAAAAAcAXnz59XxYoVr7qNxShIvSpFbDabTpw4oYCAAFksFrPjKDU1VSEhITp27JgCAwPNjgMXx3iBsxgzcBZjBs5izMBZrjRmDMPQ+fPnVatWLXl4XP0spjI34+Th4aHg4GCzY+QQGBho+sCB+2C8wFmMGTiLMQNnMWbgLFcZM/nNNGXh4hAAAAAAkA+KEwAAAADkg+JkMl9fX0VHR8vX19fsKHADjBc4izEDZzFm4CzGDJzlrmOmzF0cAgAAAACcxYwTAAAAAOSD4gQAAAAA+aA4AQAAAEA+KE4AAAAAkA+Kk4nmzJmjevXqyc/PT+3bt9c333xjdiS4sC1btqhXr16qVauWLBaLVq5caXYkuLBJkyapXbt2CggIUPXq1dW7d28dOnTI7FhwYXPnzlVoaKjjCynDw8O1evVqs2PBTUyePFkWi0XDhw83OwpcWExMjCwWS7Zb48aNzY5VYBQnkyxevFgjRoxQdHS09uzZo7CwMHXv3l2nT582OxpcVFpamsLCwjRnzhyzo8ANbN68WUOGDNGOHTsUHx8vq9Wqbt26KS0tzexocFHBwcGaPHmydu/erV27dun222/XvffeqwMHDpgdDS5u586dmj9/vkJDQ82OAjfQrFkzJScnO25bt241O1KBcTlyk7Rv317t2rXT7NmzJUk2m00hISEaNmyYXn75ZZPTwdVZLBatWLFCvXv3NjsK3MSZM2dUvXp1bd68WbfddpvZceAmqlSpoilTpujJJ580Owpc1IULF9S6dWu98847mjBhglq2bKkZM2aYHQsuKiYmRitXrlRiYqLZUQqFGScTZGRkaPfu3eratatjnYeHh7p27art27ebmAxAaZWSkiLJ/kEYyE9mZqY+++wzpaWlKTw83Ow4cGFDhgzRXXfdle0zDXA1P/zwg2rVqqXrr79ejz76qI4ePWp2pALzMjtAWXT27FllZmaqRo0a2dbXqFFDBw8eNCkVgNLKZrNp+PDh6tChg5o3b252HLiw/fv3Kzw8XH/++acqVKigFStWqGnTpmbHgov67LPPtGfPHu3cudPsKHAT7du3V1xcnBo1aqTk5GTFxsaqY8eOSkpKUkBAgNnx8kVxAoBSbsiQIUpKSnKr48hhjkaNGikxMVEpKSlatmyZBgwYoM2bN1OekMOxY8f0wgsvKD4+Xn5+fmbHgZvo0aOHYzk0NFTt27dX3bp1tWTJErc4JJjiZIKqVavK09NTp06dyrb+1KlTCgoKMikVgNJo6NChWrVqlbZs2aLg4GCz48DF+fj4qGHDhpKkNm3aaOfOnZo5c6bmz59vcjK4mt27d+v06dNq3bq1Y11mZqa2bNmi2bNnKz09XZ6eniYmhDuoVKmSbrzxRv34449mRykQznEygY+Pj9q0aaMNGzY41tlsNm3YsIFjyQEUCcMwNHToUK1YsUIbN25U/fr1zY4EN2Sz2ZSenm52DLigO+64Q/v371diYqLj1rZtWz366KNKTEykNKFALly4oJ9++kk1a9Y0O0qBMONkkhEjRmjAgAFq27atbrrpJs2YMUNpaWkaNGiQ2dHgoi5cuJDtNzK//PKLEhMTVaVKFdWpU8fEZHBFQ4YM0aJFi/TFF18oICBAJ0+elCRVrFhR5cqVMzkdXFFkZKR69OihOnXq6Pz581q0aJESEhK0du1as6PBBQUEBOQ4Z7J8+fK67rrrOJcSeRo1apR69eqlunXr6sSJE4qOjpanp6f69u1rdrQCoTiZ5OGHH9aZM2cUFRWlkydPqmXLllqzZk2OC0YAWXbt2qUuXbo47o8YMUKSNGDAAMXFxZmUCq5q7ty5kqTOnTtnW//BBx9o4MCBJR8ILu/06dPq37+/kpOTVbFiRYWGhmrt2rWKiIgwOxqAUuLXX39V37599dtvv6latWq69dZbtWPHDlWrVs3saAXC9zgBAAAAQD44xwkAAAAA8kFxAgAAAIB8UJwAAAAAIB8UJwAAAADIB8UJAAAAAPJBcQIAAACAfFCcAAAAACAfFCcAAAAAyAfFCQCAa9S5c2cNHz7c7BgAgGJEcQIAuIWBAwfKYrHIYrHI29tb9evX15gxY/Tnn3+aHQ0AUAZ4mR0AAICCuvPOO/XBBx/IarVq9+7dGjBggCwWi9544w2zowEASjlmnAAAbsPX11dBQUEKCQlR79691bVrV8XHx0uS0tPT9a9//UvVq1eXn5+fbr31Vu3cudPx3Li4OFWqVCnb661cuVIWi8VxPyYmRi1bttTHH3+sevXqqWLFinrkkUd0/vx5xzZpaWnq37+/KlSooJo1a2rq1KnF+6YBAC6B4gQAcEtJSUnatm2bfHx8JEljxozR559/rg8//FB79uxRw4YN1b17d/3+++9Ove5PP/2klStXatWqVVq1apU2b96syZMnOx4fPXq0Nm/erC+++ELr1q1TQkKC9uzZU6TvDQDgeihOAAC3sWrVKlWoUEF+fn5q0aKFTp8+rdGjRystLU1z587VlClT1KNHDzVt2lTvvvuuypUrp/fee8+pfdhsNsXFxal58+bq2LGjHn/8cW3YsEGSdOHCBb333nt66623dMcdd6hFixb68MMPdfny5eJ4uwAAF8I5TgAAt9GlSxfNnTtXaWlpmj59ury8vPTAAw/ov//9r6xWqzp06ODY1tvbWzfddJO+++47p/ZRr149BQQEOO7XrFlTp0+flmSfjcrIyFD79u0dj1epUkWNGjW6xncGAHB1FCcAgNsoX768GjZsKEl6//33FRYWpvfee0/t2rXL97keHh4yDCPbOqvVmmM7b2/vbPctFotsNts1pAYAlAYcqgcAcEseHh4aO3asXn31VTVo0EA+Pj766quvHI9brVbt3LlTTZs2lSRVq1ZN58+fV1pammObxMREp/bZoEEDeXt76+uvv3as++OPP/T9999f25sBALg8ihMAwG316dNHnp6emjt3rp577jmNHj1aa9as0bfffqunnnpKFy9e1JNPPilJat++vfz9/TV27Fj99NNPWrRokeLi4pzaX4UKFfTkk09q9OjR2rhxo5KSkjRw4EB5ePC/UwAo7ThUDwDgtry8vDR06FC9+eab+uWXX2Sz2fT444/r/Pnzatu2rdauXavKlStLsp+L9Mknn2j06NF69913dccddygmJkZPP/20U/ucMmWKLly4oF69eikgIEAjR45USkpKcbw9AIALsRj/POAbAAAAAJANxxYAAAAAQD4oTgAAAACQD4oTAAAAAOSD4gQAAAAA+aA4AQAAAEA+KE4AAAAAkA+KEwAAAADkg+IEAAAAAPmgOAEAAABAPihOAAAAAJAPihMAAAAA5OP/ATySjj8uqFOXAAAAAElFTkSuQmCC\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- LOG START ---\n",
        "print(f\"{Colours.YELLOW.value}\\nDeploy simulation... Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\")\n",
        "print(f\"Number of Clients = {NUM_OF_CLIENTS}\\n\")\n",
        "print(f\"Writing output to: {sub_dir_name}/{test_directory_name}\\n{Colours.NORMAL.value}\")\n",
        "\n",
        "# Ghi thông tin ban đầu vào file\n",
        "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Run_details.txt\", \"a\") as f:\n",
        "    f.write(f\"{datetime.datetime.now()} - Deploy simulation... Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\\n\")\n",
        "    f.write(f\"{datetime.datetime.now()} - Number of Clients = {NUM_OF_CLIENTS}\\n\")\n",
        "    f.write(f\"{datetime.datetime.now()} - Original train_df size: {train_df_shape}\\n\")\n",
        "\n",
        "    for i in range(len(fl_X_train)):\n",
        "        f.write(f\"{datetime.datetime.now()} - {i}: X Shape {fl_X_train[i].shape}, Y Shape {fl_y_train[i].shape}\\n\")\n",
        "\n",
        "    f.write(f\"{datetime.datetime.now()} - X_test size: {X_test.shape}\\n\")\n",
        "    f.write(f\"{datetime.datetime.now()} - y_test size: {y_test.shape}\\n\")\n",
        "\n",
        "# --- START SIMULATION ---\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "ray_init_args = {\"num_cpus\": 2}\n",
        "\n",
        "server_accuracy_history = []\n",
        "server_loss_history = []\n",
        "\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_OF_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=NUM_OF_ROUNDS),\n",
        "    strategy=strategy,\n",
        "    client_resources={\"num_cpus\": 1},\n",
        "    ray_init_args=ray_init_args,\n",
        ")\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(\"Total time taken: \", end_time - start_time)\n",
        "\n",
        "# --- LOG END ---\n",
        "print(f\"{Colours.YELLOW.value}SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\")\n",
        "print(f\"Number of Clients = {NUM_OF_CLIENTS}{Colours.NORMAL.value}\\n\")\n",
        "\n",
        "# Ghi thông tin kết thúc vào file\n",
        "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Run_details.txt\", \"a\") as f:\n",
        "    f.write(f\"{datetime.datetime.now()} - SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\\n\")\n",
        "    f.write(f\"{datetime.datetime.now()} - Total time taken: {end_time - start_time}\\n\")\n",
        "\n",
        "# --- PLOT ACCURACY ---\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(server_accuracy_history, marker='o')\n",
        "plt.title('Server Accuracy per Round')\n",
        "plt.xlabel('Round')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"Output/{sub_dir_name}/{test_directory_name}/server_accuracy_per_round.png\")\n",
        "plt.show()\n",
        "\n",
        "# --- PLOT LOSS ---\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(server_loss_history, marker='o', color='red')\n",
        "plt.title('Server Loss per Round')\n",
        "plt.xlabel('Round')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"Output/{sub_dir_name}/{test_directory_name}/server_loss_per_round.png\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}