{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "VKGr6k91wdI-",
      "metadata": {
        "id": "VKGr6k91wdI-"
      },
      "source": [
        "# 01-Data_Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "id": "Kr3V2eUhxNrD",
      "metadata": {
        "id": "Kr3V2eUhxNrD"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "# %pip install flwr[simulation] torch torchvision matplotlib sklearn openml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "id": "bThv7qdJHQF6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "bThv7qdJHQF6",
        "outputId": "7b0fdaa5-e003-4618-ccae-b6b04107e4f4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.8.0+cu126)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.12/dist-packages (0.23.0+cu126)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (3.10.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: openml in /usr/local/lib/python3.12/dist-packages (0.15.1)\n",
            "Requirement already satisfied: flwr[simulation] in /usr/local/lib/python3.12/dist-packages (1.20.0)\n",
            "Requirement already satisfied: click<8.2.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (8.1.8)\n",
            "Requirement already satisfied: cryptography<45.0.0,>=44.0.1 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (44.0.3)\n",
            "Requirement already satisfied: grpcio!=1.65.0,<2.0.0,>=1.62.3 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (1.74.0)\n",
            "Requirement already satisfied: grpcio-health-checking<2.0.0,>=1.62.3 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (1.62.3)\n",
            "Requirement already satisfied: iterators<0.0.3,>=0.0.2 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (0.0.2)\n",
            "Requirement already satisfied: numpy<3.0.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.0.2)\n",
            "Requirement already satisfied: pathspec<0.13.0,>=0.12.1 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (0.12.1)\n",
            "Requirement already satisfied: protobuf<5.0.0,>=4.21.6 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (4.25.8)\n",
            "Requirement already satisfied: pycryptodome<4.0.0,>=3.18.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (3.23.0)\n",
            "Requirement already satisfied: pyyaml<7.0.0,>=6.0.2 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (6.0.2)\n",
            "Requirement already satisfied: ray==2.31.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.31.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.31.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.32.4)\n",
            "Requirement already satisfied: rich<14.0.0,>=13.5.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (13.9.4)\n",
            "Requirement already satisfied: tomli<3.0.0,>=2.0.1 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (2.2.1)\n",
            "Requirement already satisfied: tomli-w<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (1.2.0)\n",
            "Requirement already satisfied: typer<0.13.0,>=0.12.5 in /usr/local/lib/python3.12/dist-packages (from flwr[simulation]) (0.12.5)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (3.19.1)\n",
            "Requirement already satisfied: jsonschema in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (4.25.1)\n",
            "Requirement already satisfied: msgpack<2.0.0,>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (25.0)\n",
            "Requirement already satisfied: aiosignal in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (1.4.0)\n",
            "Requirement already satisfied: frozenlist in /usr/local/lib/python3.12/dist-packages (from ray==2.31.0->flwr[simulation]) (1.7.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.12/dist-packages (from torch) (4.15.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.13.3)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.12/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.3 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.4.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.4.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.12/dist-packages (from torchvision) (11.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (4.59.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib) (2.9.0.post0)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.1)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: liac-arff>=2.4.0 in /usr/local/lib/python3.12/dist-packages (from openml) (2.5.0)\n",
            "Requirement already satisfied: xmltodict in /usr/local/lib/python3.12/dist-packages (from openml) (0.14.2)\n",
            "Requirement already satisfied: pandas>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from openml) (2.2.2)\n",
            "Requirement already satisfied: minio in /usr/local/lib/python3.12/dist-packages (from openml) (7.2.16)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.12/dist-packages (from openml) (18.1.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from openml) (4.67.1)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.12/dist-packages (from cryptography<45.0.0,>=44.0.1->flwr[simulation]) (1.17.1)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->openml) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas>=1.0.0->openml) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3.0.0,>=2.31.0->flwr[simulation]) (2025.8.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich<14.0.0,>=13.5.0->flwr[simulation]) (2.19.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.12/dist-packages (from typer<0.13.0,>=0.12.5->flwr[simulation]) (1.5.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Requirement already satisfied: argon2-cffi in /usr/local/lib/python3.12/dist-packages (from minio->openml) (25.1.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12->cryptography<45.0.0,>=44.0.1->flwr[simulation]) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich<14.0.0,>=13.5.0->flwr[simulation]) (0.1.2)\n",
            "Requirement already satisfied: argon2-cffi-bindings in /usr/local/lib/python3.12/dist-packages (from argon2-cffi->minio->openml) (25.1.0)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (25.3.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (2025.4.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.36.2)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.12/dist-packages (from jsonschema->ray==2.31.0->flwr[simulation]) (0.27.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install flwr[simulation] torch torchvision matplotlib scikit-learn openml"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "id": "K1gp_nItxkUT",
      "metadata": {
        "id": "K1gp_nItxkUT"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import flwr as fl\n",
        "from tqdm import tqdm\n",
        "import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "\n",
        "import sklearn\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn import preprocessing\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "from flwr.common import Metrics\n",
        "from torch.utils.data import DataLoader, random_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "id": "OKah0ChG06AA",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OKah0ChG06AA",
        "outputId": "6320da30-e7f7-47b2-fef7-6058fa751dae"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "flwr 1.20.0\n",
            "numpy 2.0.2\n",
            "torch 2.8.0+cu126\n",
            "Training on cpu\n"
          ]
        }
      ],
      "source": [
        "print(\"flwr\", fl.__version__)\n",
        "print(\"numpy\", np.__version__)\n",
        "print(\"torch\", torch.__version__)\n",
        "\n",
        "DEVICE = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Training on {DEVICE}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "id": "69d0407d6015e2ba",
      "metadata": {
        "id": "69d0407d6015e2ba"
      },
      "outputs": [],
      "source": [
        "### THIS SECTION NEEDS TO BE SET TO DETERMINE WHICH CONFIGURATION METHOD TO UTILISE\n",
        "\n",
        "SPLIT_AVAILABLE_METHODS = ['STRATIFIED','LEAVE_ONE_OUT', 'ONE_CLASS', 'HALF_BENIGN' ]\n",
        "METHOD = 'LEAVE_ONE_OUT'\n",
        "NUM_OF_STRATIFIED_CLIENTS = 10  # only applies to stratified method\n",
        "NUM_OF_ROUNDS = 5              # Number of FL rounds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "id": "1caa8703e9870d43",
      "metadata": {
        "id": "1caa8703e9870d43"
      },
      "outputs": [],
      "source": [
        "individual_classifier = True\n",
        "group_classifier = False\n",
        "binary_classifier = False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "id": "6lWhv3SmrgMx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6lWhv3SmrgMx",
        "outputId": "ede7fc06-3b6e-4e9f-bab2-1de573d11cd2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "id": "28e5b1bb8c5b4c77",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "28e5b1bb8c5b4c77",
        "outputId": "47a1951a-6801-4aa3-b8de-0d9b604f80ee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading 4 training files...\n",
            "Available columns in dataset: ['Header_Length', 'Protocol Type', 'Time_To_Live', 'Rate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number', 'cwr_flag_number', 'ack_count', 'syn_count', 'fin_count', 'rst_count', 'HTTP', 'HTTPS', 'DNS', 'Telnet', 'SMTP', 'SSH', 'IRC', 'TCP', 'UDP', 'DHCP', 'ARP', 'ICMP', 'IGMP', 'IPv', 'LLC', 'Tot sum', 'Min', 'Max', 'AVG', 'Std', 'Tot size', 'IAT', 'Number', 'Variance', 'Label']\n",
            "Dataset shape: (712311, 40)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "\r  0%|          | 0/4 [00:00<?, ?it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rounding numbers in Merged01.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "\r 25%|██▌       | 1/4 [00:09<00:27,  9.00s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rounding numbers in Merged02.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            " 50%|█████     | 2/4 [00:18<00:19,  9.51s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rounding numbers in Merged03.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            " 75%|███████▌  | 3/4 [00:22<00:06,  6.76s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Rounding numbers in Merged04.csv...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "100%|██████████| 4/4 [00:25<00:00,  6.44s/it]\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Combined dataset shape: (2834805, 40)\n",
            "All numbers have been rounded during loading process\n",
            "Using 39 feature columns\n",
            "Feature columns: ['Header_Length', 'Protocol Type', 'Time_To_Live', 'Rate', 'fin_flag_number', 'syn_flag_number', 'rst_flag_number', 'psh_flag_number', 'ack_flag_number', 'ece_flag_number']...\n",
            "Label distribution:\n",
            "Label\n",
            "0      66396\n",
            "1     244023\n",
            "2     247295\n",
            "3     245610\n",
            "4     326263\n",
            "5     270683\n",
            "6     432865\n",
            "7     217410\n",
            "8      17330\n",
            "9      17271\n",
            "10     27248\n",
            "11      1380\n",
            "12      1766\n",
            "13    199952\n",
            "14    121972\n",
            "15    160806\n",
            "16      4426\n",
            "17     59802\n",
            "18     45003\n",
            "19     53710\n",
            "20       141\n",
            "21      5795\n",
            "22      4962\n",
            "23     22623\n",
            "24      8165\n",
            "25     10938\n",
            "26     18654\n",
            "27       344\n",
            "28       180\n",
            "29       258\n",
            "30        84\n",
            "31       323\n",
            "32       313\n",
            "33       814\n",
            "Name: count, dtype: int64\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Load and combine all training data\n",
        "import os\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define dataset directory (adjust path as needed)\n",
        "DATASET_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/data/CICIoT2023/'\n",
        "\n",
        "# Load all CSV files\n",
        "df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
        "df_sets.sort()\n",
        "\n",
        "# Use 80% for training\n",
        "training_sets = df_sets[:int(len(df_sets)*.8)]\n",
        "\n",
        "print(f\"Loading {len(training_sets)} training files...\")\n",
        "\n",
        "# First, let's check what columns are actually available in the dataset\n",
        "sample_df = pd.read_csv(DATASET_DIRECTORY + training_sets[0])\n",
        "print(f\"Available columns in dataset: {list(sample_df.columns)}\")\n",
        "print(f\"Dataset shape: {sample_df.shape}\")\n",
        "\n",
        "# Combine all training data with immediate rounding\n",
        "combined_df = pd.DataFrame()\n",
        "for file in tqdm(training_sets):\n",
        "    df_temp = pd.read_csv(DATASET_DIRECTORY + file)\n",
        "\n",
        "    # Round numbers immediately after loading each file\n",
        "    print(f\"Rounding numbers in {file}...\")\n",
        "    for col in df_temp.columns:\n",
        "        if col != 'Label' and df_temp[col].dtype in ['float64', 'float32']:\n",
        "            # Get max value to determine rounding precision\n",
        "            col_max = df_temp[col].abs().max()\n",
        "\n",
        "            if col_max > 1000:\n",
        "                # Large values: round to 2 decimal places\n",
        "                df_temp[col] = df_temp[col].round(2)\n",
        "            elif col_max > 1:\n",
        "                # Medium values: round to 4 decimal places\n",
        "                df_temp[col] = df_temp[col].round(4)\n",
        "            else:\n",
        "                # Small values (0-1): round to 6 decimal places\n",
        "                df_temp[col] = df_temp[col].round(6)\n",
        "\n",
        "    combined_df = pd.concat([combined_df, df_temp], ignore_index=True)\n",
        "\n",
        "print(f\"Combined dataset shape: {combined_df.shape}\")\n",
        "print(\"All numbers have been rounded during loading process\")\n",
        "\n",
        "# Use actual column names from the dataset (excluding the Label column)\n",
        "X_columns = [col for col in combined_df.columns if col != 'Label']\n",
        "y_column = 'Label'\n",
        "\n",
        "print(f\"Using {len(X_columns)} feature columns\")\n",
        "print(f\"Feature columns: {X_columns[:10]}...\")  # Show first 10 columns\n",
        "\n",
        "# Apply label mapping based on classification type\n",
        "dict_34_classes = {\n",
        "    'BENIGN': 0, 'DDOS-RSTFINFLOOD': 1, 'DDOS-PSHACK_FLOOD': 2, 'DDOS-SYN_FLOOD': 3,\n",
        "    'DDOS-UDP_FLOOD': 4, 'DDOS-TCP_FLOOD': 5, 'DDOS-ICMP_FLOOD': 6, 'DDOS-SYNONYMOUSIP_FLOOD': 7,\n",
        "    'DDOS-ACK_FRAGMENTATION': 8, 'DDOS-UDP_FRAGMENTATION': 9, 'DDOS-ICMP_FRAGMENTATION': 10,\n",
        "    'DDOS-SLOWLORIS': 11, 'DDOS-HTTP_FLOOD': 12, 'DOS-UDP_FLOOD': 13, 'DOS-SYN_FLOOD': 14,\n",
        "    'DOS-TCP_FLOOD': 15, 'DOS-HTTP_FLOOD': 16, 'MIRAI-GREETH_FLOOD': 17, 'MIRAI-GREIP_FLOOD': 18,\n",
        "    'MIRAI-UDPPLAIN': 19, 'RECON-PINGSWEEP': 20, 'RECON-OSSCAN': 21, 'RECON-PORTSCAN': 22,\n",
        "    'VULNERABILITYSCAN': 23, 'RECON-HOSTDISCOVERY': 24, 'DNS_SPOOFING': 25, 'MITM-ARPSPOOFING': 26,\n",
        "    'BROWSERHIJACKING': 27, 'BACKDOOR_MALWARE': 28, 'XSS': 29, 'UPLOADING_ATTACK': 30,\n",
        "    'SQLINJECTION': 31, 'COMMANDINJECTION': 32, 'DICTIONARYBRUTEFORCE': 33\n",
        "}\n",
        "\n",
        "dict_8_classes = {\n",
        "    0: 0,  # Benign\n",
        "    1:1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1,  # DDoS\n",
        "    13: 7, 14: 7, 15: 7, 16: 7,  # DoS\n",
        "    17: 2, 18: 2, 19: 2,  # Mirai\n",
        "    20: 3, 21: 3, 22: 3, 23: 3, 24: 3,  # Reconnaissance\n",
        "    25: 4, 26: 4,  # Spoofing\n",
        "    27: 5, 28: 5, 29: 5, 30: 5, 31: 5, 32: 5,  # Web\n",
        "    33: 6  # Brute Force\n",
        "}\n",
        "\n",
        "dict_2_classes = {\n",
        "    0: 0,  # Benign\n",
        "    1:1, 2:1, 3:1, 4:1, 5:1, 6:1, 7:1, 8:1, 9:1, 10:1, 11:1, 12:1, 13:1, 14:1, 15:1, 16:1,\n",
        "    17: 1, 18: 1, 19: 1, 20: 1, 21: 1, 22: 1, 23: 1, 24: 1, 25: 1, 26: 1,\n",
        "    27: 1, 28: 1, 29: 1, 30: 1, 31: 1, 32: 1, 33: 1  # All attacks as malicious\n",
        "}\n",
        "\n",
        "# Apply label mapping\n",
        "combined_df['Label'] = combined_df['Label'].map(dict_34_classes)\n",
        "\n",
        "if group_classifier:\n",
        "    combined_df['Label'] = combined_df['Label'].map(dict_8_classes)\n",
        "elif binary_classifier:\n",
        "    combined_df['Label'] = combined_df['Label'].map(dict_2_classes)\n",
        "\n",
        "# Remove rows with missing labels\n",
        "combined_df = combined_df.dropna(subset=['Label'])\n",
        "combined_df['Label'] = combined_df['Label'].astype(int)\n",
        "\n",
        "print(f\"Label distribution:\\n{combined_df['Label'].value_counts().sort_index()}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "K-UyWRgN2Xkf",
      "metadata": {
        "id": "K-UyWRgN2Xkf"
      },
      "source": [
        "# Training data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "jLsxKT1I1G_K",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jLsxKT1I1G_K",
        "outputId": "7f66a05f-3e28-4a62-91aa-3c0d63f8370d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File exists, loading data...\n",
            "Training data loaded from pickle file.\n",
            "Training data size: (2806456, 40)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Check to see if the file 'training_data.pkl' exists in the directory. If it does, load it. If not, print an error.\n",
        "if os.path.isfile('training_data.pkl'):\n",
        "    print(\"File exists, loading data...\")\n",
        "    train_df = pd.read_pickle('training_data.pkl')\n",
        "    print(\"Training data loaded from pickle file.\")\n",
        "\n",
        "else:\n",
        "    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
        "    df_sets.sort()\n",
        "    training_sets = df_sets[:int(len(df_sets)*.8)]\n",
        "    test_sets = df_sets[int(len(df_sets)*.8):]\n",
        "\n",
        "    # Print the number of files in each set\n",
        "    print('Training sets: {}'.format(len(training_sets)))\n",
        "    print('Test sets: {}'.format(len(test_sets)))\n",
        "\n",
        "    # ######################\n",
        "    # # TEMP CODE - This would replicate the original authors code with the last CSV\n",
        "    # # for training data. Uncomment this section to use this code.\n",
        "    # ######################\n",
        "    # # Set training_sets to the last entry of training_sets\n",
        "    # training_sets = training_sets[-33:]\n",
        "    # print(f\"TO REPLICATE ORIGINAL AUTHORS CODE WITH ONE FILE TRAIN - {training_sets}\")\n",
        "    # #####################\n",
        "    # # END TEMP CODE\n",
        "    # ######################\n",
        "\n",
        "    # Concatenate all training sets into one dataframe\n",
        "    dfs = []\n",
        "    print(\"Reading training data...\")\n",
        "    for train_set in tqdm(training_sets):\n",
        "        df_new = pd.read_csv(DATASET_DIRECTORY + train_set)\n",
        "        dfs.append(df_new)\n",
        "    train_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Complete training data set size\n",
        "    print(\"Complete training data size: {}\".format(train_df.shape))\n",
        "\n",
        "    # Map y column to the dict_34_classes values - The pickle file already has this done.\n",
        "    train_df['Label'] = train_df['Label'].map(dict_34_classes)\n",
        "\n",
        "    # The training data is the 80% of the CSV files in the dataset. The test data is the remaining 20%.\n",
        "    # The Ray Federated learning mechanism cannot cope with all of the 80% training data, so we will split\n",
        "    # the training data using test_train_split. The test data will be ignored as we will use all the data\n",
        "    # from the train_sets files as our training data to keep parity with the original authors code.\n",
        "    #\n",
        "    # By using a subset of the training data split this way, we can have a randomised selection of data\n",
        "    # from all the training CSV files, stratified by the attack types.\n",
        "\n",
        "    # Percentage of original training data to use.\n",
        "    TRAIN_SIZE = 0.99\n",
        "\n",
        "    print(f\"Splitting the data into {TRAIN_SIZE*100}%\")\n",
        "\n",
        "    X_train, X_test, y_train, y_test = train_test_split(train_df[X_columns], train_df[y_column], test_size= (1 - TRAIN_SIZE), random_state=42, stratify=train_df[y_column])\n",
        "\n",
        "    # Recombine X_train, and y_train into a dataframe\n",
        "    train_df = pd.concat([X_train, y_train], axis=1)\n",
        "\n",
        "    # Clean up unused variables\n",
        "\n",
        "    del X_train, y_train, X_test, y_test\n",
        "\n",
        "    # Save the output to a pickle file\n",
        "    print(\"Writing training data to pickle file...\")\n",
        "    train_df.to_pickle('training_data.pkl')\n",
        "\n",
        "print(\"Training data size: {}\".format(train_df.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "id": "0J9uy3er17f1",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0J9uy3er17f1",
        "outputId": "37145a5f-f86e-4269-e523-5cc27442b63a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Counts of attacks in train_df:\n",
            "Label\n",
            "6     428536\n",
            "4     323000\n",
            "5     267976\n",
            "2     244822\n",
            "3     243154\n",
            "1     241583\n",
            "7     215236\n",
            "13    197952\n",
            "15    159198\n",
            "14    120752\n",
            "0      65732\n",
            "17     59204\n",
            "19     53173\n",
            "18     44553\n",
            "10     26976\n",
            "23     22397\n",
            "26     18467\n",
            "8      17157\n",
            "9      17098\n",
            "25     10829\n",
            "24      8083\n",
            "21      5737\n",
            "22      4912\n",
            "16      4382\n",
            "12      1748\n",
            "11      1366\n",
            "33       806\n",
            "27       341\n",
            "31       320\n",
            "32       310\n",
            "29       255\n",
            "28       178\n",
            "20       140\n",
            "30        83\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# show the unique values counts in the label column for train_df\n",
        "print(\"Counts of attacks in train_df:\")\n",
        "print(train_df['Label'].value_counts())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "id": "PT6QtcaV2MTH",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 497
        },
        "id": "PT6QtcaV2MTH",
        "outputId": "c3479877-1a25-439f-e24d-394d40bfe34a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "         Header_Length  Protocol Type  Time_To_Live          Rate  \\\n",
              "106878           20.00              6         64.00  66313.106719   \n",
              "1146355          20.00              6         64.00  26442.466272   \n",
              "1166244           8.00             17         64.00   6952.614915   \n",
              "2670446          20.00              6         64.00  34433.166407   \n",
              "380158            7.20             17         83.10   8807.492335   \n",
              "...                ...            ...           ...           ...   \n",
              "1443499          19.68              6         62.76  15858.081591   \n",
              "1029352           0.00              1         64.00  32564.472050   \n",
              "323791           20.00              6         64.00  48663.464439   \n",
              "354404           20.00              6         64.00  17346.170389   \n",
              "1166569          20.00              6         64.00  32488.799380   \n",
              "\n",
              "         fin_flag_number  syn_flag_number  rst_flag_number  psh_flag_number  \\\n",
              "106878              0.00              1.0             0.00              0.0   \n",
              "1146355             0.00              1.0             0.00              0.0   \n",
              "1166244             0.00              0.0             0.00              0.0   \n",
              "2670446             0.00              1.0             0.00              0.0   \n",
              "380158              0.00              0.0             0.00              0.0   \n",
              "...                  ...              ...              ...              ...   \n",
              "1443499             0.98              0.0             0.98              0.0   \n",
              "1029352             0.00              0.0             0.00              0.0   \n",
              "323791              0.00              0.0             0.00              0.0   \n",
              "354404              0.00              0.0             0.00              1.0   \n",
              "1166569             1.00              0.0             1.00              0.0   \n",
              "\n",
              "         ack_flag_number  ece_flag_number  ...  Tot sum  Min  Max     AVG  \\\n",
              "106878               0.0              0.0  ...     6000   60   60   60.00   \n",
              "1146355              0.0              0.0  ...     6000   60   60   60.00   \n",
              "1166244              0.0              0.0  ...    55400  554  554  554.00   \n",
              "2670446              0.0              0.0  ...     6000   60   60   60.00   \n",
              "380158               0.0              0.0  ...     6100   60   70   61.00   \n",
              "...                  ...              ...  ...      ...  ...  ...     ...   \n",
              "1443499              0.0              0.0  ...     6303   60  363   63.03   \n",
              "1029352              0.0              0.0  ...     6000   60   60   60.00   \n",
              "323791               0.0              0.0  ...     6000   60   60   60.00   \n",
              "354404               1.0              0.0  ...     6000   60   60   60.00   \n",
              "1166569              0.0              0.0  ...     6000   60   60   60.00   \n",
              "\n",
              "               Std  Tot size       IAT  Number    Variance  Label  \n",
              "106878    0.000000     60.00  0.000065     100    0.000000      7  \n",
              "1146355   0.000000     60.00  0.000038     100    0.000000      3  \n",
              "1166244   0.000000    554.00  0.000160     100    0.000000     19  \n",
              "2670446   0.000000     60.00  0.000029     100    0.000000      3  \n",
              "380158    3.015113     61.00  0.000114     100    9.090909     13  \n",
              "...            ...       ...       ...     ...         ...    ...  \n",
              "1443499  30.300000     63.03  0.000064     100  918.090000      1  \n",
              "1029352   0.000000     60.00  0.000031     100    0.000000      6  \n",
              "323791    0.000000     60.00  0.000021     100    0.000000      5  \n",
              "354404    0.000000     60.00  0.000058     100    0.000000      2  \n",
              "1166569   0.000000     60.00  0.000031     100    0.000000      1  \n",
              "\n",
              "[2806456 rows x 40 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-0d8776bc-9f32-455a-afbb-58a54ce59acb\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Header_Length</th>\n",
              "      <th>Protocol Type</th>\n",
              "      <th>Time_To_Live</th>\n",
              "      <th>Rate</th>\n",
              "      <th>fin_flag_number</th>\n",
              "      <th>syn_flag_number</th>\n",
              "      <th>rst_flag_number</th>\n",
              "      <th>psh_flag_number</th>\n",
              "      <th>ack_flag_number</th>\n",
              "      <th>ece_flag_number</th>\n",
              "      <th>...</th>\n",
              "      <th>Tot sum</th>\n",
              "      <th>Min</th>\n",
              "      <th>Max</th>\n",
              "      <th>AVG</th>\n",
              "      <th>Std</th>\n",
              "      <th>Tot size</th>\n",
              "      <th>IAT</th>\n",
              "      <th>Number</th>\n",
              "      <th>Variance</th>\n",
              "      <th>Label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>106878</th>\n",
              "      <td>20.00</td>\n",
              "      <td>6</td>\n",
              "      <td>64.00</td>\n",
              "      <td>66313.106719</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000065</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1146355</th>\n",
              "      <td>20.00</td>\n",
              "      <td>6</td>\n",
              "      <td>64.00</td>\n",
              "      <td>26442.466272</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1166244</th>\n",
              "      <td>8.00</td>\n",
              "      <td>17</td>\n",
              "      <td>64.00</td>\n",
              "      <td>6952.614915</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>55400</td>\n",
              "      <td>554</td>\n",
              "      <td>554</td>\n",
              "      <td>554.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>554.00</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>19</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2670446</th>\n",
              "      <td>20.00</td>\n",
              "      <td>6</td>\n",
              "      <td>64.00</td>\n",
              "      <td>34433.166407</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>380158</th>\n",
              "      <td>7.20</td>\n",
              "      <td>17</td>\n",
              "      <td>83.10</td>\n",
              "      <td>8807.492335</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6100</td>\n",
              "      <td>60</td>\n",
              "      <td>70</td>\n",
              "      <td>61.00</td>\n",
              "      <td>3.015113</td>\n",
              "      <td>61.00</td>\n",
              "      <td>0.000114</td>\n",
              "      <td>100</td>\n",
              "      <td>9.090909</td>\n",
              "      <td>13</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1443499</th>\n",
              "      <td>19.68</td>\n",
              "      <td>6</td>\n",
              "      <td>62.76</td>\n",
              "      <td>15858.081591</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.98</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6303</td>\n",
              "      <td>60</td>\n",
              "      <td>363</td>\n",
              "      <td>63.03</td>\n",
              "      <td>30.300000</td>\n",
              "      <td>63.03</td>\n",
              "      <td>0.000064</td>\n",
              "      <td>100</td>\n",
              "      <td>918.090000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1029352</th>\n",
              "      <td>0.00</td>\n",
              "      <td>1</td>\n",
              "      <td>64.00</td>\n",
              "      <td>32564.472050</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>323791</th>\n",
              "      <td>20.00</td>\n",
              "      <td>6</td>\n",
              "      <td>64.00</td>\n",
              "      <td>48663.464439</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000021</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>354404</th>\n",
              "      <td>20.00</td>\n",
              "      <td>6</td>\n",
              "      <td>64.00</td>\n",
              "      <td>17346.170389</td>\n",
              "      <td>0.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.00</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1166569</th>\n",
              "      <td>20.00</td>\n",
              "      <td>6</td>\n",
              "      <td>64.00</td>\n",
              "      <td>32488.799380</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>1.00</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>...</td>\n",
              "      <td>6000</td>\n",
              "      <td>60</td>\n",
              "      <td>60</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>60.00</td>\n",
              "      <td>0.000031</td>\n",
              "      <td>100</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2806456 rows × 40 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-0d8776bc-9f32-455a-afbb-58a54ce59acb')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-0d8776bc-9f32-455a-afbb-58a54ce59acb button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-0d8776bc-9f32-455a-afbb-58a54ce59acb');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-840d03ae-cb17-4b7e-b5b6-2fe639d19b89\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-840d03ae-cb17-4b7e-b5b6-2fe639d19b89')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-840d03ae-cb17-4b7e-b5b6-2fe639d19b89 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_dc22caf4-e0dc-4edf-984e-4e1113a50082\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('train_df')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_dc22caf4-e0dc-4edf-984e-4e1113a50082 button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('train_df');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "variable_name": "train_df"
            }
          },
          "metadata": {},
          "execution_count": 43
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "train_df"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NGyzOX3x2dNf",
      "metadata": {
        "id": "NGyzOX3x2dNf"
      },
      "source": [
        "# Test Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "id": "wXD4NTdM2hCz",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wXD4NTdM2hCz",
        "outputId": "66ce881c-60de-4480-ef78-3416c988a4a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "File testing_data.pkl exists, loading data...\n",
            "Test data loaded from pickle file.\n",
            "Testing data size: (744804, 40)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Check to see if the file 'test_data.pkl' exists in the directory. If it does, load it. If not, print an error.\n",
        "testing_data_pickle_file = 'testing_data.pkl'\n",
        "\n",
        "if os.path.isfile(testing_data_pickle_file):\n",
        "    print(f\"File {testing_data_pickle_file} exists, loading data...\")\n",
        "    test_df = pd.read_pickle(testing_data_pickle_file)\n",
        "    print(\"Test data loaded from pickle file.\")\n",
        "\n",
        "else:\n",
        "    print(f\"File {testing_data_pickle_file} does not exist, constructing data...\")\n",
        "\n",
        "    df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]\n",
        "    df_sets.sort()\n",
        "    training_sets = df_sets[:int(len(df_sets)*.8)]\n",
        "    test_sets = df_sets[int(len(df_sets)*.8):]\n",
        "\n",
        "    # Print the number of files in each set\n",
        "    print('Test sets: {}'.format(len(test_sets)))\n",
        "\n",
        "    # Concatenate all testing sets into one dataframe\n",
        "    dfs = []\n",
        "    print(\"Reading test data...\")\n",
        "    for test_set in tqdm(test_sets):\n",
        "        df_new = pd.read_csv(DATASET_DIRECTORY + test_set)\n",
        "        dfs.append(df_new)\n",
        "    test_df = pd.concat(dfs, ignore_index=True)\n",
        "\n",
        "    # Map y column to the dict_34_classes values - The pickle file already has this done.\n",
        "    test_df['Label'] = test_df['Label'].map(dict_34_classes)\n",
        "\n",
        "    # Save the output to a pickle file\n",
        "    print(f\"Writing test data to pickle file {testing_data_pickle_file}...\")\n",
        "    test_df.to_pickle(testing_data_pickle_file)\n",
        "\n",
        "print(\"Testing data size: {}\".format(test_df.shape))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "id": "-nkwO63h2yQm",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-nkwO63h2yQm",
        "outputId": "79c01571-964e-49af-b288-a3046de73f1f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of rows in train_df: 2806456\n",
            "Number of rows in test_df: 744804\n"
          ]
        }
      ],
      "source": [
        "print(\"Number of rows in train_df: {}\".format(len(train_df)))\n",
        "print(\"Number of rows in test_df: {}\".format(len(test_df)))\n",
        "\n",
        "train_size = len(train_df)\n",
        "test_size = len(test_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "NM5ZMAep2-Cx",
      "metadata": {
        "id": "NM5ZMAep2-Cx"
      },
      "source": [
        "\n",
        "# Scale the test and train data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mIuxtu6z2_Rs",
      "metadata": {
        "id": "mIuxtu6z2_Rs"
      },
      "source": [
        "\n",
        "Scale the training data input features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "id": "cOOsYXiO3A1-",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cOOsYXiO3A1-",
        "outputId": "444ea9a5-d9ff-40d3-8618-6a1578a34ac9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for and handling infinite values...\n",
            "Infinite values handled and rows with NaN removed.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "\n",
        "# Check for and handle infinite values\n",
        "print(\"Checking for and handling infinite values...\")\n",
        "train_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "train_df.dropna(inplace=True)\n",
        "print(\"Infinite values handled and rows with NaN removed.\")\n",
        "\n",
        "train_df[X_columns] = scaler.fit_transform(train_df[X_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "id": "J6ijRAD23ooV",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6ijRAD23ooV",
        "outputId": "7ad257c0-fbc5-4389-9840-b0c1d12fe8a5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Checking for and handling infinite values in test data...\n",
            "Infinite values handled and rows with NaN removed from test data.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Check for and handle infinite values\n",
        "print(\"Checking for and handling infinite values in test data...\")\n",
        "test_df.replace([np.inf, -np.inf], np.nan, inplace=True)\n",
        "test_df.dropna(inplace=True)\n",
        "print(\"Infinite values handled and rows with NaN removed from test data.\")\n",
        "\n",
        "# Fit the scaler on the training data and then transform the test data\n",
        "#scaler.fit(train_df[X_columns])\n",
        "test_df[X_columns] = scaler.transform(test_df[X_columns])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 48,
      "id": "Ks2iTZxO4DyJ",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ks2iTZxO4DyJ",
        "outputId": "530dc7b3-2c9c-4a4c-c68b-0f145e3d416e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Individual 34 Class classifier... - No adjustments to labels in test and train dataframes\n"
          ]
        }
      ],
      "source": [
        "class_size_map = {2: \"Binary\", 8: \"Group\", 34: \"Individual\"}\n",
        "\n",
        "if group_classifier:\n",
        "    print(\"Group 8 Class Classifier... - Adjusting labels in test and train dataframes\")\n",
        "    # Map y column to the dict_7_classes values\n",
        "    test_df['label'] = test_df['label'].map(dict_8_classes)\n",
        "    train_df['label'] = train_df['label'].map(dict_8_classes)\n",
        "    class_size = \"8\"\n",
        "\n",
        "elif binary_classifier:\n",
        "    print(\"Binary 2 Class Classifier... - Adjusting labels in test and train dataframes\")\n",
        "    # Map y column to the dict_2_classes values\n",
        "    test_df['label'] = test_df['label'].map(dict_2_classes)\n",
        "    train_df['label'] = train_df['label'].map(dict_2_classes)\n",
        "    class_size = \"2\"\n",
        "\n",
        "else:\n",
        "    print (\"Individual 34 Class classifier... - No adjustments to labels in test and train dataframes\")\n",
        "    class_size = \"34\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "R5FT7UxH6Q7Z",
      "metadata": {
        "id": "R5FT7UxH6Q7Z"
      },
      "source": [
        "# Split the Training Data into partitions for the Federated Learning clients depending on the test required"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 49,
      "id": "-4By9-B5CNR3",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-4By9-B5CNR3",
        "outputId": "41f2f334-d8fc-4733-db8e-74da65a11a2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mLEAVE_ONE_OUT METHOD\u001b[0m with 34 class classifier\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "from sklearn.model_selection import StratifiedKFold\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "\n",
        "# Define the colours used for text printing\n",
        "from enum import Enum\n",
        "class Colours(Enum):\n",
        "    RED = \"\\033[31m\"\n",
        "    YELLOW = \"\\033[33m\"\n",
        "    NORMAL = \"\\033[0m\"\n",
        "\n",
        "# Define fl_X_train and fl_y_train\n",
        "fl_X_train = []\n",
        "fl_y_train = []\n",
        "\n",
        "client_df = pd.DataFrame()\n",
        "\n",
        "# Define the target label column\n",
        "y_column = 'Label'\n",
        "\n",
        "# STRATIFIED method: evenly distribute class labels across clients\n",
        "if METHOD == 'STRATIFIED':\n",
        "    print(f\"{Colours.YELLOW.value}STRATIFIED METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
        "    skf = StratifiedKFold(n_splits=NUM_OF_STRATIFIED_CLIENTS, shuffle=True, random_state=42)\n",
        "    for _, test_index in skf.split(train_df[X_columns], train_df[y_column]):\n",
        "        fl_X_train.append(train_df.iloc[test_index][X_columns])\n",
        "        fl_y_train.append(train_df.iloc[test_index][y_column])\n",
        "\n",
        "# LEAVE_ONE_OUT: remove one class (or benign) from each client’s dataset\n",
        "elif METHOD == 'LEAVE_ONE_OUT':\n",
        "    print(f\"{Colours.YELLOW.value}LEAVE_ONE_OUT METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
        "\n",
        "    num_splits = int(class_size) - 1 if (individual_classifier or group_classifier) else 10\n",
        "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for i, (_, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
        "        current_fold_df = train_df.iloc[test_index]\n",
        "        if binary_classifier:\n",
        "            # Even-indexed client: exclude attack class 1\n",
        "            if i % 2 == 0:\n",
        "                client_df = current_fold_df[current_fold_df[y_column] != 1].copy()\n",
        "            else:\n",
        "                client_df = current_fold_df.copy()\n",
        "        else:\n",
        "            # Exclude one specific attack class\n",
        "            client_df = current_fold_df[current_fold_df[y_column] != (i + 1)].copy()\n",
        "\n",
        "        fl_X_train.append(client_df[X_columns])\n",
        "        fl_y_train.append(client_df[y_column])\n",
        "\n",
        "# ONE_CLASS: each client has Benign + 1 attack class only\n",
        "elif METHOD == 'ONE_CLASS':\n",
        "    print(f\"{Colours.YELLOW.value}ONE_CLASS METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
        "\n",
        "    num_splits = int(class_size) - 1 if (individual_classifier or group_classifier) else 10\n",
        "    skf = StratifiedKFold(n_splits=num_splits, shuffle=True, random_state=42)\n",
        "\n",
        "    for i, (_, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
        "        current_fold_df = train_df.iloc[test_index]\n",
        "        if binary_classifier:\n",
        "            # Even-indexed client: only Benign data\n",
        "            if i % 2 == 0:\n",
        "                client_df = current_fold_df[current_fold_df[y_column] != 1].copy()\n",
        "            else:\n",
        "                client_df = current_fold_df.copy()\n",
        "        else:\n",
        "            # Include only Benign and the (i+1)-th attack class\n",
        "            mask = (current_fold_df[y_column] == 0) | (current_fold_df[y_column] == (i + 1))\n",
        "            client_df = current_fold_df[mask].copy()\n",
        "\n",
        "        fl_X_train.append(client_df[X_columns])\n",
        "        fl_y_train.append(client_df[y_column])\n",
        "\n",
        "# HALF_BENIGN: alternate clients between only-benign and full-class datasets\n",
        "elif METHOD == 'HALF_BENIGN':\n",
        "    print(f\"{Colours.YELLOW.value}HALF_BENIGN METHOD{Colours.NORMAL.value} with {class_size} class classifier\")\n",
        "\n",
        "    skf = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)\n",
        "    for i, (_, test_index) in enumerate(skf.split(train_df[X_columns], train_df[y_column])):\n",
        "        current_fold_df = train_df.iloc[test_index]\n",
        "        if i % 2 == 0:\n",
        "            # Even-indexed clients: only Benign data\n",
        "            client_df = current_fold_df[current_fold_df[y_column] == 0].copy()\n",
        "        else:\n",
        "            # Odd-indexed clients: all data\n",
        "            client_df = current_fold_df.copy()\n",
        "\n",
        "        fl_X_train.append(client_df[X_columns])\n",
        "        fl_y_train.append(client_df[y_column])\n",
        "\n",
        "# Handle unknown METHOD value\n",
        "else:\n",
        "    print(f\"{Colours.RED.value}ERROR: Method {METHOD} not recognised{Colours.NORMAL.value}\")\n",
        "\n",
        "# Update the number of clients created\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 50,
      "id": "a0fc20b8",
      "metadata": {
        "id": "a0fc20b8",
        "outputId": "4175993e-4dc7-4aff-8405-96e70f69bb10",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Applying Model Poisoning ---\n",
            "Poisoning 11 clients: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]\n",
            "Flipping label from 6 to 0 for 100.0% of samples of malicious clients.\n",
            "Client 0: Flipped 12985 labels from class 6 to 0.\n",
            "Client 1: Flipped 12985 labels from class 6 to 0.\n",
            "Client 2: Flipped 12985 labels from class 6 to 0.\n",
            "Client 3: Flipped 12986 labels from class 6 to 0.\n",
            "Client 4: Flipped 12986 labels from class 6 to 0.\n",
            "Client 5: Flipped 0 labels from class 6 to 0.\n",
            "Client 6: Flipped 12986 labels from class 6 to 0.\n",
            "Client 7: Flipped 12986 labels from class 6 to 0.\n",
            "Client 8: Flipped 12986 labels from class 6 to 0.\n",
            "Client 9: Flipped 12986 labels from class 6 to 0.\n",
            "Client 10: Flipped 12986 labels from class 6 to 0.\n",
            "--- Model Poisoning Applied ---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Model Poisoning Attack\n",
        "print(\"--- Applying Model Poisoning ---\")\n",
        "num_malicious_clients = 11  # about 33% of 33 clients\n",
        "malicious_client_ids = list(range(num_malicious_clients))\n",
        "target_class = 6 # 'DDOS-ICMP_FLOOD' - This is the most frequent class\n",
        "new_class = 0    # 'BENIGN'\n",
        "poison_fraction = 1.0 # poison 100% of the target class samples in malicious clients\n",
        "\n",
        "print(f\"Poisoning {num_malicious_clients} clients: {malicious_client_ids}\")\n",
        "print(f\"Flipping label from {target_class} to {new_class} for {poison_fraction*100}% of samples of malicious clients.\")\n",
        "\n",
        "for client_id in malicious_client_ids:\n",
        "    # fl_y_train[client_id] is a pandas Series.\n",
        "    # We can get the indices of the target class and replace the labels.\n",
        "    y_train_series = fl_y_train[client_id]\n",
        "    target_indices = y_train_series[y_train_series == target_class].index\n",
        "\n",
        "    num_to_poison = int(len(target_indices) * poison_fraction)\n",
        "\n",
        "    # Randomly select indices to poison\n",
        "    poison_indices = np.random.choice(target_indices, num_to_poison, replace=False)\n",
        "\n",
        "    # Flip the labels in the pandas Series\n",
        "    fl_y_train[client_id].loc[poison_indices] = new_class\n",
        "\n",
        "    print(f\"Client {client_id}: Flipped {len(poison_indices)} labels from class {target_class} to {new_class}.\")\n",
        "\n",
        "print(\"--- Model Poisoning Applied ---\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "id": "2vEhlpsLMCMF",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "2vEhlpsLMCMF",
        "outputId": "045ad0ac-5a83-463d-9097-ade652a67724"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Client ID: 0 ---\n",
            "fl_X_train[0].shape: (77723, 39)\n",
            "fl_y_train[0].value_counts():\n",
            "Label\n",
            "0     14977\n",
            "4      9788\n",
            "5      8121\n",
            "2      7419\n",
            "3      7368\n",
            "7      6523\n",
            "13     5998\n",
            "15     4825\n",
            "14     3659\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      678\n",
            "26      560\n",
            "8       519\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      148\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "27       11\n",
            "31        9\n",
            "32        9\n",
            "29        8\n",
            "20        5\n",
            "28        5\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[0].unique(): [ 4  0 13 18 25 19 24  2  5  7 14 15  3 17 10 23 11  8 33 26 22 12 21  9\n",
            " 16 27 30 29 28 20 31 32]\n",
            "\n",
            "--- Client ID: 1 ---\n",
            "fl_X_train[1].shape: (77624, 39)\n",
            "fl_y_train[1].value_counts():\n",
            "Label\n",
            "0     14977\n",
            "4      9788\n",
            "5      8121\n",
            "3      7368\n",
            "1      7320\n",
            "7      6523\n",
            "13     5998\n",
            "15     4825\n",
            "14     3659\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      678\n",
            "26      560\n",
            "8       519\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      148\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "27       11\n",
            "31        9\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        5\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[1].unique(): [ 1  7  0 18  3 19  4 14  5 10 13 15 17 16 26 23 25  9 12 22 24 21  8 33\n",
            " 31 11 27 30 29 32 28 20]\n",
            "\n",
            "--- Client ID: 2 ---\n",
            "fl_X_train[2].shape: (77675, 39)\n",
            "fl_y_train[2].value_counts():\n",
            "Label\n",
            "0     14977\n",
            "4      9788\n",
            "5      8121\n",
            "2      7419\n",
            "1      7320\n",
            "7      6523\n",
            "13     5998\n",
            "15     4825\n",
            "14     3659\n",
            "17     1794\n",
            "19     1611\n",
            "18     1349\n",
            "10      818\n",
            "23      678\n",
            "26      560\n",
            "8       519\n",
            "9       519\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      148\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "27       11\n",
            "32        9\n",
            "31        9\n",
            "29        8\n",
            "28        5\n",
            "20        5\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[2].unique(): [ 7  2  5 14  0 13 18  4  1 25 15 26 17  8 19 23 10 21  9 16 11 22 12 24\n",
            " 32 28 27 33 29 31 20 30]\n",
            "\n",
            "--- Client ID: 3 ---\n",
            "fl_X_train[3].shape: (75254, 39)\n",
            "fl_y_train[3].value_counts():\n",
            "Label\n",
            "0     14978\n",
            "5      8121\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6523\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "17     1793\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      678\n",
            "26      560\n",
            "9       519\n",
            "8       519\n",
            "25      328\n",
            "24      244\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "27       11\n",
            "31        9\n",
            "32        9\n",
            "29        8\n",
            "20        5\n",
            "28        5\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[3].unique(): [ 7  0  3  9 15  1  5 14  2 23 26 19 13 10  8 18 25 24 17 12 21 16 22 11\n",
            " 27 33 31 29 20 32 28 30]\n",
            "\n",
            "--- Client ID: 4 ---\n",
            "fl_X_train[4].shape: (76921, 39)\n",
            "fl_y_train[4].value_counts():\n",
            "Label\n",
            "0     14978\n",
            "4      9788\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6523\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "17     1793\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      678\n",
            "26      560\n",
            "8       519\n",
            "9       518\n",
            "25      329\n",
            "24      244\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "32       10\n",
            "27       10\n",
            "31        9\n",
            "29        8\n",
            "28        5\n",
            "20        5\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[4].unique(): [ 3 15 13  0  1  7 21  8  4  2 14 19 18 17 25 23 33 24 16 10 22  9 26 11\n",
            " 28 12 32 29 20 31 27 30]\n",
            "\n",
            "--- Client ID: 5 ---\n",
            "fl_X_train[5].shape: (72056, 39)\n",
            "fl_y_train[5].value_counts():\n",
            "Label\n",
            "4     9788\n",
            "5     8120\n",
            "2     7419\n",
            "3     7368\n",
            "1     7320\n",
            "7     6523\n",
            "13    5998\n",
            "15    4824\n",
            "14    3660\n",
            "0     1992\n",
            "17    1793\n",
            "19    1611\n",
            "18    1350\n",
            "10     817\n",
            "23     678\n",
            "26     559\n",
            "8      520\n",
            "9      518\n",
            "25     329\n",
            "24     245\n",
            "21     174\n",
            "22     149\n",
            "16     133\n",
            "12      53\n",
            "11      41\n",
            "33      25\n",
            "27      10\n",
            "32      10\n",
            "31       9\n",
            "29       8\n",
            "28       5\n",
            "20       5\n",
            "30       2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[5].unique(): [ 5  4 19  1 15 14  2  8 13 17  7  3 23 10  9  0 18 25 26 21 11 33 16 12\n",
            " 28 22 24 27 29 32 20 30 31]\n",
            "\n",
            "--- Client ID: 6 ---\n",
            "fl_X_train[6].shape: (78520, 39)\n",
            "fl_y_train[6].value_counts():\n",
            "Label\n",
            "0     14978\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7369\n",
            "1      7321\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      678\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      329\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "32       10\n",
            "27       10\n",
            "31        9\n",
            "29        8\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[6].unique(): [ 1 15  0  2 13  3  4 19 18 14  5 17  9  8 10 26 23 21 22 16 24 32 25 12\n",
            " 11 31 33 27 29 30 28 20]\n",
            "\n",
            "--- Client ID: 7 ---\n",
            "fl_X_train[7].shape: (84522, 39)\n",
            "fl_y_train[7].value_counts():\n",
            "Label\n",
            "0     14978\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7369\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      678\n",
            "26      559\n",
            "9       518\n",
            "25      329\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "27       10\n",
            "32       10\n",
            "31        9\n",
            "29        8\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[7].unique(): [17  0  5  1  4 15  2 13  7 10  3 18 23  9 14 25 26 22 19 16 24 33 21 27\n",
            " 12 11 32 28 29 31 30 20]\n",
            "\n",
            "--- Client ID: 8 ---\n",
            "fl_X_train[8].shape: (84524, 39)\n",
            "fl_y_train[8].value_counts():\n",
            "Label\n",
            "0     14978\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "10      817\n",
            "23      678\n",
            "26      559\n",
            "8       520\n",
            "25      329\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "31       10\n",
            "27       10\n",
            "32       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[8].unique(): [ 3 14  1  0 22 15  4  7  8 13  5 17  2 19 25 16 18 26 23 10 21 24 12 20\n",
            " 32 27 11 33 28 31 29 30]\n",
            "\n",
            "--- Client ID: 9 ---\n",
            "fl_X_train[9].shape: (84225, 39)\n",
            "fl_y_train[9].value_counts():\n",
            "Label\n",
            "0     14978\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "32       10\n",
            "27       10\n",
            "31       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[9].unique(): [ 1 14 15  2  0  4  5 18  7 17 19  3 13 23 25  8 26  9 21 16 24 22 32 29\n",
            " 11 28 12 33 31 20 27 30]\n",
            "\n",
            "--- Client ID: 10 ---\n",
            "fl_X_train[10].shape: (85001, 39)\n",
            "fl_y_train[10].value_counts():\n",
            "Label\n",
            "0     14978\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "33       25\n",
            "31       10\n",
            "27       10\n",
            "32       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[10].unique(): [ 3  5  4  1  7  0 10 13 14  2 17 15 18 23 22 19  8 25 21 24 26  9 12 29\n",
            " 16 33 27 28 32 31 30 20]\n",
            "\n",
            "--- Client ID: 11 ---\n",
            "fl_X_train[11].shape: (84989, 39)\n",
            "fl_y_train[11].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "11       41\n",
            "33       25\n",
            "32       10\n",
            "31       10\n",
            "27       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[11].unique(): [13  9  7 15  5  2 10  3 14  6 18  1 23 25  4  0 17 16 11 19 26  8 21 33\n",
            " 22 24 32 20 29 27 31 28 30]\n",
            "\n",
            "--- Client ID: 12 ---\n",
            "fl_X_train[12].shape: (79044, 39)\n",
            "fl_y_train[12].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9787\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "32       10\n",
            "27       10\n",
            "31       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[12].unique(): [10  7  6  5  3 19 17 25  4  2  1 14 15  0 23 18  8 26  9 24 21 11 12 22\n",
            " 16 28 33 32 29 31 27 20 30]\n",
            "\n",
            "--- Client ID: 13 ---\n",
            "fl_X_train[13].shape: (81383, 39)\n",
            "fl_y_train[13].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "0      1991\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "33       25\n",
            "31       10\n",
            "32       10\n",
            "27       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[13].unique(): [19  1  6  7  0 13 17  3 15  5  4  2 18 10  8 26 25 23 22 12 16 21 24  9\n",
            " 33 29 28 11 20 27 32 30 31]\n",
            "\n",
            "--- Client ID: 14 ---\n",
            "fl_X_train[14].shape: (80218, 39)\n",
            "fl_y_train[14].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5998\n",
            "14     3659\n",
            "0      1991\n",
            "17     1794\n",
            "19     1612\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "32       10\n",
            "31       10\n",
            "27       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[14].unique(): [13 18 14  4  1  2  3  6 17  7  5 26 10  0  8 19  9 25 23 21 22 24 12 32\n",
            " 16 31 11 33 20 27 28 29 30]\n",
            "\n",
            "--- Client ID: 15 ---\n",
            "fl_X_train[15].shape: (84909, 39)\n",
            "fl_y_train[15].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1991\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "32       10\n",
            "31       10\n",
            "27       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[15].unique(): [ 2  1  4  7  5 10  3  6 25 19 13  9 15 14  0 11 21 17  8 18 24 12 22 26\n",
            " 23 28 32 33 20 27 31 29 30]\n",
            "\n",
            "--- Client ID: 16 ---\n",
            "fl_X_train[16].shape: (83248, 39)\n",
            "fl_y_train[16].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1991\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "32       10\n",
            "27       10\n",
            "31       10\n",
            "29        7\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[16].unique(): [13  6 14  1 15  4  9  5  3 18  8 12 23  0  2 19 21 26 10  7 16 24 25 11\n",
            " 22 32 28 31 20 33 27 30 29]\n",
            "\n",
            "--- Client ID: 17 ---\n",
            "fl_X_train[17].shape: (83692, 39)\n",
            "fl_y_train[17].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1991\n",
            "17     1794\n",
            "19     1611\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "27       10\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[17].unique(): [ 1  4 14  6  3 23  0  2  7 13  9 21 26 15 10  5 17 11 19 24  8 32 25 27\n",
            " 16 22 31 29 12 33 30 28 20]\n",
            "\n",
            "--- Client ID: 18 ---\n",
            "fl_X_train[18].shape: (83431, 39)\n",
            "fl_y_train[18].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1991\n",
            "17     1794\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      559\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "31       10\n",
            "27       10\n",
            "32        9\n",
            "29        8\n",
            "28        6\n",
            "20        4\n",
            "30        2\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[18].unique(): [ 7 23  5  6  3  4  1 15 13 26 14  0 17  2 18  8 10 25 16  9 24 22 11 21\n",
            " 29 33 32 20 27 12 30 28 31]\n",
            "\n",
            "--- Client ID: 19 ---\n",
            "fl_X_train[19].shape: (85038, 39)\n",
            "fl_y_train[19].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1991\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      173\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "31       10\n",
            "27       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[19].unique(): [ 2  4  6 13  7  5 10 15  1  0 14  8 18 26  3 25  9 24 17 23 19 16 21 22\n",
            " 11 29 12 27 28 32 33 31 30]\n",
            "\n",
            "--- Client ID: 20 ---\n",
            "fl_X_train[20].shape: (84869, 39)\n",
            "fl_y_train[20].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1991\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "31       10\n",
            "27       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[20].unique(): [ 7  6 15  5  4  2  3 17  0 14 19 13  8  1 26 23 18 10 22 11  9 25 24 16\n",
            " 33 12 31 29 27 30 32 28 20]\n",
            "\n",
            "--- Client ID: 21 ---\n",
            "fl_X_train[21].shape: (84893, 39)\n",
            "fl_y_train[21].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      173\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "31       10\n",
            "27       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[21].unique(): [23  6  2 13  9  5 17  7 15  4 14  1 33  3 10 24  0 19 18 12  8 21 16 26\n",
            " 25 11 29 31 27 32 28 30 20]\n",
            "\n",
            "--- Client ID: 22 ---\n",
            "fl_X_train[22].shape: (84363, 39)\n",
            "fl_y_train[22].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      173\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "27       10\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[22].unique(): [15  4  1  2  7 14  3  5 18  6  8 10 13  0 21 19 17 12 26  9 24 25 16 22\n",
            " 11 29 20 33 28 32 27 31 30]\n",
            "\n",
            "--- Client ID: 23 ---\n",
            "fl_X_train[23].shape: (84797, 39)\n",
            "fl_y_train[23].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "21      173\n",
            "22      149\n",
            "16      133\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "27       10\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[23].unique(): [ 2 25  1  4  7  3  5  6 13  0 14 18  8 23 15 19  9 17 26 22 11 10 16 21\n",
            " 33 12 27 32 20 29 30 31 28]\n",
            "\n",
            "--- Client ID: 24 ---\n",
            "fl_X_train[24].shape: (84714, 39)\n",
            "fl_y_train[24].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "27       10\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[24].unique(): [ 3  5  7 10 18  6  2  1  4 15 26 14 17 13  0  9 19 22 27 23  8 21 16 24\n",
            " 11 33 12 31 28 20 32 29 30]\n",
            "\n",
            "--- Client ID: 25 ---\n",
            "fl_X_train[25].shape: (84482, 39)\n",
            "fl_y_train[25].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       53\n",
            "11       42\n",
            "33       24\n",
            "27       10\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[25].unique(): [ 6 13  7  5 14  1  4  3 19 17  0  2  8 21 15 23 18 24 22  9 10 16 29 25\n",
            " 12 20 11 27 33 31 32 28 30]\n",
            "\n",
            "--- Client ID: 26 ---\n",
            "fl_X_train[26].shape: (85031, 39)\n",
            "fl_y_train[26].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7321\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      817\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       52\n",
            "11       42\n",
            "33       24\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[26].unique(): [ 3  6  7 17 15  9  1 13  4 14  5  2  0 21 18 19 22 10  8 24 16 26 23 25\n",
            " 31 32 20 30 11 12 33 29 28]\n",
            "\n",
            "--- Client ID: 27 ---\n",
            "fl_X_train[27].shape: (85037, 39)\n",
            "fl_y_train[27].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7418\n",
            "3      7368\n",
            "1      7320\n",
            "7      6522\n",
            "13     5999\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       53\n",
            "11       41\n",
            "33       24\n",
            "27       11\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[27].unique(): [ 0  7 13 24  2 15  6  5  3  4 18 14 26  1 23 11 17  8 19  9 12 25 21 22\n",
            " 10 16 27 31 33 30 20 29 32]\n",
            "\n",
            "--- Client ID: 28 ---\n",
            "fl_X_train[28].shape: (85034, 39)\n",
            "fl_y_train[28].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       53\n",
            "11       41\n",
            "33       24\n",
            "27       11\n",
            "31       10\n",
            "32        9\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[28].unique(): [ 3  4  9 13  1 14  2  5 19  7 25  0 23 15  6 10 17  8 26 18 16 22 12 11\n",
            " 21 24 27 33 32 20 31 28 30]\n",
            "\n",
            "--- Client ID: 29 ---\n",
            "fl_X_train[29].shape: (85039, 39)\n",
            "fl_y_train[29].value_counts():\n",
            "Label\n",
            "6     12986\n",
            "4      9788\n",
            "5      8120\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       53\n",
            "11       41\n",
            "33       24\n",
            "27       11\n",
            "31       10\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[29].unique(): [ 0  4 19 13  5  6 14 18 15  3  1  2 10 17 24 26  7 23  9 11 16  8 22 21\n",
            " 25 29 33 32 31 27 12 20 28]\n",
            "\n",
            "--- Client ID: 30 ---\n",
            "fl_X_train[30].shape: (85032, 39)\n",
            "fl_y_train[30].value_counts():\n",
            "Label\n",
            "6     12985\n",
            "4      9788\n",
            "5      8121\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6522\n",
            "13     5998\n",
            "15     4824\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      149\n",
            "16      132\n",
            "12       53\n",
            "11       41\n",
            "33       24\n",
            "27       11\n",
            "32        9\n",
            "29        8\n",
            "28        5\n",
            "20        4\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[30].unique(): [ 7 17  6  4  3 13 15  2 24  1  0 10 26  5 14 18  8 19 25 23 16 22 21 33\n",
            "  9 28 11 12 27 32 29 30 20]\n",
            "\n",
            "--- Client ID: 31 ---\n",
            "fl_X_train[31].shape: (85033, 39)\n",
            "fl_y_train[31].value_counts():\n",
            "Label\n",
            "6     12985\n",
            "4      9788\n",
            "5      8121\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6522\n",
            "13     5998\n",
            "15     4825\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      679\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      148\n",
            "16      132\n",
            "12       53\n",
            "11       41\n",
            "33       24\n",
            "27       11\n",
            "31        9\n",
            "29        8\n",
            "20        5\n",
            "28        5\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[31].unique(): [13  4  3  6 14  1 15  5  7 19  2 24 26  0 23 25  9 17 22 10  8 18 16 21\n",
            " 11 12 29 27 31 20 33 28 30]\n",
            "\n",
            "--- Client ID: 32 ---\n",
            "fl_X_train[32].shape: (85018, 39)\n",
            "fl_y_train[32].value_counts():\n",
            "Label\n",
            "6     12985\n",
            "4      9788\n",
            "5      8121\n",
            "2      7419\n",
            "3      7368\n",
            "1      7320\n",
            "7      6522\n",
            "13     5998\n",
            "15     4825\n",
            "14     3659\n",
            "0      1992\n",
            "17     1794\n",
            "19     1611\n",
            "18     1350\n",
            "10      818\n",
            "23      678\n",
            "26      560\n",
            "8       520\n",
            "9       518\n",
            "25      328\n",
            "24      245\n",
            "21      174\n",
            "22      148\n",
            "16      133\n",
            "12       53\n",
            "11       41\n",
            "27       11\n",
            "31        9\n",
            "32        9\n",
            "29        8\n",
            "20        5\n",
            "28        5\n",
            "30        3\n",
            "Name: count, dtype: int64\n",
            "fl_y_train[32].unique(): [ 7 13  6 15  5  1  4  3 14  2 19 18 26 17  0 16  8 25 10 23 11  9 24 31\n",
            " 22 21 12 20 28 27 29 32 30]\n",
            "\n",
            "fl_X_train[0].equals(fl_X_train[1]): False\n"
          ]
        }
      ],
      "source": [
        "# Update the number of clients created\n",
        "NUM_OF_CLIENTS = len(fl_X_train)\n",
        "# --- Inspect the training data for each client ---\n",
        "for i in range(NUM_OF_CLIENTS):\n",
        "    print(f\"\\n--- Client ID: {i} ---\")\n",
        "    print(f\"fl_X_train[{i}].shape: {fl_X_train[i].shape}\")\n",
        "    print(f\"fl_y_train[{i}].value_counts():\\n{fl_y_train[i].value_counts()}\")\n",
        "    print(f\"fl_y_train[{i}].unique(): {fl_y_train[i].unique()}\")\n",
        "\n",
        "# Check if two clients have identical feature data\n",
        "print(f\"\\nfl_X_train[0].equals(fl_X_train[1]): {fl_X_train[0].equals(fl_X_train[1])}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ada9yHsr_lqP",
      "metadata": {
        "id": "ada9yHsr_lqP"
      },
      "source": [
        "Visualize Data Distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "UNrBBSrXQ_8G",
      "metadata": {
        "id": "UNrBBSrXQ_8G"
      },
      "source": [
        "STRATIFIED Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "id": "kJMALUIIRItz",
      "metadata": {
        "id": "kJMALUIIRItz"
      },
      "outputs": [],
      "source": [
        "#STRATIFIED Client"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "id": "SL58X3MxRbrH",
      "metadata": {
        "id": "SL58X3MxRbrH"
      },
      "outputs": [],
      "source": [
        "#LEAVE_ONE_OUT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "id": "hi6fOex_ShvS",
      "metadata": {
        "id": "hi6fOex_ShvS"
      },
      "outputs": [],
      "source": [
        "#Half begign"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "id": "ZzkWjT0WUUg9",
      "metadata": {
        "id": "ZzkWjT0WUUg9"
      },
      "outputs": [],
      "source": [
        "#One class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "id": "_1yUTXTWUlbe",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 396
        },
        "id": "_1yUTXTWUlbe",
        "outputId": "c40780c4-60cf-4254-d4e9-2eb7b10623b2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-3333892375.py:16: MatplotlibDeprecationWarning: The get_cmap function was deprecated in Matplotlib 3.7 and will be removed in 3.11. Use ``matplotlib.colormaps[name]`` or ``matplotlib.colormaps.get_cmap()`` or ``pyplot.get_cmap()`` instead.\n",
            "  colors1 = plt.cm.get_cmap('tab20', 20)\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 2200x800 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "client_distributions = {}\n",
        "for i in range(NUM_OF_CLIENTS):\n",
        "    # .value_counts() returns a Series, convert it to a dictionary\n",
        "    client_distributions[f'Client {i}'] = fl_y_train[i].value_counts().to_dict()\n",
        "\n",
        "# 2. Convert the dictionary to a Pandas DataFrame for easy manipulation\n",
        "df_dist = pd.DataFrame(client_distributions).fillna(0).astype(int)\n",
        "df_dist = df_dist.sort_index() # Sort the labels numerically for consistent colors\n",
        "\n",
        "# 3. Plot a Stacked Bar Chart\n",
        "# Convert absolute counts to percentages for the chart y-axis\n",
        "df_percent = df_dist.divide(df_dist.sum(axis=0), axis=1) * 100\n",
        "\n",
        "# Create a diverse color palette to ensure distinct colors for all 34 labels\n",
        "# We combine multiple colormaps to get enough unique colors\n",
        "colors1 = plt.cm.get_cmap('tab20', 20)\n",
        "colors2 = plt.cm.get_cmap('tab20b', 20)\n",
        "colors = np.vstack((colors1.colors, colors2.colors))\n",
        "\n",
        "# Plot the chart with a much wider figure size\n",
        "ax = df_percent.T.plot(\n",
        "    kind='bar',\n",
        "    stacked=True,\n",
        "    figsize=(22, 8), # Increased width for readability\n",
        "    color=colors,\n",
        "    width=0.8,\n",
        "    edgecolor=\"white\"\n",
        ")\n",
        "\n",
        "# 4. Customize the plot for better presentation\n",
        "plt.title(f'Data Distribution Across Clients (Method: {METHOD})', fontsize=20, fontweight='bold')\n",
        "plt.xlabel('Client ID', fontsize=16, fontweight='bold')\n",
        "plt.ylabel('Percentage of Samples (%)', fontsize=16, fontweight='bold')\n",
        "plt.xticks(rotation=45, ha='right', fontsize=12) # Rotate labels for better fit\n",
        "plt.yticks(fontsize=12)\n",
        "plt.ylim(0, 105) # Add a little space at the top for annotations\n",
        "\n",
        "# Move the legend outside the plot and arrange it in 2 columns\n",
        "plt.legend(\n",
        "    title='Labels',\n",
        "    bbox_to_anchor=(1.02, 1),\n",
        "    loc='upper left',\n",
        "    borderaxespad=0.,\n",
        "    fontsize='medium',\n",
        "    ncol=2 # Arrange legend in two columns\n",
        ")\n",
        "\n",
        "# Adjust layout to prevent the legend from being cut off\n",
        "plt.tight_layout(rect=[0, 0, 0.9, 1])\n",
        "\n",
        "# Add total sample count (n=...) above each bar for context\n",
        "for i, total in enumerate(df_dist.sum(axis=0)):\n",
        "    ax.text(i, 101, f'n={total}', ha='center', va='bottom', fontsize=8, fontweight='bold')\n",
        "\n",
        "#  plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "YatCDOP-MYhk",
      "metadata": {
        "id": "YatCDOP-MYhk"
      },
      "source": [
        "Prepare an output directory where we can store the results of the federated learning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "id": "NImC3PNjMZbj",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NImC3PNjMZbj",
        "outputId": "617af22e-466b-415d-ef41-85ebf46d9c7f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Output directory and summary file created at: Output/train_size-2806456/LEAVE_ONE_OUT_Classifier-34_Clients-33\n"
          ]
        }
      ],
      "source": [
        "# Create an \"Output\" directory if it doesnt exist already\n",
        "if not os.path.exists(\"Output\"):\n",
        "    os.makedirs(\"Output\")\n",
        "\n",
        "sub_dir_name = f\"train_size-{train_size}\"\n",
        "\n",
        "# if sub_dir_name does not exist, create it\n",
        "if not os.path.exists(f\"Output/{sub_dir_name}\"):\n",
        "    os.makedirs(f\"Output/{sub_dir_name}\")\n",
        "\n",
        "test_directory_name = f\"{METHOD}_Classifier-{class_size}_Clients-{NUM_OF_CLIENTS}\"\n",
        "output_path = f\"Output/{sub_dir_name}/{test_directory_name}\" # Lưu lại đường dẫn để dùng sau\n",
        "\n",
        "# Create an \"Output/{METHOD}-{NUM_OF_CLIENTS}-{NUM_OF_ROUNDS}\" directory if it doesnt exist already\n",
        "if not os.path.exists(output_path):\n",
        "    os.makedirs(output_path)\n",
        "\n",
        "# Ensure the directory is empty\n",
        "for file in os.listdir(output_path):\n",
        "    file_path = os.path.join(output_path, file)\n",
        "    if os.path.isfile(file_path):\n",
        "        os.unlink(file_path)\n",
        "\n",
        "# Original training size is the sum of all the fl_X_train sizes\n",
        "original_training_size = 0\n",
        "for i in range(len(fl_X_train)):\n",
        "    original_training_size += fl_X_train[i].shape[0]\n",
        "\n",
        "# Write this same info to the output directory/Class Split Info.txt\n",
        "with open(f\"{output_path}/Class Split Info.txt\", \"w\") as f:\n",
        "    for i in range(len(fl_X_train)):\n",
        "        f.write(f\"Client ID: {i}\\n\")\n",
        "        f.write(f\"fl_X_train.shape: {fl_X_train[i].shape}\\n\")\n",
        "        f.write(f\"Training data used {original_training_size}\\n\")\n",
        "        f.write(f\"fl_y_train.value_counts():\\n{fl_y_train[i].value_counts().to_string()}\\n\")\n",
        "        f.write(f\"fl_y_train.unique(): {fl_y_train[i].unique()}\\n\\n\")\n",
        "\n",
        "print(f\"Output directory and summary file created at: {output_path}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hLLaFTx9PeNs",
      "metadata": {
        "id": "hLLaFTx9PeNs"
      },
      "source": [
        "Convert the training dataset\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "id": "K3SnxWmJPfAx",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K3SnxWmJPfAx",
        "outputId": "325d4553-1480-437e-f73a-8d7466e93ffb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ],
      "source": [
        "# Convert the testing dataframe to numpy arrays for TensorFlow/Keras\n",
        "X_test = test_df[X_columns].to_numpy()\n",
        "y_test = test_df[y_column].to_numpy()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "id": "6sCWMHTXPmDb",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6sCWMHTXPmDb",
        "outputId": "610bbdf7-0e10-4744-c6ec-b140ee4c9d66"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Final data conversion to numpy complete. Dataframes deleted to free up memory.\n"
          ]
        }
      ],
      "source": [
        "# Determine the number of unique classes in the target column\n",
        "num_unique_classes = len(train_df[y_column].unique())\n",
        "\n",
        "# Store the shapes of the original dataframes for logging purposes before deleting them\n",
        "train_df_shape = train_df.shape\n",
        "test_df_shape = test_df.shape\n",
        "\n",
        "# Now that we have fl_X_train, fl_y_train, X_test, and y_test extracted,\n",
        "# we can safely delete the large dataframes to free up memory\n",
        "if 'train_df' in locals():\n",
        "    del train_df\n",
        "if 'test_df' in locals():\n",
        "    del test_df\n",
        "if 'client_df' in locals():\n",
        "    del client_df\n",
        "print(\"Final data conversion to numpy complete. Dataframes deleted to free up memory.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "OvyRoEagQYez",
      "metadata": {
        "id": "OvyRoEagQYez"
      },
      "source": [
        "Data check"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "id": "ZP98p2NCQXtE",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZP98p2NCQXtE",
        "outputId": "74c6db60-fa03-4cfb-f1a2-41db26b892d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NUM_CLIENTS: 33\n",
            "NUM_ROUNDS: 5\n",
            "\n",
            "Original training size: 2723339\n",
            "Checking training data split groups\n",
            "0 : X Shape (77723, 39) Y Shape (77723,)\n",
            "1 : X Shape (77624, 39) Y Shape (77624,)\n",
            "2 : X Shape (77675, 39) Y Shape (77675,)\n",
            "3 : X Shape (75254, 39) Y Shape (75254,)\n",
            "4 : X Shape (76921, 39) Y Shape (76921,)\n",
            "5 : X Shape (72056, 39) Y Shape (72056,)\n",
            "6 : X Shape (78520, 39) Y Shape (78520,)\n",
            "7 : X Shape (84522, 39) Y Shape (84522,)\n",
            "8 : X Shape (84524, 39) Y Shape (84524,)\n",
            "9 : X Shape (84225, 39) Y Shape (84225,)\n",
            "10 : X Shape (85001, 39) Y Shape (85001,)\n",
            "11 : X Shape (84989, 39) Y Shape (84989,)\n",
            "12 : X Shape (79044, 39) Y Shape (79044,)\n",
            "13 : X Shape (81383, 39) Y Shape (81383,)\n",
            "14 : X Shape (80218, 39) Y Shape (80218,)\n",
            "15 : X Shape (84909, 39) Y Shape (84909,)\n",
            "16 : X Shape (83248, 39) Y Shape (83248,)\n",
            "17 : X Shape (83692, 39) Y Shape (83692,)\n",
            "18 : X Shape (83431, 39) Y Shape (83431,)\n",
            "19 : X Shape (85038, 39) Y Shape (85038,)\n",
            "20 : X Shape (84869, 39) Y Shape (84869,)\n",
            "21 : X Shape (84893, 39) Y Shape (84893,)\n",
            "22 : X Shape (84363, 39) Y Shape (84363,)\n",
            "23 : X Shape (84797, 39) Y Shape (84797,)\n",
            "24 : X Shape (84714, 39) Y Shape (84714,)\n",
            "25 : X Shape (84482, 39) Y Shape (84482,)\n",
            "26 : X Shape (85031, 39) Y Shape (85031,)\n",
            "27 : X Shape (85037, 39) Y Shape (85037,)\n",
            "28 : X Shape (85034, 39) Y Shape (85034,)\n",
            "29 : X Shape (85039, 39) Y Shape (85039,)\n",
            "30 : X Shape (85032, 39) Y Shape (85032,)\n",
            "31 : X Shape (85033, 39) Y Shape (85033,)\n",
            "32 : X Shape (85018, 39) Y Shape (85018,)\n",
            "\n",
            "Checking testing data\n",
            "X_test size: (744790, 39)\n",
            "y_test size: (744790,)\n",
            "\n",
            "Deploy Simulation\n"
          ]
        }
      ],
      "source": [
        "print(\"NUM_CLIENTS:\", NUM_OF_CLIENTS)\n",
        "\n",
        "print(\"NUM_ROUNDS:\", NUM_OF_ROUNDS)\n",
        "print()\n",
        "\n",
        "\n",
        "print(\"Original training size: {}\".format(original_training_size))\n",
        "\n",
        "\n",
        "print(\"Checking training data split groups\")\n",
        "for i in range(len(fl_X_train)):\n",
        "    print(i, \":\", \"X Shape\", fl_X_train[i].shape, \"Y Shape\", fl_y_train[i].shape)\n",
        "\n",
        "\n",
        "# Print the sizes of X_test and y_test\n",
        "print(\"\\nChecking testing data\")\n",
        "print(\"X_test size: {}\".format(X_test.shape))\n",
        "print(\"y_test size: {}\".format(y_test.shape))\n",
        "\n",
        "print(\"\\nDeploy Simulation\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "id": "hdm4eb53tKQU",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hdm4eb53tKQU",
        "outputId": "6d322702-1d33-4fe4-e494-92cdd724a6ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "X_test min/max/mean: -52.92539879493338 476.80242930958366 -0.000634890663062242\n",
            "Client 0 X min/max/mean: -52.92539879493338 136.22155747982657 -0.02330975502083858\n"
          ]
        }
      ],
      "source": [
        "print(\"X_test min/max/mean:\", np.min(X_test), np.max(X_test), np.mean(X_test))\n",
        "print(\"Client 0 X min/max/mean:\", np.min(fl_X_train[0]), np.max(fl_X_train[0]), np.mean(fl_X_train[0]))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "yQNirMRQtYL_",
      "metadata": {
        "id": "yQNirMRQtYL_"
      },
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "E-jVdOjKRFdj",
      "metadata": {
        "id": "E-jVdOjKRFdj"
      },
      "source": [
        "#Federated Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "CA7bCig1RrDu",
      "metadata": {
        "id": "CA7bCig1RrDu"
      },
      "source": [
        "Import the libraries and print the versions\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "id": "WcQyhdi1RFLY",
      "metadata": {
        "id": "WcQyhdi1RFLY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import flwr as fl\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "# Make TensorFlow log less verbose\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ShtOj_q-R0R7",
      "metadata": {
        "id": "ShtOj_q-R0R7"
      },
      "source": [
        "\n",
        "Define the Client and Server code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "id": "XbTpGaYTS5Ok",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XbTpGaYTS5Ok",
        "outputId": "a789d79f-5543-4c15-e0f6-6baa004acfdd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "scikit-learn 1.6.1.\n",
            "flwr 1.20.0\n",
            "numpy 2.0.2\n",
            "tf 2.19.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import flwr as fl\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "print('scikit-learn {}.'.format(sklearn.__version__))\n",
        "print(\"flwr\", fl.__version__)\n",
        "print(\"numpy\", np.__version__)\n",
        "print(\"tf\", tf.__version__)\n",
        "# Make TensorFlow log less verbose\n",
        "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
        "\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Flatten\n",
        "from tensorflow.keras.layers import Dense\n",
        "from tensorflow.keras.layers import Activation\n",
        "from tensorflow.keras.layers import BatchNormalization\n",
        "from tensorflow.keras.layers import Dropout\n",
        "\n",
        "import datetime\n",
        "\n",
        "client_evaluations = []\n",
        "\n",
        "class NumpyFlowerClient(fl.client.NumPyClient):\n",
        "    def __init__(self, cid, model, train_data, train_labels):\n",
        "        self.model = model\n",
        "        self.cid = cid\n",
        "        self.train_data = train_data\n",
        "        self.train_labels = train_labels\n",
        "\n",
        "    def get_parameters(self, config):\n",
        "        return self.model.get_weights()\n",
        "\n",
        "    def fit(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        print (\"Client \", self.cid, \"Training...\")\n",
        "        self.model.fit(self.train_data, self.train_labels, epochs=1, batch_size=32)\n",
        "        print (\"Client \", self.cid, \"Training complete...\")\n",
        "        return self.model.get_weights(), len(self.train_data), {}\n",
        "\n",
        "    def evaluate(self, parameters, config):\n",
        "        self.model.set_weights(parameters)\n",
        "        print (\"Client \", self.cid, \"Evaluating...\")\n",
        "        loss, accuracy = self.model.evaluate(self.train_data, self.train_labels, batch_size=32)\n",
        "        print(f\"{Colours.YELLOW.value}Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}{Colours.NORMAL.value}\")\n",
        "\n",
        "        # Write the same message to the \"Output/{cid}_Evaluation.txt\" file\n",
        "        with open(f\"Output/{sub_dir_name}/{test_directory_name}/{self.cid}_Evaluation.txt\", \"a\") as f:\n",
        "            f.write(f\"{datetime.datetime.now()} - Client {self.cid} evaluation complete - Accuracy: {accuracy:.6f}, Loss: {loss:.6f}\\n\")\n",
        "\n",
        "            # Close the file\n",
        "            f.close()\n",
        "\n",
        "        return loss, len(self.train_data), {\"accuracy\": accuracy}\n",
        "\n",
        "    def predict(self, incoming):\n",
        "        prediction = np.argmax( self.model.predict(incoming) ,axis=1)\n",
        "        return prediction\n",
        "\n",
        "def client_fn(cid: str) -> NumpyFlowerClient:\n",
        "    \"\"\"Create a Flower client representing a single organization.\"\"\"\n",
        "\n",
        "    # Load model\n",
        "    #model = tf.keras.applications.MobileNetV2((32, 32, 3), classes=10, weights=None)\n",
        "    #model.compile(\"adam\", \"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "    print (\"Client ID:\", cid)\n",
        "\n",
        "    model = Sequential([\n",
        "      #Flatten(input_shape=(79,1)),\n",
        "      Dense(50, activation='relu', input_shape=(fl_X_train[0].shape[1],)),\n",
        "      Dense(25, activation='relu'),\n",
        "      Dense(num_unique_classes, activation='softmax')\n",
        "    ])\n",
        "\n",
        "    model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "\n",
        "    partition_id = int(cid)\n",
        "    X_train_c = fl_X_train[partition_id]\n",
        "    y_train_c = fl_y_train[partition_id]\n",
        "\n",
        "    # Create a  single Flower client representing a single organization\n",
        "    return NumpyFlowerClient(cid, model, X_train_c, y_train_c)\n",
        "\n",
        "\n",
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "eval_count = 0\n",
        "\n",
        "def get_evaluate_fn(server_model):\n",
        "    global eval_count\n",
        "\n",
        "    def evaluate(server_round, parameters, config):\n",
        "        global eval_count\n",
        "\n",
        "        # Update model weights from the latest client parameters\n",
        "        server_model.set_weights(parameters)\n",
        "        print(f\"Server Evaluating... Evaluation Count: {eval_count}\")\n",
        "\n",
        "        # Evaluate the model on the test set\n",
        "        loss, accuracy = server_model.evaluate(X_test, y_test)\n",
        "\n",
        "        # Record accuracy and loss for visualization or tracking\n",
        "        server_accuracy_history.append(accuracy)\n",
        "        server_loss_history.append(loss)\n",
        "\n",
        "        # Make predictions and save them to a file\n",
        "        y_pred = server_model.predict(X_test)\n",
        "        print(\"Prediction: \", y_pred, y_pred.shape)\n",
        "        np.save(\"y_pred-\" + str(eval_count) + \".npy\", y_pred)\n",
        "\n",
        "        # Print evaluation result to console\n",
        "        print(f\"{Colours.YELLOW.value}Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}{Colours.NORMAL.value}\")\n",
        "\n",
        "        # Write evaluation log to file\n",
        "        with open(f\"Output/{sub_dir_name}/{test_directory_name}/Server_Evaluation.txt\", \"a\") as f:\n",
        "            f.write(f\"{datetime.datetime.now()} - {server_round} : Server evaluation complete - Accuracy: {accuracy:.4f}, Loss: {loss:.4f}\\n\")\n",
        "\n",
        "        # Increment evaluation counter\n",
        "        eval_count += 1\n",
        "\n",
        "        return loss, {\"accuracy\": accuracy}\n",
        "\n",
        "    return evaluate\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "h-afiVhOUOOD",
      "metadata": {
        "id": "h-afiVhOUOOD"
      },
      "source": [
        "Initialize Server Model and Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "id": "DWCJDKtAUQYC",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWCJDKtAUQYC",
        "outputId": "ce011a61-ff5d-400d-b32f-12c06d4f8965"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/keras/src/layers/reshaping/flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(**kwargs)\n"
          ]
        }
      ],
      "source": [
        "from flwr.server.strategy import FedProx\n",
        "\n",
        "from flwr.common import ndarrays_to_parameters\n",
        "server_model = Sequential([\n",
        "    #Flatten(input_shape=(79,1)),\n",
        "    Flatten(input_shape=(fl_X_train[0].shape[1] , 1)),\n",
        "    Dense(50, activation='relu'),\n",
        "    Dense(25, activation='relu'),\n",
        "    Dense(num_unique_classes, activation='softmax')\n",
        "])\n",
        "\n",
        "\n",
        "server_model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "initial_weights = server_model.get_weights()\n",
        "initial_parameters = ndarrays_to_parameters(initial_weights)\n",
        "\n",
        "\n",
        "# Define FedProx strategy\n",
        "strategy = FedProx(\n",
        "    proximal_mu=1.0,  # Proximal term coefficient (tune as needed)\n",
        "    fraction_fit=1.0,  # Fraction of clients to sample for training\n",
        "    fraction_evaluate=1.0,  # Fraction of clients to sample for evaluation\n",
        "    min_fit_clients=NUM_OF_CLIENTS,  # Minimum number of clients for training\n",
        "    min_evaluate_clients=NUM_OF_CLIENTS,  # Minimum number of clients for evaluation\n",
        "    min_available_clients=NUM_OF_CLIENTS,  # Minimum number of available clients\n",
        "    evaluate_fn=get_evaluate_fn(server_model),\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "SRP0GFTBUeFI",
      "metadata": {
        "id": "SRP0GFTBUeFI"
      },
      "source": [
        "## Deploy Simulation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "id": "cmUsZg1bUl0r",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "cmUsZg1bUl0r",
        "outputId": "178c281f-3abe-4f6e-dfeb-f9cadd637da7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
            "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
            "\n",
            "\t\t$ flwr new  # Create a new Flower app from a template\n",
            "\n",
            "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
            "\n",
            "\tUsing `start_simulation()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "WARNING:flwr:DEPRECATED FEATURE: flwr.simulation.start_simulation() is deprecated.\n",
            "\tInstead, use the `flwr run` CLI command to start a local simulation in your Flower app, as shown for example below:\n",
            "\n",
            "\t\t$ flwr new  # Create a new Flower app from a template\n",
            "\n",
            "\t\t$ flwr run  # Run the Flower app in Simulation Mode\n",
            "\n",
            "\tUsing `start_simulation()` is deprecated.\n",
            "\n",
            "            This is a deprecated feature. It will be removed\n",
            "            entirely in future versions of Flower.\n",
            "        \n",
            "\u001b[92mINFO \u001b[0m:      Starting Flower simulation, config: num_rounds=5, no round_timeout\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33m\n",
            "Deploy simulation... Method = LEAVE_ONE_OUT - Individual (34) Classifier\n",
            "Number of Clients = 33\n",
            "\n",
            "Writing output to: train_size-2806456/LEAVE_ONE_OUT_Classifier-34_Clients-33\n",
            "\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "2025-09-04 11:57:37,849\tINFO worker.py:1771 -- Started a local Ray instance.\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Ray initialized with resources: {'CPU': 2.0, 'node:__internal_head__': 1.0, 'node:172.28.0.12': 1.0, 'memory': 7987964315.0, 'object_store_memory': 3993982156.0}\n",
            "\u001b[92mINFO \u001b[0m:      Optimize your simulation with Flower VCE: https://flower.ai/docs/framework/how-to-run-simulations.html\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Resources for each Virtual Client: {'num_cpus': 1}\n",
            "\u001b[92mINFO \u001b[0m:      Flower VCE: Creating VirtualClientEngineActorPool with 2 actors\n",
            "\u001b[92mINFO \u001b[0m:      [INIT]\n",
            "\u001b[92mINFO \u001b[0m:      Requesting initial parameters from one random client\n",
            "\u001b[36m(pid=2977)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=2977)\u001b[0m E0000 00:00:1756987066.274959    2977 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=2977)\u001b[0m E0000 00:00:1756987066.290142    2977 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=2977)\u001b[0m W0000 00:00:1756987066.356703    2977 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=2977)\u001b[0m W0000 00:00:1756987066.356765    2977 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=2977)\u001b[0m W0000 00:00:1756987066.356772    2977 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(pid=2977)\u001b[0m W0000 00:00:1756987066.356777    2977 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(pid=2978)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "\u001b[36m(pid=2978)\u001b[0m E0000 00:00:1756987066.274931    2978 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "\u001b[36m(pid=2978)\u001b[0m E0000 00:00:1756987066.292345    2978 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "\u001b[36m(pid=2978)\u001b[0m W0000 00:00:1756987066.354921    2978 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\u001b[32m [repeated 4x across cluster] (Ray deduplicates logs by default. Set RAY_DEDUP_LOGS=0 to disable log deduplication, or see https://docs.ray.io/en/master/ray-observability/user-guides/configure-logging.html#log-deduplication for more options.)\u001b[0m\n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[92mINFO \u001b[0m:      Received initial parameters from one random client\n",
            "\u001b[92mINFO \u001b[0m:      Starting evaluation of initial global parameters\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m Client ID: 32\n",
            "Server Evaluating... Evaluation Count: 0\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m47s\u001b[0m 2ms/step - accuracy: 0.0084 - loss: 3.7655\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m29s\u001b[0m 1ms/step\n",
            "Prediction:  [[0.0413319  0.02055698 0.03648969 ... 0.02702613 0.02861045 0.03887458]\n",
            " [0.02498031 0.02370758 0.02850591 ... 0.02448734 0.01920249 0.03271217]\n",
            " [0.0336929  0.02255688 0.03151886 ... 0.03298358 0.03326057 0.03156703]\n",
            " ...\n",
            " [0.04512829 0.01057832 0.06488446 ... 0.02224102 0.02021614 0.03779423]\n",
            " [0.03317607 0.02395911 0.02858121 ... 0.02895967 0.02671944 0.0326627 ]\n",
            " [0.03369412 0.02455747 0.02893348 ... 0.02970754 0.02600018 0.03150441]] (744790, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      initial parameters (loss, other metrics): 3.7659053802490234, {'accuracy': 0.008301668800413609}\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 1]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 33 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mServer evaluation complete - Accuracy: 0.0083, Loss: 3.7659\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m Client ID: 27\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m Client  27 Training...\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m Client ID: 13\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m         \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m \r\u001b[1m   1/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:21:20\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 3.8960\n",
            "\u001b[1m  25/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m11s\u001b[0m 4ms/step - accuracy: 0.0781 - loss: 3.6322\n",
            "\u001b[1m  59/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.2098 - loss: 3.4128\n",
            "\u001b[1m  95/2658\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m8s\u001b[0m 3ms/step - accuracy: 0.2939 - loss: 3.1746\n",
            "\u001b[1m 448/2544\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 3ms/step - accuracy: 0.5494 - loss: 1.8390\n",
            "\u001b[1m 768/2544\u001b[0m \u001b[32m━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6114 - loss: 1.4559\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m Client  13 Training...\n",
            "\u001b[1m   1/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m1:10:17\u001b[0m 2s/step - accuracy: 0.0000e+00 - loss: 3.8134\n",
            "\u001b[1m  29/2544\u001b[0m \u001b[37m━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m9s\u001b[0m 4ms/step - accuracy: 0.1060 - loss: 3.5506 \n",
            "\u001b[1m1168/2544\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m6s\u001b[0m 4ms/step - accuracy: 0.6490 - loss: 1.2142\u001b[32m [repeated 92x across cluster]\u001b[0m\n",
            "\u001b[1m1323/2658\u001b[0m \u001b[32m━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6263 - loss: 1.2435\n",
            "\u001b[1m1330/2658\u001b[0m \u001b[32m━━━━━━━━━━\u001b[0m\u001b[37m━━━━━━━━━━\u001b[0m \u001b[1m5s\u001b[0m 4ms/step - accuracy: 0.6267 - loss: 1.2409\u001b[32m [repeated 6x across cluster]\u001b[0m\n",
            "\u001b[1m2162/2544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━━━━\u001b[0m \u001b[1m1s\u001b[0m 5ms/step - accuracy: 0.6899 - loss: 0.9455\u001b[32m [repeated 93x across cluster]\u001b[0m\n",
            "\u001b[1m2543/2544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6986 - loss: 0.8896\n",
            "\u001b[1m2658/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 4ms/step - accuracy: 0.6695 - loss: 0.9626\n",
            "\u001b[36m(ClientAppActor pid=2978)\u001b[0m Client  27 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m \u001b[93mWARNING \u001b[0m:   DEPRECATED FEATURE: `client_fn` now expects a signature `def client_fn(context: Context)`.The provided `client_fn` has signature: {'cid': <Parameter \"cid: str\">}. You can import the `Context` like this: `from flwr.common import Context`\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m \n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m             This is a deprecated feature. It will be removed\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m             entirely in future versions of Flower.\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m         \n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m \u001b[93mWARNING \u001b[0m:   Deprecation Warning: The `client_fn` function must return an instance of `Client`, but an instance of `NumpyClient` was returned. Please use `NumPyClient.to_client()` method to convert it to `Client`.\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m /usr/local/lib/python3.12/dist-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m   super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m Client ID: 1\n",
            "\u001b[1m2654/2658\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m━\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6694 - loss: 0.9632\u001b[32m [repeated 23x across cluster]\u001b[0m\n",
            "\u001b[1m2544/2544\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 5ms/step - accuracy: 0.6986 - loss: 0.8893\n",
            "\u001b[36m(ClientAppActor pid=2977)\u001b[0m Client  13 Training complete...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[33m(raylet)\u001b[0m [2025-09-04 12:00:38,729 E 2811 2811] (raylet) node_manager.cc:3064: 2 Workers (tasks / actors) killed due to memory pressure (OOM), 0 Workers crashed due to other reasons at node (ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e, IP: 172.28.0.12) over the last time period. To see more information about the Workers killed on this node, use `ray logs raylet.out -ip 172.28.0.12`\n",
            "\u001b[33m(raylet)\u001b[0m \n",
            "\u001b[33m(raylet)\u001b[0m Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 2 results and 31 failures\n",
            "\u001b[93mWARNING \u001b[0m:   No fit_metrics_aggregation_fn provided\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server Evaluating... Evaluation Count: 1\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m203s\u001b[0m 2ms/step - accuracy: 0.7383 - loss: 0.7716\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m36s\u001b[0m 2ms/step\n",
            "Prediction:  [[3.7498703e-05 4.7369587e-05 5.8653914e-08 ... 1.0649617e-05\n",
            "  1.2349701e-06 1.9447485e-05]\n",
            " [3.7317541e-06 4.6791870e-06 9.9916810e-01 ... 4.9120388e-08\n",
            "  4.2365772e-07 4.9663521e-07]\n",
            " [1.8861522e-06 2.1678122e-07 2.2670961e-06 ... 1.9270923e-05\n",
            "  2.3345630e-07 2.0407981e-06]\n",
            " ...\n",
            " [5.0557053e-05 6.7188736e-08 2.3877772e-05 ... 2.9357304e-07\n",
            "  1.2736498e-07 1.0254053e-05]\n",
            " [1.0112968e-05 8.8631696e-06 2.0721997e-04 ... 1.8453353e-06\n",
            "  4.4773769e-06 5.9385566e-06]\n",
            " [1.9867626e-05 9.5082942e-06 3.4895650e-04 ... 3.3184817e-06\n",
            "  6.5629915e-06 9.6267995e-06]] (744790, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (1, 0.7710522413253784, {'accuracy': 0.7384846806526184}, 775.39648173)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 33 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mServer evaluation complete - Accuracy: 0.7385, Loss: 0.7711\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 33 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 2]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 33 clients (out of 33)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 33 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server Evaluating... Evaluation Count: 2\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 3ms/step - accuracy: 0.7383 - loss: 0.7716\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 2ms/step\n",
            "Prediction:  [[3.7498703e-05 4.7369587e-05 5.8653914e-08 ... 1.0649617e-05\n",
            "  1.2349701e-06 1.9447485e-05]\n",
            " [3.7317541e-06 4.6791870e-06 9.9916810e-01 ... 4.9120388e-08\n",
            "  4.2365772e-07 4.9663521e-07]\n",
            " [1.8861522e-06 2.1678122e-07 2.2670961e-06 ... 1.9270923e-05\n",
            "  2.3345630e-07 2.0407981e-06]\n",
            " ...\n",
            " [5.0557053e-05 6.7188736e-08 2.3877772e-05 ... 2.9357304e-07\n",
            "  1.2736498e-07 1.0254053e-05]\n",
            " [1.0112968e-05 8.8631696e-06 2.0721997e-04 ... 1.8453353e-06\n",
            "  4.4773769e-06 5.9385566e-06]\n",
            " [1.9867626e-05 9.5082942e-06 3.4895650e-04 ... 3.3184817e-06\n",
            "  6.5629915e-06 9.6267995e-06]] (744790, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (2, 0.7710522413253784, {'accuracy': 0.7384846806526184}, 961.3143005009999)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 33 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mServer evaluation complete - Accuracy: 0.7385, Loss: 0.7711\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 33 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 3]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 33 clients (out of 33)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 33 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server Evaluating... Evaluation Count: 3\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m66s\u001b[0m 3ms/step - accuracy: 0.7383 - loss: 0.7716\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 2ms/step\n",
            "Prediction:  [[3.7498703e-05 4.7369587e-05 5.8653914e-08 ... 1.0649617e-05\n",
            "  1.2349701e-06 1.9447485e-05]\n",
            " [3.7317541e-06 4.6791870e-06 9.9916810e-01 ... 4.9120388e-08\n",
            "  4.2365772e-07 4.9663521e-07]\n",
            " [1.8861522e-06 2.1678122e-07 2.2670961e-06 ... 1.9270923e-05\n",
            "  2.3345630e-07 2.0407981e-06]\n",
            " ...\n",
            " [5.0557053e-05 6.7188736e-08 2.3877772e-05 ... 2.9357304e-07\n",
            "  1.2736498e-07 1.0254053e-05]\n",
            " [1.0112968e-05 8.8631696e-06 2.0721997e-04 ... 1.8453353e-06\n",
            "  4.4773769e-06 5.9385566e-06]\n",
            " [1.9867626e-05 9.5082942e-06 3.4895650e-04 ... 3.3184817e-06\n",
            "  6.5629915e-06 9.6267995e-06]] (744790, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (3, 0.7710522413253784, {'accuracy': 0.7384846806526184}, 1146.988079988)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 33 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mServer evaluation complete - Accuracy: 0.7385, Loss: 0.7711\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 33 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 4]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 33 clients (out of 33)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 33 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server Evaluating... Evaluation Count: 4\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m75s\u001b[0m 3ms/step - accuracy: 0.7383 - loss: 0.7716\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m45s\u001b[0m 2ms/step\n",
            "Prediction:  [[3.7498703e-05 4.7369587e-05 5.8653914e-08 ... 1.0649617e-05\n",
            "  1.2349701e-06 1.9447485e-05]\n",
            " [3.7317541e-06 4.6791870e-06 9.9916810e-01 ... 4.9120388e-08\n",
            "  4.2365772e-07 4.9663521e-07]\n",
            " [1.8861522e-06 2.1678122e-07 2.2670961e-06 ... 1.9270923e-05\n",
            "  2.3345630e-07 2.0407981e-06]\n",
            " ...\n",
            " [5.0557053e-05 6.7188736e-08 2.3877772e-05 ... 2.9357304e-07\n",
            "  1.2736498e-07 1.0254053e-05]\n",
            " [1.0112968e-05 8.8631696e-06 2.0721997e-04 ... 1.8453353e-06\n",
            "  4.4773769e-06 5.9385566e-06]\n",
            " [1.9867626e-05 9.5082942e-06 3.4895650e-04 ... 3.3184817e-06\n",
            "  6.5629915e-06 9.6267995e-06]] (744790, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (4, 0.7710522413253784, {'accuracy': 0.7384846806526184}, 1332.058578315)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 33 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mServer evaluation complete - Accuracy: 0.7385, Loss: 0.7711\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 33 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [ROUND 5]\n",
            "\u001b[92mINFO \u001b[0m:      configure_fit: strategy sampled 33 clients (out of 33)\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_fit: received 0 results and 33 failures\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Server Evaluating... Evaluation Count: 5\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m90s\u001b[0m 4ms/step - accuracy: 0.7383 - loss: 0.7716\n",
            "\u001b[1m23275/23275\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m55s\u001b[0m 2ms/step\n",
            "Prediction:  [[3.7498703e-05 4.7369587e-05 5.8653914e-08 ... 1.0649617e-05\n",
            "  1.2349701e-06 1.9447485e-05]\n",
            " [3.7317541e-06 4.6791870e-06 9.9916810e-01 ... 4.9120388e-08\n",
            "  4.2365772e-07 4.9663521e-07]\n",
            " [1.8861522e-06 2.1678122e-07 2.2670961e-06 ... 1.9270923e-05\n",
            "  2.3345630e-07 2.0407981e-06]\n",
            " ...\n",
            " [5.0557053e-05 6.7188736e-08 2.3877772e-05 ... 2.9357304e-07\n",
            "  1.2736498e-07 1.0254053e-05]\n",
            " [1.0112968e-05 8.8631696e-06 2.0721997e-04 ... 1.8453353e-06\n",
            "  4.4773769e-06 5.9385566e-06]\n",
            " [1.9867626e-05 9.5082942e-06 3.4895650e-04 ... 3.3184817e-06\n",
            "  6.5629915e-06 9.6267995e-06]] (744790, 34)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[92mINFO \u001b[0m:      fit progress: (5, 0.7710522413253784, {'accuracy': 0.7384846806526184}, 1526.0734293260002)\n",
            "\u001b[92mINFO \u001b[0m:      configure_evaluate: strategy sampled 33 clients (out of 33)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mServer evaluation complete - Accuracy: 0.7385, Loss: 0.7711\u001b[0m\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: afca4a195530ef57293fb14201000000, name=ClientAppActor.__init__, pid=2978, memory used=0.38GB) was running was 12.08GB / 12.67GB (0.953362), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: 49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-49d33490149fdc47c4f6a8425b05b43b1a87d1cc480cd5eb723cc832*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t7.36\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2978\t0.38\tray::ClientAppActor\n",
            "2977\t0.38\tray::ClientAppActor\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_client_proxy.py\", line 95, in _submit_job\n",
            "    out_mssg, updated_context = self.actor_pool.get_client_result(\n",
            "                                ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 401, in get_client_result\n",
            "    return self._fetch_future_result(cid)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/flwr/simulation/ray_transport/ray_actor.py\", line 282, in _fetch_future_result\n",
            "    res_cid, out_mssg, updated_context = ray.get(\n",
            "                                         ^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/auto_init_hook.py\", line 21, in auto_init_wrapper\n",
            "    return fn(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/client_mode_hook.py\", line 103, in wrapper\n",
            "    return func(*args, **kwargs)\n",
            "           ^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 2639, in get\n",
            "    values, debugger_breakpoint = worker.get_objects(object_refs, timeout=timeout)\n",
            "                                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
            "  File \"/usr/local/lib/python3.12/dist-packages/ray/_private/worker.py\", line 866, in get_objects\n",
            "    raise value\n",
            "ray.exceptions.OutOfMemoryError: Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[91mERROR \u001b[0m:     Task was killed due to the node running low on memory.\n",
            "Memory on the node (IP: 172.28.0.12, ID: c516f5aec14c21e6eae1e7b57a0ac48984b12d8aa7716451614d4f1e) where the task (actor ID: c1a0afb4442f01b6b08a974a01000000, name=ClientAppActor.__init__, pid=2977, memory used=0.38GB) was running was 12.07GB / 12.67GB (0.952324), which exceeds the memory usage threshold of 0.95. Ray killed this worker (ID: e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248) because it was the most recently scheduled task; to see more information about memory usage on this node, use `ray logs raylet.out -ip 172.28.0.12`. To see the logs of the worker, use `ray logs worker-e5c3b3f9ca3fe565cde83029806986595b47672c5b1b237db0361248*out -ip 172.28.0.12. Top 10 memory users:\n",
            "PID\tMEM(GB)\tCOMMAND\n",
            "653\t8.17\t/usr/bin/python3 -m colab_kernel_launcher -f /root/.local/share/jupyter/runtime/kernel-1225167b-6877...\n",
            "2977\t0.38\tray::ClientAppActor.run\n",
            "2477\t0.30\tnode /datalab/web/pyright/pyright-langserver.js --stdio --cancellationReceive=file:ae9f3edebe89ab63c...\n",
            "85\t0.10\t/usr/bin/python3 /usr/local/bin/jupyter-server --debug --transport=\"ipc\" --ip=172.28.0.12 --ServerAp...\n",
            "2772\t0.06\t/usr/bin/python3 /usr/local/lib/python3.12/dist-packages/ray/dashboard/dashboard.py --host=127.0.0.1...\n",
            "2858\t0.06\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/dashboard/agent.py --node-ip-address...\n",
            "2771\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/autoscaler/_private/monitor.py --log...\n",
            "2812\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/log_monitor.py --session-di...\n",
            "2860\t0.04\t/usr/bin/python3 -u /usr/local/lib/python3.12/dist-packages/ray/_private/runtime_env/agent/main.py -...\n",
            "891\t0.04\t/opt/google/drive/drive --features=crash_throttle_percentage:100,fuse_max_background:1000,max_read_q...\n",
            "Refer to the documentation on how to address the out of memory issue: https://docs.ray.io/en/latest/ray-core/scheduling/ray-oom-prevention.html. Consider provisioning more memory on this node or reducing task parallelism by requesting more CPUs per task. Set max_restarts and max_task_retries to enable retry when the task crashes due to OOM. To adjust the kill threshold, set the environment variable `RAY_memory_usage_threshold` when starting Ray. To disable worker killing, set the environment variable `RAY_memory_monitor_refresh_ms` to zero.\n",
            "\u001b[92mINFO \u001b[0m:      aggregate_evaluate: received 0 results and 33 failures\n",
            "\u001b[92mINFO \u001b[0m:      \n",
            "\u001b[92mINFO \u001b[0m:      [SUMMARY]\n",
            "\u001b[92mINFO \u001b[0m:      Run finished 5 round(s) in 1536.10s\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (loss, centralized):\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 0: 3.7659053802490234\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 1: 0.7710522413253784\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 2: 0.7710522413253784\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 3: 0.7710522413253784\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 4: 0.7710522413253784\n",
            "\u001b[92mINFO \u001b[0m:      \t\tround 5: 0.7710522413253784\n",
            "\u001b[92mINFO \u001b[0m:      \tHistory (metrics, centralized):\n",
            "\u001b[92mINFO \u001b[0m:      \t{'accuracy': [(0, 0.008301668800413609),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (1, 0.7384846806526184),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (2, 0.7384846806526184),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (3, 0.7384846806526184),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (4, 0.7384846806526184),\n",
            "\u001b[92mINFO \u001b[0m:      \t              (5, 0.7384846806526184)]}\n",
            "\u001b[92mINFO \u001b[0m:      \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total time taken:  0:27:36.443386\n",
            "\u001b[33mSIMULATION COMPLETE. Method = LEAVE_ONE_OUT - Individual (34) Classifier\n",
            "Number of Clients = 33\u001b[0m\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:151: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x400 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# --- LOG START ---\n",
        "print(f\"{Colours.YELLOW.value}\\nDeploy simulation... Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\")\n",
        "print(f\"Number of Clients = {NUM_OF_CLIENTS}\\n\")\n",
        "print(f\"Writing output to: {sub_dir_name}/{test_directory_name}\\n{Colours.NORMAL.value}\")\n",
        "\n",
        "# Ghi thông tin ban đầu vào file\n",
        "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Run_details.txt\", \"a\") as f:\n",
        "    f.write(f\"{datetime.datetime.now()} - Deploy simulation... Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\\n\")\n",
        "    f.write(f\"{datetime.datetime.now()} - Number of Clients = {NUM_OF_CLIENTS}\\n\")\n",
        "    f.write(f\"{datetime.datetime.now()} - Original train_df size: {train_df_shape}\\n\")\n",
        "\n",
        "    for i in range(len(fl_X_train)):\n",
        "        f.write(f\"{datetime.datetime.now()} - {i}: X Shape {fl_X_train[i].shape}, Y Shape {fl_y_train[i].shape}\\n\")\n",
        "\n",
        "    f.write(f\"{datetime.datetime.now()} - X_test size: {X_test.shape}\\n\")\n",
        "    f.write(f\"{datetime.datetime.now()} - y_test size: {y_test.shape}\\n\")\n",
        "\n",
        "# --- START SIMULATION ---\n",
        "start_time = datetime.datetime.now()\n",
        "\n",
        "ray_init_args = {\"num_cpus\": 2}\n",
        "\n",
        "server_accuracy_history = []\n",
        "server_loss_history = []\n",
        "\n",
        "fl.simulation.start_simulation(\n",
        "    client_fn=client_fn,\n",
        "    num_clients=NUM_OF_CLIENTS,\n",
        "    config=fl.server.ServerConfig(num_rounds=NUM_OF_ROUNDS),\n",
        "    strategy=strategy,\n",
        "    client_resources={\"num_cpus\": 1},\n",
        "    ray_init_args=ray_init_args,\n",
        ")\n",
        "\n",
        "end_time = datetime.datetime.now()\n",
        "print(\"Total time taken: \", end_time - start_time)\n",
        "\n",
        "# --- LOG END ---\n",
        "print(f\"{Colours.YELLOW.value}SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\")\n",
        "print(f\"Number of Clients = {NUM_OF_CLIENTS}{Colours.NORMAL.value}\\n\")\n",
        "\n",
        "# Ghi thông tin kết thúc vào file\n",
        "with open(f\"Output/{sub_dir_name}/{test_directory_name}/Run_details.txt\", \"a\") as f:\n",
        "    f.write(f\"{datetime.datetime.now()} - SIMULATION COMPLETE. Method = {METHOD} - {class_size_map[num_unique_classes]} ({class_size}) Classifier\\n\")\n",
        "    f.write(f\"{datetime.datetime.now()} - Total time taken: {end_time - start_time}\\n\")\n",
        "\n",
        "# --- PLOT ACCURACY ---\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(server_accuracy_history, marker='o')\n",
        "plt.title('Server Accuracy per Round')\n",
        "plt.xlabel('Round')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"Output/{sub_dir_name}/{test_directory_name}/server_accuracy_per_round.png\")\n",
        "plt.show()\n",
        "\n",
        "# --- PLOT LOSS ---\n",
        "plt.figure(figsize=(10, 4))\n",
        "plt.plot(server_loss_history, marker='o', color='red')\n",
        "plt.title('Server Loss per Round')\n",
        "plt.xlabel('Round')\n",
        "plt.ylabel('Loss')\n",
        "plt.grid(True)\n",
        "plt.savefig(f\"Output/{sub_dir_name}/{test_directory_name}/server_loss_per_round.png\")\n",
        "plt.show()\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}