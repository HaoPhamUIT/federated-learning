#%% md
# Federated Learning with CIC-IoT-2023 Dataset - Attack & Defense Implementation
#%% md
## Step 1: Environment Setup
#%%
!pip install flwr[simulation] torch torchvision matplotlib scikit-learn openml
#%%
import os
import pandas as pd
import numpy as np
import flwr as fl
from tqdm import tqdm
import warnings
from typing import List, Tuple, Optional, Dict, Union
from scipy.spatial.distance import cosine

import sklearn
from sklearn.linear_model import LogisticRegression
from sklearn import preprocessing
from sklearn.utils import shuffle
from sklearn.model_selection import train_test_split, StratifiedKFold
from sklearn.preprocessing import StandardScaler, MinMaxScaler
from sklearn.metrics import classification_report, confusion_matrix, accuracy_score

import torch
import torch.nn as nn
import torch.nn.functional as F
import torchvision.transforms as transforms
from flwr.common import Metrics, Parameters, Scalar
from flwr.client import NumPyClient
from torch.utils.data import DataLoader, random_split, TensorDataset
import matplotlib.pyplot as plt
import seaborn as sns

# Import defense system
from defense_system import FLTrustDefense, KrumDefense, create_defense, get_defense_config
#%%
print("flwr", fl.__version__)
print("numpy", np.__version__)
print("torch", torch.__version__)

DEVICE = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
print(f"Training on {DEVICE}")
#%% md
## Step 2: Configuration Settings
#%%
SPLIT_AVAILABLE_METHODS = ['STRATIFIED','LEAVE_ONE_OUT', 'ONE_CLASS', 'HALF_BENIGN' ,'DIRICHLET']
METHOD = 'DIRICHLET'
NUM_OF_STRATIFIED_CLIENTS = 10
NUM_OF_ROUNDS = 5
NUM_OF_DIRICHLET_CLIENTS = 10
DIRICHLET_ALPHA = 0.5
#%%
individual_classifier = True
group_classifier = False
binary_classifier = False
#%%
# Defense Strategy Configuration
DEFENSE_STRATEGY = 'FLTRUST'  # Options: 'NONE', 'KRUM', 'TRIMMED_MEAN', 'MEDIAN', 'ANOMALY_DETECTION', 'FLTRUST'

# Defense parameters
ANOMALY_THRESHOLD = 0.7
TRIM_RATIO = 0.2
BYZANTINE_CLIENTS = 2
FLTRUST_BETA = 0.5
SERVER_DATA_RATIO = 0.1
KRUM_M = 2

print(f"Selected defense strategy: {DEFENSE_STRATEGY}")
#%% md
## Step 3: Data Access Setup
#%%
from google.colab import drive
drive.mount('/content/drive')
#%%
DATASET_DIRECTORY = '/content/drive/MyDrive/Colab Notebooks/data/CICIoT2023/'
#%% md
## Step 4: Data Loading and Preprocessing
#%%
# Load all CSV files
df_sets = [k for k in os.listdir(DATASET_DIRECTORY) if k.endswith('.csv')]
df_sets.sort()

# Use 80% for training
training_sets = df_sets[:int(len(df_sets)*.8)]

# Check dataset structure
sample_df = pd.read_csv(DATASET_DIRECTORY + training_sets[0])
print(f"Available columns in dataset: {list(sample_df.columns)}")
print(f"Dataset shape: {sample_df.shape}")
#%%
# Combine all training data with immediate rounding
combined_df = pd.DataFrame()
for file in tqdm(training_sets):
    df_temp = pd.read_csv(DATASET_DIRECTORY + file)
    
    # Round numbers for memory efficiency
    for col in df_temp.columns:
        if col != 'Label' and df_temp[col].dtype in ['float64', 'float32']:
            col_max = df_temp[col].abs().max()
            
            if col_max > 1000:
                df_temp[col] = df_temp[col].round(2)
            elif col_max > 1:
                df_temp[col] = df_temp[col].round(4)
            else:
                df_temp[col] = df_temp[col].round(6)
    
    combined_df = pd.concat([combined_df, df_temp], ignore_index=True)

print(f"Combined dataset shape: {combined_df.shape}")
#%% md
## Step 5: Label Mapping and Classification Setup
#%%
# 34-class mapping (individual attacks)
dict_34_classes = {
    'BENIGN': 0, 'DDOS-RSTFINFLOOD': 1, 'DDOS-PSHACK_FLOOD': 2, 'DDOS-SYN_FLOOD': 3,
    'DDOS-UDP_FLOOD': 4, 'DDOS-TCP_FLOOD': 5, 'DDOS-ICMP_FLOOD': 6, 'DDOS-SYNONYMOUSIP_FLOOD': 7,
    'DDOS-ACK_FRAGMENTATION': 8, 'DDOS-UDP_FRAGMENTATION': 9, 'DDOS-ICMP_FRAGMENTATION': 10,
    'DDOS-SLOWLORIS': 11, 'DDOS-HTTP_FLOOD': 12, 'DOS-UDP_FLOOD': 13, 'DOS-SYN_FLOOD': 14,
    'DOS-TCP_FLOOD': 15, 'DOS-HTTP_FLOOD': 16, 'MIRAI-GREETH_FLOOD': 17, 'MIRAI-GREIP_FLOOD': 18,
    'MIRAI-UDPPLAIN': 19, 'RECON-PINGSWEEP': 20, 'RECON-OSSCAN': 21, 'RECON-PORTSCAN': 22,
    'VULNERABILITYSCAN': 23, 'RECON-HOSTDISCOVERY': 24, 'DNS_SPOOFING': 25, 'MITM-ARPSPOOFING': 26,
    'BROWSERHIJACKING': 27, 'BACKDOOR_MALWARE': 28, 'XSS': 29, 'UPLOADING_ATTACK': 30,
    'SQLINJECTION': 31, 'COMMANDINJECTION': 32, 'DICTIONARYBRUTEFORCE': 33
}

# 8-class mapping (attack groups)
dict_8_classes = {
    0: 0,  # Benign
    1:1, 2: 1, 3: 1, 4: 1, 5: 1, 6: 1, 7: 1, 8: 1, 9: 1, 10: 1, 11: 1, 12: 1,  # DDoS
    13: 7, 14: 7, 15: 7, 16: 7,  # DoS
    17: 2, 18: 2, 19: 2,  # Mirai
    20: 3, 21: 3, 22: 3, 23: 3, 24: 3,  # Reconnaissance
    25: 4, 26: 4,  # Spoofing
    27: 5, 28: 5, 29: 5, 30: 5, 31: 5, 32: 5,  # Web
    33: 6  # Brute Force
}

# 2-class mapping (binary classification)
dict_2_classes = {
    0: 0,  # Benign
    **{i: 1 for i in range(1, 34)}  # All attacks mapped to 1
}
#%%
# Use actual column names from the dataset
X_columns = [col for col in combined_df.columns if col != 'Label']
y_column = 'Label'

# Apply appropriate label mapping
combined_df['Label'] = combined_df['Label'].map(dict_34_classes)

if group_classifier:
    combined_df['Label'] = combined_df['Label'].map(dict_8_classes)
    class_size = "8"
elif binary_classifier:
    combined_df['Label'] = combined_df['Label'].map(dict_2_classes)
    class_size = "2"
else:
    class_size = "34"

# Clean data
combined_df = combined_df.dropna(subset=['Label'])
combined_df['Label'] = combined_df['Label'].astype(int)

print(f"Using {class_size}-class classification")
print(f"Label distribution:\n{combined_df['Label'].value_counts().sort_index()}")
#%% md
## Step 6: Training Data Preparation
#%%
if os.path.isfile('training_data.pkl'):
    print("Loading existing training data...")
    train_df = pd.read_pickle('training_data.pkl')
else:
    print("Processing training data...")
    # Use combined_df as train_df for simplicity
    TRAIN_SIZE = 0.99
    
    X_train, X_test, y_train, y_test = train_test_split(
        combined_df[X_columns], 
        combined_df[y_column], 
        test_size=(1 - TRAIN_SIZE), 
        random_state=42, 
        stratify=combined_df[y_column]
    )
    
    train_df = pd.concat([X_train, y_train], axis=1)
    train_df.to_pickle('training_data.pkl')

print(f"Training data size: {train_df.shape}")
#%% md
## Step 7: Test Data Preparation
#%%
if os.path.isfile('testing_data.pkl'):
    print("Loading existing test data...")
    test_df = pd.read_pickle('testing_data.pkl')
else:
    print("Processing test data...")
    test_sets = df_sets[int(len(df_sets)*.8):]
    
    dfs = []
    for test_set in tqdm(test_sets):
        df_new = pd.read_csv(DATASET_DIRECTORY + test_set)
        dfs.append(df_new)
    test_df = pd.concat(dfs, ignore_index=True)
    
    test_df['Label'] = test_df['Label'].map(dict_34_classes)
    if group_classifier:
        test_df['Label'] = test_df['Label'].map(dict_8_classes)
    elif binary_classifier:
        test_df['Label'] = test_df['Label'].map(dict_2_classes)
    
    test_df.to_pickle('testing_data.pkl')

print(f"Test data size: {test_df.shape}")
#%% md
## Step 8: Data Scaling and Preprocessing
#%%
scaler = StandardScaler()

# Handle infinite values and NaN
train_df.replace([np.inf, -np.inf], np.nan, inplace=True)
train_df.dropna(inplace=True)
test_df.replace([np.inf, -np.inf], np.nan, inplace=True)
test_df.dropna(inplace=True)

# Apply scaling
train_df[X_columns] = scaler.fit_transform(train_df[X_columns])
test_df[X_columns] = scaler.transform(test_df[X_columns])

# Prepare FLTrust server data if needed
if DEFENSE_STRATEGY == 'FLTRUST' and defense_instance:
    server_data = defense_instance.prepare_server_data(train_df, X_columns, y_column)
    print(f"FLTrust server data prepared: {len(server_data)} samples")

print("Data scaling completed")
#%% md
## Step 9: Federated Learning Data Distribution
#%%
# Initialize client data containers
fl_X_train = []
fl_y_train = []

if METHOD == 'DIRICHLET':
    print(f"DIRICHLET METHOD with {class_size} class classifier")
    
    num_clients = NUM_OF_DIRICHLET_CLIENTS
    alpha = DIRICHLET_ALPHA
    
    y_data = train_df[y_column].values
    X_data = train_df[X_columns].values
    
    # Split sample indices by class
    class_indices = {}
    for cls in np.unique(y_data):
        class_indices[cls] = np.where(y_data == cls)[0]
    
    # Create Dirichlet distribution for each class
    client_indices = [[] for _ in range(num_clients)]
    for cls, indices in class_indices.items():
        n_samples = len(indices)
        proportions = np.random.dirichlet([alpha] * num_clients)
        splits = (proportions * n_samples).astype(int)
        
        # Handle rounding errors
        while splits.sum() < n_samples:
            splits[np.argmax(proportions)] += 1
        while splits.sum() > n_samples:
            splits[np.argmax(splits)] -= 1
        
        # Distribute samples to clients
        np.random.shuffle(indices)
        start = 0
        for client_id, split_size in enumerate(splits):
            end = start + split_size
            client_indices[client_id].extend(indices[start:end])
            start = end
    
    # Create client datasets
    for client_id in range(num_clients):
        idxs = client_indices[client_id]
        client_df = train_df.iloc[idxs]
        fl_X_train.append(client_df[X_columns])
        fl_y_train.append(client_df[y_column])

NUM_OF_CLIENTS = len(fl_X_train)
print(f"Created {NUM_OF_CLIENTS} clients")
#%% md
## Step 10: Attack Configuration and Implementation
#%%
# Attack Configuration
ATTACK_TYPES = [
    'MODEL_POISONING',
    'MIMIC_ATTACK', 
    'LABEL_FLIPPING',
    'GRADIENT_ASCENT',
    'BACKDOOR_ATTACK',
    'BYZANTINE_ATTACK',
    'DATA_POISONING',
    'NONE'
]

ATTACK_TYPE = 'MODEL_POISONING'
num_malicious_clients = min(3, NUM_OF_CLIENTS // 3)  # 33% of clients
malicious_client_ids = [str(i) for i in range(num_malicious_clients)]

print(f"Attack Type: {ATTACK_TYPE}")
print(f"Malicious clients: {num_malicious_clients} ({malicious_client_ids})")
#%%
# Attack Implementation Functions
def apply_model_poisoning(model_params, poison_factor=3.0):
    """Scale model parameters by poison factor"""
    return [param * poison_factor for param in model_params]

def apply_mimic_attack(model_params, mimic_strength=1.5, noise_std=0.1):
    """Apply subtle parameter modifications with noise"""
    poisoned_params = []
    for param in model_params:
        noise = np.random.normal(0, noise_std, param.shape)
        poisoned_param = param * mimic_strength + noise
        poisoned_params.append(poisoned_param)
    return poisoned_params

def apply_byzantine_attack(model_params, noise_scale=10.0):
    """Generate random adversarial updates"""
    poisoned_params = []
    for param in model_params:
        random_noise = np.random.normal(0, noise_scale, param.shape)
        poisoned_params.append(random_noise.astype(param.dtype))
    return poisoned_params

print("Attack functions defined")
#%% md
## Step 11: Defense Strategy Implementation
#%%
# Initialize defense instance
defense_instance = create_defense(DEFENSE_STRATEGY, **get_defense_config(DEFENSE_STRATEGY))

# FLTrust server model setup
server_model_fltrust = None
if DEFENSE_STRATEGY == 'FLTRUST':
    # Will be initialized after data loading
    print("FLTrust defense selected - server model will be prepared after data loading")
elif DEFENSE_STRATEGY == 'KRUM':
    print(f"Krum defense selected with {KRUM_M} Byzantine clients")

print(f"Defense instance: {type(defense_instance).__name__ if defense_instance else 'None'}")
#%% md
## Step 12: Neural Network Model Definition
#%%
class IoTAttackNet(nn.Module):
    def __init__(self, input_size, num_classes):
        super(IoTAttackNet, self).__init__()
        self.fc1 = nn.Linear(input_size, 128)
        self.fc2 = nn.Linear(128, 64)
        self.fc3 = nn.Linear(64, 32)
        self.fc4 = nn.Linear(32, num_classes)
        self.dropout = nn.Dropout(0.2)
        
    def forward(self, x):
        x = F.relu(self.fc1(x))
        x = self.dropout(x)
        x = F.relu(self.fc2(x))
        x = self.dropout(x)
        x = F.relu(self.fc3(x))
        x = self.fc4(x)
        return x

# Model parameters
INPUT_SIZE = len(X_columns)
NUM_CLASSES = int(class_size)
LEARNING_RATE = 0.001
EPOCHS = 5
BATCH_SIZE = 32

print(f"Model: {INPUT_SIZE} -> 128 -> 64 -> 32 -> {NUM_CLASSES}")
#%%
def get_model_parameters(model):
    """Extract model parameters as a list of numpy arrays."""
    return [val.cpu().numpy() for _, val in model.state_dict().items()]

def set_model_parameters(model, parameters):
    """Set model parameters from a list of numpy arrays."""
    params_dict = zip(model.state_dict().keys(), parameters)
    state_dict = {k: torch.tensor(v) for k, v in params_dict}
    model.load_state_dict(state_dict, strict=True)

def train_model(model, trainloader, epochs=1):
    """Train the model on the training set."""
    model.train()
    criterion = nn.CrossEntropyLoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=LEARNING_RATE)
    
    for epoch in range(epochs):
        for batch_idx, (data, target) in enumerate(trainloader):
            data, target = data.to(DEVICE), target.to(DEVICE)
            optimizer.zero_grad()
            output = model(data)
            loss = criterion(output, target)
            loss.backward()
            optimizer.step()
    return model

def test_model(model, testloader):
    """Evaluate the model on the test set."""
    model.eval()
    criterion = nn.CrossEntropyLoss()
    test_loss = 0
    correct = 0
    with torch.no_grad():
        for data, target in testloader:
            data, target = data.to(DEVICE), target.to(DEVICE)
            output = model(data)
            test_loss += criterion(output, target).item()
            pred = output.argmax(dim=1, keepdim=True)
            correct += pred.eq(target.view_as(pred)).sum().item()
    
    accuracy = correct / len(testloader.dataset)
    return test_loss, accuracy

print("Model utility functions defined")
#%% md
## Step 13: Flower Client Implementation
#%%
class IoTClient(NumPyClient):
    def __init__(self, client_id, X_train, y_train, X_test, y_test, is_malicious=False):
        self.client_id = client_id
        self.is_malicious = is_malicious
        
        # Convert to tensors
        self.X_train = torch.FloatTensor(X_train.values)
        self.y_train = torch.LongTensor(y_train.values)
        self.X_test = torch.FloatTensor(X_test.values)
        self.y_test = torch.LongTensor(y_test.values)
        
        # Create data loaders
        train_dataset = TensorDataset(self.X_train, self.y_train)
        test_dataset = TensorDataset(self.X_test, self.y_test)
        
        self.trainloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)
        self.testloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False)
        
        # Initialize model
        self.model = IoTAttackNet(INPUT_SIZE, NUM_CLASSES).to(DEVICE)
    
    def get_parameters(self, config):
        """Return current model parameters."""
        return get_model_parameters(self.model)
    
    def set_parameters(self, parameters):
        """Set model parameters."""
        set_model_parameters(self.model, parameters)
    
    def fit(self, parameters, config):
        """Train the model with the given parameters."""
        self.set_parameters(parameters)
        
        # Normal training
        train_model(self.model, self.trainloader, epochs=EPOCHS)
        
        # Apply attack if malicious client
        model_params = get_model_parameters(self.model)
        if self.is_malicious:
            if ATTACK_TYPE == 'MODEL_POISONING':
                model_params = apply_model_poisoning(model_params)
            elif ATTACK_TYPE == 'MIMIC_ATTACK':
                model_params = apply_mimic_attack(model_params)
            elif ATTACK_TYPE == 'BYZANTINE_ATTACK':
                model_params = apply_byzantine_attack(model_params)
            print(f"Client {self.client_id}: Applied {ATTACK_TYPE}")
        
        return model_params, len(self.trainloader.dataset), {}
    
    def evaluate(self, parameters, config):
        """Evaluate the model with the given parameters."""
        self.set_parameters(parameters)
        loss, accuracy = test_model(self.model, self.testloader)
        return loss, len(self.testloader.dataset), {"accuracy": accuracy}

print("IoT Client class defined")
#%%
def client_fn(cid: str) -> IoTClient:
    """Create a Flower client representing a single organization."""
    client_id = int(cid)
    
    # Get client data
    X_train_client = fl_X_train[client_id]
    y_train_client = fl_y_train[client_id]
    
    # Use a portion of test data for client evaluation
    test_size_per_client = len(test_df) // NUM_OF_CLIENTS
    start_idx = client_id * test_size_per_client
    end_idx = start_idx + test_size_per_client
    
    X_test_client = test_df[X_columns].iloc[start_idx:end_idx]
    y_test_client = test_df[y_column].iloc[start_idx:end_idx]
    
    # Check if client is malicious
    is_malicious = cid in malicious_client_ids
    
    return IoTClient(client_id, X_train_client, y_train_client, 
                    X_test_client, y_test_client, is_malicious)

print("Client factory function defined")
#%% md
## Step 14: Server Strategy with Defense
#%%
from flwr.server.strategy import FedAvg

class DefenseStrategy(FedAvg):
    def __init__(self, defense_instance, *args, **kwargs):
        super().__init__(*args, **kwargs)
        self.defense_instance = defense_instance
        self.defense_strategy = DEFENSE_STRATEGY
        self.server_model = None
        
        # Initialize server model for FLTrust
        if DEFENSE_STRATEGY == 'FLTRUST':
            self.server_model = IoTAttackNet(INPUT_SIZE, NUM_CLASSES).to(DEVICE)
    
    def train_server_model(self):
        """Train server model for FLTrust"""
        if self.defense_strategy != 'FLTRUST' or not self.defense_instance.server_data:
            return None
            
        server_X = torch.FloatTensor(self.defense_instance.server_data[X_columns].values)
        server_y = torch.LongTensor(self.defense_instance.server_data[y_column].values)
        server_dataset = TensorDataset(server_X, server_y)
        server_loader = DataLoader(server_dataset, batch_size=BATCH_SIZE, shuffle=True)
        
        train_model(self.server_model, server_loader, epochs=EPOCHS)
        return get_model_parameters(self.server_model)
    
    def aggregate_fit(self, server_round, results, failures):
        print(f"\nRound {server_round}: Using {self.defense_strategy} defense")
        
        if self.defense_strategy == 'NONE':
            return super().aggregate_fit(server_round, results, failures)
        
        # Extract client updates
        client_updates = [fl.common.parameters_to_ndarrays(fit_res.parameters) 
                         for _, fit_res in results]
        
        # Apply defense-specific aggregation
        if self.defense_strategy == 'FLTRUST':
            server_update = self.train_server_model()
            if server_update:
                defended_params, trust_scores = self.defense_instance.aggregate(client_updates, server_update)
                print(f"Trust scores: {trust_scores}")
            else:
                return super().aggregate_fit(server_round, results, failures)
        
        elif self.defense_strategy == 'KRUM':
            defended_params, selected_client = self.defense_instance.aggregate(client_updates)
            print(f"Krum selected client: {selected_client}")
        
        else:
            return super().aggregate_fit(server_round, results, failures)
            
        return fl.common.ndarrays_to_parameters(defended_params), {}
    
    def aggregate_evaluate(self, server_round, results, failures):
        if not results:
            return None, {}
        
        accuracies = [r.metrics["accuracy"] * r.num_examples for _, r in results]
        examples = [r.num_examples for _, r in results]
        aggregated_accuracy = sum(accuracies) / sum(examples)
        aggregated_loss = sum([r.loss * r.num_examples for _, r in results]) / sum(examples)
        
        print(f"Round {server_round} - Accuracy: {aggregated_accuracy:.4f}")
        
        return aggregated_loss, {"accuracy": aggregated_accuracy}

print("Defense strategy class defined")
#%% md
## Step 15: Initialize Server Model and Strategy
#%%
# Create initial model for server
server_model = IoTAttackNet(INPUT_SIZE, NUM_CLASSES).to(DEVICE)
initial_parameters = get_model_parameters(server_model)

# Initialize strategy with defense instance
strategy = DefenseStrategy(
    defense_instance=defense_instance,
    fraction_fit=1.0,
    fraction_evaluate=1.0,
    min_fit_clients=NUM_OF_CLIENTS,
    min_evaluate_clients=NUM_OF_CLIENTS,
    min_available_clients=NUM_OF_CLIENTS,
    initial_parameters=fl.common.ndarrays_to_parameters(initial_parameters),
)

print(f"Server initialized with {DEFENSE_STRATEGY} defense")
print(f"Model parameters: {sum(p.numel() for p in server_model.parameters())}")
#%% md
## Step 16: Run Federated Learning Simulation
#%%
from flwr.simulation import start_simulation

print(f"Starting FL simulation with {NUM_OF_CLIENTS} clients")
print(f"Attack: {ATTACK_TYPE}, Defense: {DEFENSE_STRATEGY}")
print(f"Malicious clients: {malicious_client_ids}")

# Run simulation
history = start_simulation(
    client_fn=client_fn,
    num_clients=NUM_OF_CLIENTS,
    config=fl.server.ServerConfig(num_rounds=NUM_OF_ROUNDS),
    strategy=strategy,
    client_resources={"num_cpus": 1, "num_gpus": 0.0},
)

print("\nSimulation completed!")
#%% md
## Step 17: Results Analysis
#%%
# Plot training results
plt.figure(figsize=(12, 4))

# Plot accuracy
plt.subplot(1, 2, 1)
rounds = range(1, NUM_OF_ROUNDS + 1)
accuracies = [metrics["accuracy"] for _, metrics in history.metrics_distributed["accuracy"]]
plt.plot(rounds, accuracies, 'b-o')
plt.title(f'Accuracy - {ATTACK_TYPE} vs {DEFENSE_STRATEGY}')
plt.xlabel('Round')
plt.ylabel('Accuracy')
plt.grid(True)

# Plot loss
plt.subplot(1, 2, 2)
losses = [loss for loss, _ in history.losses_distributed]
plt.plot(rounds, losses, 'r-o')
plt.title(f'Loss - {ATTACK_TYPE} vs {DEFENSE_STRATEGY}')
plt.xlabel('Round')
plt.ylabel('Loss')
plt.grid(True)

plt.tight_layout()
plt.show()

# Print final results
final_accuracy = accuracies[-1] if accuracies else 0
final_loss = losses[-1] if losses else 0

print(f"\nFinal Results:")
print(f"Attack Type: {ATTACK_TYPE}")
print(f"Defense Strategy: {DEFENSE_STRATEGY}")
print(f"Final Accuracy: {final_accuracy:.4f}")
print(f"Final Loss: {final_loss:.4f}")
print(f"Malicious Clients: {len(malicious_client_ids)}/{NUM_OF_CLIENTS}")

if hasattr(strategy, 'detected_anomalies') and strategy.detected_anomalies:
    print(f"Detected Anomalies: {strategy.detected_anomalies}")
#%% md
## Step 18: Experiment Configuration
#%%
# Experiment configurations for different attack-defense combinations
experiments = [
    {'attack': 'NONE', 'defense': 'NONE'},
    {'attack': 'MODEL_POISONING', 'defense': 'NONE'},
    {'attack': 'MODEL_POISONING', 'defense': 'KRUM'},
    {'attack': 'MODEL_POISONING', 'defense': 'TRIMMED_MEAN'},
    {'attack': 'MIMIC_ATTACK', 'defense': 'ANOMALY_DETECTION'},
    {'attack': 'BYZANTINE_ATTACK', 'defense': 'MEDIAN'},
]

print("Available experiment configurations:")
for i, exp in enumerate(experiments):
    print(f"{i+1}. Attack: {exp['attack']}, Defense: {exp['defense']}")

print("\nTo run different experiments, modify ATTACK_TYPE and DEFENSE_STRATEGY variables and re-run the simulation.")